{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\">Deep Learning Fundamentals</h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">Multiclass Classification: MNIST</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:36.081105Z",
     "start_time": "2021-05-26T22:26:35.040138Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxliary plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:37.473177Z",
     "start_time": "2021-05-26T22:26:37.465910Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/view-classify-in-module-helper/30279/6\n",
    "\n",
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset\n",
    "First up, we need to get our dataset. This is provided through the `torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:38.402766Z",
     "start_time": "2021-05-26T22:26:38.298968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:38.632988Z",
     "start_time": "2021-05-26T22:26:38.477558Z"
    }
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. We'd use this to loop through the dataset for training, but here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:39.407000Z",
     "start_time": "2021-05-26T22:26:39.265256Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAHwCAYAAAC7cCafAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAABYlAAAWJQFJUiTwAAAcdUlEQVR4nO3dfaxlZX0v8O+vjICOMqBp1abXAlIlpfUFbEGIyEvrRdsqKhD/aKWNWO01UizctGmlF2tvYxNzfb1XW0nFSnKxwWjTW6reCAjKWxy0aADBAiIRROAK4ogIPPePvUanp+fMzNl7z6xznv35JDvP2WutZz+/WbNmvmetvV6qtRYAoB8/NXYBAMB8CXcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6MyGsQvYFarq1iT7JLlt5FIAYFr7J3mgtXbAajt2Ge6ZBPuThxcALJReD8vfNnYBADAHt03TadRwr6qfq6q/q6pvVdUPq+q2qnp3Ve03Zl0AsJ6Ndli+qp6Z5IokP5PkH5PcmORXk/xhkhOq6qjW2r1j1QcA69WYe+7/K5NgP721dmJr7U9aa8cleVeSZyf57yPWBgDrVrXWdv+gVQcm+bdMvkt4ZmvtsW3mPSnJnUkqyc+01r4/xedvTnLofKoFgNFc21o7bLWdxjosf9zQfmbbYE+S1tr3quoLSV6S5Igkn13pQ4YQX87Bc6kSANahsQ7LP3tob1ph/s1D+6zdUAsAdGWsPfdNQ3v/CvO3Tt93ex+y0qEKh+UBWGRr9Tr3Gtrdf0IAAKxzY4X71j3zTSvM32fJcgDAThor3L82tCt9p/4LQ7vSd/IAwArGCvdLhvYlVfXvahguhTsqyQ+SXLW7CwOA9W6UcG+t/VuSz2TyxJs3LZn9tiQbk/z9NNe4A8CiG/OpcP8lk9vPvreqjk9yQ5LDkxybyeH4PxuxNgBYt0Y7W37Ye39BkvMyCfUzkzwzyXuTvNB95QFgOqM+z7219s0kvzdmDQDQm7V6nTsAMCXhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0JkNYxcAa8Ev/uIvTt33i1/84kxj77///lP3vfvuu2caG+jTaHvuVXVbVbUVXneNVRcArHdj77nfn+Tdy0x/cDfXAQDdGDvcv9taO2fkGgCgK06oA4DOjL3nvldV/XaSZyT5fpLrklzWWnt03LIAYP0aO9yfluSjS6bdWlW/11r73I46V9XmFWYdPHNlALBOjXlY/sNJjs8k4Dcm+eUkf5Nk/yT/UlXPHa80AFi/Rttzb629bcmkryZ5Y1U9mOTMJOckeeUOPuOw5aYPe/SHzqFMAFh31uIJdR8c2qNHrQIA1qm1GO5bb7m1cdQqAGCdWovh/sKhvWXUKgBgnRol3KvqkKp68jLTfz7J+4e35+/eqgCgD2OdUHdykj+pqkuS3Jrke0memeQ3kuyd5KIk7xypNgBY18YK90uSPDvJ8zM5DL8xyXeTfD6T694/2lprI9UGAOvaKOE+3KBmhzepgd3lta997dR9N2yY7Z+RR74C87YWT6gDAGYg3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADpTrbWxa5i7qtqc5NCx62D3Oeyww2bqf80110zd98EHH5xp7E2bNs3Un8Xyzne+c+q+Bx100Exjv+1tb5u675e+9KWZxl5g17bWVv0fnD13AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzmwYuwDY6olPfOLUff/2b/92prEfffTRqfuee+65M43NYnnqU586U/83vOENU/d9+OGHZxr7hhtumKk/u489dwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojOe5s2a87GUvm7rv85///JnG/uY3vzl13zPPPHOmsVksH/nIR2bqv3Hjxqn7vvGNb5xp7Iceemim/uw+9twBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA645GvzM2BBx44U/8PfehDU/fdsmXLTGP/5m/+5kz9WSxPf/rTp+571FFHzTT2ww8/PHXfa665ZqaxWT/suQNAZ+YS7lV1UlW9r6our6oHqqpV1fk76HNkVV1UVfdV1Zaquq6qzqiqPeZREwAsqnkdln9rkucmeTDJHUkO3t7CVfWKJB9P8lCSjyW5L8lvJXlXkqOSnDynugBg4czrsPxbkjwryT5J/mB7C1bVPkk+lOTRJMe01l7XWvuvSZ6X5MokJ1XVa+ZUFwAsnLmEe2vtktbaza21thOLn5Tkp5Nc0Fr74jaf8VAmRwCSHfyCAACsbIwT6o4b2k8tM++yJFuSHFlVe+2+kgCgH2NcCvfsob1p6YzW2iNVdWuSQ5IcmOSG7X1QVW1eYdZ2v/MHgJ6Nsee+aWjvX2H+1un77vpSAKA/a/EmNjW0O/z+vrV22LIfMNmjP3SeRQHAejHGnvvWPfNNK8zfZ8lyAMAqjBHuXxvaZy2dUVUbkhyQ5JEkt+zOogCgF2OE+8VDe8Iy845O8oQkV7TWfrj7SgKAfowR7hcmuSfJa6rqBVsnVtXeSf5yePuBEeoCgC7M5YS6qjoxyYnD26cN7Qur6rzh53taa2clSWvtgap6fSYhf2lVXZDJ7WdfnsllchdmcktaAGAK8zpb/nlJTl0y7cDhlSTfSHLW1hmttU9W1YuT/FmSVyfZO8nXk/xRkvfu5J3uAIBlzCXcW2vnJDlnlX2+kORl8xifteHtb3/7TP2f9KQnTd33qquummnsr3zlKzP1Z7GcfvrpU/fduHHjTGPfeOONU/e9+eabZxqb9cPz3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADozr+e504kDDzxw6r4nnXTSTGNv2bJl6r5vfvObZxobVuOUU04ZbewrrrhitLFZP+y5A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnPM+df+fwww+fuu/jHve4mca+6667pu57/fXXzzQ2rMa+++472tgf+9jHRhub9cOeOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeEOwB0RrgDQGeqtTZ2DXNXVZuTHDp2HevR05/+9Kn73nrrrTONveeee07d94EHHphp7BNOOGHqvlddddVMY7P+3HvvvVP33W+//WYa+xvf+MbUfQ844ICZxmYU17bWDlttJ3vuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANCZDWMXwNpy5513Tt33tNNOm2nsD3/4w1P33bRp00xjX3nllVP3/c53vjPT2J/4xCem7nv11VfPNPaYPvKRj8zU/7jjjpu67x133DHT2E984hOn7ltVM409y79RFoc9dwDozFzCvapOqqr3VdXlVfVAVbWqOn+FZfcf5q/0umAeNQHAoprXYfm3JnlukgeT3JHk4J3o869JPrnM9K/OqSYAWEjzCve3ZBLqX0/y4iSX7ESfL7fWzpnT+ADAYC7h3lr7cZjPerIIADCbMc+W/9mqekOSpyS5N8mVrbXrVvMBVbV5hVk787UAAHRpzHD/9eH1Y1V1aZJTW2u3j1IRAHRgjHDfkuTtmZxMd8sw7TlJzklybJLPVtXzWmvf39EHtdYOW276sEd/6DyKBYD1Zrdf595au7u19uettWtba98dXpcleUmSq5MclGS2u6EAwAJbMzexaa09kuTc4e3RY9YCAOvZmgn3wdb7eG4ctQoAWMfWWrgfMbS3bHcpAGBFuz3cq+rwqtpzmenHZXIznCRZ9ta1AMCOzeVs+ao6McmJw9unDe0Lq+q84ed7WmtnDT//dZJDhsvetj6a6TlJtj7i6ezW2hXzqAsAFtG8LoV7XpJTl0w7cHglyTeSbA33jyZ5ZZJfSfLSJI9L8u0k/5Dk/a21y+dUEwAspGqtjV3D3LnOfX16xjOeMXXf3//9359p7FNOOWXqvgcddNBMY69Xs95q+kc/+tFM/ffYY4+p+z722GOjjT3rn/uII47Y8UIr+NKXvjTT2Izi2pXu6bI9a+2EOgBgRsIdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADoj3AGgM8IdADrjka+Q2R7hue+++8409l577TV139/93d+daexvfetbU/ed5RG9SXL00UfP1P/b3/721H1f9apXzTT2LH9nX/jCF2Ya+0UvetFM/Vl3PPIVABDuANAd4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AnRHuANAZ4Q4AndkwdgGwFjz66KNT97333nvnWMnq/NVf/dVoY49t06ZNU/d9xSteMcdKVufyyy8fbWwWhz13AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAznjkK7AunX/++VP3fcITnjDT2I888sjUfWepG3aWPXcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IxwB4DOCHcA6IznuQPr0vHHHz/a2FdfffXUfa+//vo5VgLLm3nPvaqeUlWnVdUnqurrVfWDqrq/qj5fVa+rqmXHqKojq+qiqrqvqrZU1XVVdUZV7TFrTQCwyOax535ykg8kuTPJJUluT/LUJK9Kcm6Sl1bVya21trVDVb0iyceTPJTkY0nuS/JbSd6V5KjhMwGAKcwj3G9K8vIk/9xae2zrxKr60yTXJHl1JkH/8WH6Pkk+lOTRJMe01r44TD87ycVJTqqq17TWLphDbQCwcGY+LN9au7i19k/bBvsw/a4kHxzeHrPNrJOS/HSSC7YG+7D8Q0neOrz9g1nrAoBFtavPlv/R0D6yzbTjhvZTyyx/WZItSY6sqr12ZWEA0KtddrZ8VW1I8trh7bZB/uyhvWlpn9baI1V1a5JDkhyY5IYdjLF5hVkHr65aAOjHrtxzf0eSX0pyUWvt09tM3zS096/Qb+v0fXdRXQDQtV2y515Vpyc5M8mNSX5ntd2Htm13qSSttcNWGH9zkkNXOS4AdGHue+5V9aYk70lyfZJjW2v3LVlk6575pixvnyXLAQCrMNdwr6ozkrw/yVczCfa7llnsa0P7rGX6b0hyQCYn4N0yz9oAYFHMLdyr6o8zuQnNlzMJ9rtXWPTioT1hmXlHJ3lCkitaaz+cV20AsEjmEu7DDWjekWRzkuNba/dsZ/ELk9yT5DVV9YJtPmPvJH85vP3APOoCgEU08wl1VXVqkr/I5I5zlyc5vaqWLnZba+28JGmtPVBVr88k5C+tqgsyuf3syzO5TO7CTG5JCwBMYR5nyx8wtHskOWOFZT6X5Lytb1prn6yqFyf5s0xuT7t3kq8n+aMk7932PvQAwOpUjznqUjhY+x7/+MfP1P+++5ZeiLPz9tprthtg/tqv/drUfS+++OIdLwQ/ce1Kl31vz66+/SwAsJsJdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM4IdwDojHAHgM5sGLsAYDGdffbZM/Wf9Znss7jzzjtHGxt2hj13AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAzgh3AOiMcAeAznjkKzCKs846a7SxW2uj9oddzZ47AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHRGuANAZ4Q7AHTG89yBdWnLli1T9339618/09g33njjTP1hV7PnDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BnhDgCdEe4A0BmPfAVGseeee45dAnTLnjsAdGbmcK+qp1TVaVX1iar6elX9oKrur6rPV9Xrquqnliy/f1W17bwumLUmAFhk8zgsf3KSDyS5M8klSW5P8tQkr0pybpKXVtXJrbW2pN+/JvnkMp/31TnUBAALax7hflOSlyf559baY1snVtWfJrkmyaszCfqPL+n35dbaOXMYHwDYxsyH5VtrF7fW/mnbYB+m35Xkg8PbY2YdBwDYObv6bPkfDe0jy8z72ap6Q5KnJLk3yZWttet2cT0A0L1dFu5VtSHJa4e3n1pmkV8fXtv2uTTJqa2123dyjM0rzDp4J8sEgO7sykvh3pHkl5Jc1Fr79DbTtyR5e5LDkuw3vF6cycl4xyT5bFVt3IV1AUDX6j+exD6HD606Pcl7ktyY5KjW2n070WdDks8nOTzJGa2198ww/uYkh07bHwDWiGtba4etttPc99yr6k2ZBPv1SY7dmWBPktbaI5lcOpckR8+7LgBYFHMN96o6I8n7M7lW/djhjPnV+M7QOiwPAFOaW7hX1R8neVeSL2cS7HdP8TFHDO0t86oLABbNXMK9qs7O5AS6zUmOb63ds51lD6+q//DEiKo6Lslbhrfnz6MuAFhEM18KV1WnJvmLJI8muTzJ6VW1dLHbWmvnDT//dZJDhsve7himPSfJccPPZ7fWrpi1LgBYVPO4zv2Aod0jyRkrLPO5JOcNP380ySuT/EqSlyZ5XJJvJ/mHJO9vrV0+h5oAYGHtkkvhxuZSOAA6sTYuhQMAxiXcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOiPcAaAzwh0AOtNruO8/dgEAMAf7T9Npw5yLWCseGNrbVph/8NDeuOtL6YZ1Nh3rbTrW2+pZZ9NZy+tt//wkz1alWmvzLWUdqKrNSdJaO2zsWtYL62w61tt0rLfVs86m0+t66/WwPAAsLOEOAJ0R7gDQGeEOAJ0R7gDQmYU8Wx4AembPHQA6I9wBoDPCHQA6I9wBoDPCHQA6I9wBoDPCHQA6s1DhXlU/V1V/V1XfqqofVtVtVfXuqtpv7NrWomH9tBVed41d35iq6qSqel9VXV5VDwzr5Pwd9Dmyqi6qqvuqaktVXVdVZ1TVHrur7rGtZr1V1f7b2f5aVV2wu+sfQ1U9papOq6pPVNXXq+oHVXV/VX2+ql5XVcv+P77o29tq11tv21uvz3P/D6rqmUmuSPIzSf4xk2f3/mqSP0xyQlUd1Vq7d8QS16r7k7x7mekP7uY61pq3JnluJuvhjvzkmdDLqqpXJPl4koeSfCzJfUl+K8m7khyV5ORdWewasqr1NvjXJJ9cZvpX51fWmnZykg8kuTPJJUluT/LUJK9Kcm6Sl1bVyW2bO5LZ3pJMsd4GfWxvrbWFeCX5dJKW5M1Lpv+PYfoHx65xrb2S3JbktrHrWIuvJMcm+YUkleSYYRs6f4Vl90lyd5IfJnnBNtP3zuQXzpbkNWP/mdbgett/mH/e2HWPvM6OyySYf2rJ9KdlElgtyau3mW57m269dbW9LcRh+ao6MMlLMgmr/7lk9n9L8v0kv1NVG3dzaaxTrbVLWms3t+F/hR04KclPJ7mgtfbFbT7joUz2ZJPkD3ZBmWvOKtcbSVprF7fW/qm19tiS6Xcl+eDw9phtZtneMtV668qiHJY/bmg/s8xf9Peq6guZhP8RST67u4tb4/aqqt9O8oxMfgm6LsllrbVHxy1rXdm6/X1qmXmXJdmS5Miq2qu19sPdV9a68bNV9YYkT0lyb5IrW2vXjVzTWvGjoX1km2m2tx1bbr1t1cX2tijh/uyhvWmF+TdnEu7PinBf6mlJPrpk2q1V9Xuttc+NUdA6tOL211p7pKpuTXJIkgOT3LA7C1snfn14/VhVXZrk1Nba7aNUtAZU1YYkrx3ebhvktrft2M5626qL7W0hDssn2TS0968wf+v0fXd9KevKh5Mcn0nAb0zyy0n+JpPvpv6lqp47Xmnriu1vOluSvD3JYUn2G14vzuTkqGOSfHbBv0p7R5JfSnJRa+3T20y3vW3fSuutq+1tUcJ9R2pofQ+4jdba24bvrb7dWtvSWvtqa+2NmZyE+Pgk54xbYTdsf8tord3dWvvz1tq1rbXvDq/LMjnKdnWSg5KcNm6V46iq05OcmclVP7+z2u5Du3Db2/bWW2/b26KE+9bfVDetMH+fJcuxfVtPRjl61CrWD9vfHLXWHsnkUqZkAbfBqnpTkvckuT7Jsa21+5YsYntbxk6st2Wt1+1tUcL9a0P7rBXm/8LQrvSdPP/e3UO7bg5RjWzF7W/4/u+ATE7suWV3FrXOfWdoF2obrKozkrw/k2uujx3O/F7K9rbETq637Vl329uihPslQ/uSZe5K9KRMburwgyRX7e7C1qkXDu3C/Ocwo4uH9oRl5h2d5AlJrljgM5enccTQLsw2WFV/nMlNaL6cSUDdvcKitrdtrGK9bc+6294WItxba/+W5DOZnAj2piWz35bJb2N/31r7/m4ubc2qqkOq6snLTP/5TH4DTpLt3m6VH7swyT1JXlNVL9g6sar2TvKXw9sPjFHYWlZVh1fVnstMPy7JW4a3C7ENVtXZmZwItjnJ8a21e7azuO1tsJr11tv2VotyL4llbj97Q5LDM7lj1k1JjmxuP/tjVXVOkj/J5KjHrUm+l+SZSX4jkztdXZTkla21h8eqcUxVdWKSE4e3T0vynzP5rf7yYdo9rbWzlix/YSa3A70gk9uBvjyTy5YuTHLKItzYZTXrbbj86JAkl2Zyq9okeU5+ch332a21rWHVrao6Ncl5SR5N8r4s/135ba2187bpc2IWfHtb7Xrrbnsb+xZ5u/OV5D9lcnnXnUkeTvKNTE6wePLYta21VyaXgPzvTM4q/W4mN334TpL/m8k1ojV2jSOvn3MyOdt4pddty/Q5KpNfiv5fJl8DfSWTPYI9xv7zrMX1luR1Sf5PJneWfDCT26nensm90l809p9lDa2zluRS29ts66237W1h9twBYFEsxHfuALBIhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0BnhDsAdEa4A0Bn/j+rz63iekZDnAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "image/png": {
       "width": 251,
       "height": 248
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to build a simple feedfoward network to classify the MNIST images. That is, the network will receive a digit image as input and predict the digit in the image.\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "To build a neural network with PyTorch, you use the `torch.nn` module. The network itself is a class inheriting from `torch.nn.Module`. You define each of the operations separately, like `nn.Linear(784, 128)` for a fully connected linear layer with 784 inputs and 128 units.\n",
    "\n",
    "The class needs to include a `forward` method that implements the forward pass through the network. In this method, you pass some input tensor `x` through each of the operations you defined earlier. The `torch.nn` module also has functional equivalents for things like ReLUs in `torch.nn.functional`. This module is usually imported as `F`. Then to use a ReLU activation on some layer (which is just a tensor), you'd do `F.relu(x)`. Below are a few different commonly used activation functions.\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "So, for this network, I'll build it with three fully connected layers, then a softmax output for predicting classes. The softmax function is similar to the sigmoid in that it squashes inputs between 0 and 1, but it's also normalized so that all the values sum to one like a proper probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:39.961531Z",
     "start_time": "2021-05-26T22:26:39.946776Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    # Defining the layers, 128, 64, 10 units each\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 10)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the input features are 784? Because the input images have size 28 pixels x 28 pixels for a total of 784 features. Since a Multilayer perceptron accepts only flatten inputs, we need to flatten a 28x28 grid into a 784 array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:41.213448Z",
     "start_time": "2021-05-26T22:26:41.205216Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=784, out_features=4, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=4, out_features=4, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=4, out_features=10, bias=True)\n  (5): Softmax(dim=1)\n)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [4, 4]\n",
    "output_size   = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations. Note that a dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:42.300216Z",
     "start_time": "2021-05-26T22:26:42.289009Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=4, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1',   nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2',   nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "          ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:42.972699Z",
     "start_time": "2021-05-26T22:26:42.963913Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[-0.0114, -0.0202, -0.0168,  ..., -0.0110,  0.0124, -0.0217],\n        [ 0.0202,  0.0298, -0.0162,  ...,  0.0041,  0.0299, -0.0251],\n        [-0.0329, -0.0285,  0.0260,  ...,  0.0167,  0.0040, -0.0192],\n        [-0.0281,  0.0287, -0.0321,  ..., -0.0054, -0.0290, -0.0305]],\n       requires_grad=True)\nParameter containing:\ntensor([0.0293, 0.0064, 0.0017, 0.0029], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom initialization, we want to modify these tensors in place. These are actually autograd *Variables*, so we need to get back the actual tensors with `model.fc1.weight.data`. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:43.889729Z",
     "start_time": "2021-05-26T22:26:43.883940Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Set biases to all zeros\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.084097Z",
     "start_time": "2021-05-26T22:26:44.076738Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0039, -0.0078,  0.0157,  ...,  0.0035,  0.0040,  0.0073],\n",
       "        [ 0.0003,  0.0126,  0.0027,  ...,  0.0068,  0.0208, -0.0007],\n",
       "        [ 0.0068, -0.0080,  0.0131,  ...,  0.0054, -0.0018, -0.0042],\n",
       "        [-0.0080, -0.0017, -0.0007,  ..., -0.0011, -0.0031,  0.0186]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Forward pass\n",
    "\n",
    "Now that we have a network, let's see what happens when we pass in an image. This is called the forward pass. We're going to convert the image data into a tensor, then pass it through the operations defined by the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.506324Z",
     "start_time": "2021-05-26T22:26:44.491847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.596541Z",
     "start_time": "2021-05-26T22:26:44.594169Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:44.851894Z",
     "start_time": "2021-05-26T22:26:44.845888Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=4, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=4, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:46.121246Z",
     "start_time": "2021-05-26T22:26:46.112022Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "        [[-1., -1., -1.,  ..., -1., -1., -1.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to not automatically get batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:46.519895Z",
     "start_time": "2021-05-26T22:26:46.514137Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "img_idx = 0\n",
    "images[img_idx,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:47.945952Z",
     "start_time": "2021-05-26T22:26:47.888846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model(images[img_idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:50.561845Z",
     "start_time": "2021-05-26T22:26:50.411449Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x648 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 349,
       "height": 195
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "img = images[img_idx]\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "The network we built isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is simple to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks, although it's straightforward once you learn about it. \n",
    "\n",
    "This is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/w1_backprop_graph.png' width=400px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from right to left here. To train the weights with gradient descent, we propagate the gradient of the cost backwards through the network. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial w_1} = \\frac{\\partial l_1}{\\partial w_1} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial \\ell}{\\partial l_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "w^\\prime = w - \\alpha \\frac{\\partial \\ell}{\\partial w}\n",
    "$$\n",
    "\n",
    "The learning rate is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Torch provides a module, `autograd`, for automatically calculating the gradient of tensors. It does this by keeping track of operations performed on tensors. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:52.867509Z",
     "start_time": "2021-05-26T22:26:52.860629Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 2.3745, -0.4129],\n        [-0.1983, -0.6832]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:53.383436Z",
     "start_time": "2021-05-26T22:26:53.375536Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[5.6380, 0.1705],\n        [0.0393, 0.4668]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the operation that created `y`, a power operation `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:53.870654Z",
     "start_time": "2021-05-26T22:26:53.867424Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<PowBackward0 object at 0x7f7fe1cc13a0>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autgrad module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor `y` to a scalar value, the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:54.831912Z",
     "start_time": "2021-05-26T22:26:54.824631Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.5787, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the gradients for `x` and `y` but they are empty currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:55.546143Z",
     "start_time": "2021-05-26T22:26:55.541213Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, you need to run the `.backward` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:56.607560Z",
     "start_time": "2021-05-26T22:26:56.594993Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.1872, -0.2065],\n        [-0.0992, -0.3416]])\ntensor([[ 1.1872, -0.2065],\n        [-0.0992, -0.3416]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the cost, then, go backwards to calculate the gradients with respect to the cost. Once we have the gradients we can make a gradient descent step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll build a network with `nn.Sequential` here. Only difference from the last part is I'm not actually using softmax on the output, but instead just using the raw output from the last layer. This is because the output from softmax is a probability distribution. Often, the output will have values really close to zero or really close to one. Due to [inaccuracies with representing numbers as floating points](https://docs.python.org/3/tutorial/floatingpoint.html), computations with a softmax output can lose accuracy and become unstable. To get around this, we'll use the raw output, called the **logits**, to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:56.944759Z",
     "start_time": "2021-05-26T22:26:56.936939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size  = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('logits', nn.Linear(hidden_sizes[1], output_size))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:26:57.317614Z",
     "start_time": "2021-05-26T22:26:57.313022Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider just one learning step before looping through all the data. The general process with PyTorch:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "Below I'll go through one training step and print out the weights and gradients so you can see how it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:07.408433Z",
     "start_time": "2021-05-26T22:27:07.373358Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial weights -  Parameter containing:\ntensor([[-0.0343,  0.0059, -0.0057,  ..., -0.0099,  0.0186,  0.0183],\n        [ 0.0207, -0.0218,  0.0093,  ..., -0.0075,  0.0035, -0.0143],\n        [ 0.0116,  0.0060, -0.0121,  ..., -0.0199, -0.0264, -0.0135],\n        ...,\n        [-0.0151,  0.0120, -0.0035,  ..., -0.0038, -0.0102, -0.0176],\n        [ 0.0223, -0.0168, -0.0147,  ...,  0.0052,  0.0273,  0.0042],\n        [-0.0157,  0.0302, -0.0295,  ...,  0.0151,  0.0284, -0.0260]],\n       requires_grad=True)\nGradient - tensor([[ 0.0009,  0.0009,  0.0009,  ...,  0.0009,  0.0009,  0.0009],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0009,  0.0009,  0.0009,  ...,  0.0009,  0.0009,  0.0009],\n        ...,\n        [ 0.0012,  0.0012,  0.0012,  ...,  0.0012,  0.0012,  0.0012],\n        [-0.0014, -0.0014, -0.0014,  ..., -0.0014, -0.0014, -0.0014],\n        [-0.0087, -0.0087, -0.0087,  ..., -0.0087, -0.0087, -0.0087]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(trainloader.batch_size, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:07.915247Z",
     "start_time": "2021-05-26T22:27:07.908155Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Updated weights -  Parameter containing:\ntensor([[-0.0343,  0.0059, -0.0057,  ..., -0.0100,  0.0186,  0.0183],\n        [ 0.0207, -0.0218,  0.0093,  ..., -0.0075,  0.0035, -0.0143],\n        [ 0.0116,  0.0060, -0.0121,  ..., -0.0200, -0.0264, -0.0136],\n        ...,\n        [-0.0151,  0.0120, -0.0036,  ..., -0.0038, -0.0102, -0.0177],\n        [ 0.0223, -0.0168, -0.0146,  ...,  0.0052,  0.0274,  0.0042],\n        [-0.0157,  0.0303, -0.0294,  ...,  0.0152,  0.0285, -0.0259]],\n       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "Now we'll put this algorithm into a loop so we can go through all the images. This is fairly straightforward. We'll loop through the mini-batches in our dataset, pass the data through the network to calculate the losses, get the gradients, then run the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:08.816179Z",
     "start_time": "2021-05-26T22:27:08.812807Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:27:36.083537Z",
     "start_time": "2021-05-26T22:27:09.280769Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0569\n",
      "\tIteration: 40\t Loss: 2.2960\n",
      "\tIteration: 80\t Loss: 2.2779\n",
      "\tIteration: 120\t Loss: 2.2552\n",
      "\tIteration: 160\t Loss: 2.2415\n",
      "\tIteration: 200\t Loss: 2.2215\n",
      "\tIteration: 240\t Loss: 2.1935\n",
      "\tIteration: 280\t Loss: 2.1661\n",
      "\tIteration: 320\t Loss: 2.1534\n",
      "\tIteration: 360\t Loss: 2.1146\n",
      "\tIteration: 400\t Loss: 2.0878\n",
      "\tIteration: 440\t Loss: 2.0512\n",
      "\tIteration: 480\t Loss: 1.9911\n",
      "\tIteration: 520\t Loss: 1.9362\n",
      "\tIteration: 560\t Loss: 1.8875\n",
      "\tIteration: 600\t Loss: 1.8478\n",
      "\tIteration: 640\t Loss: 1.7953\n",
      "\tIteration: 680\t Loss: 1.7100\n",
      "\tIteration: 720\t Loss: 1.6677\n",
      "\tIteration: 760\t Loss: 1.6005\n",
      "\tIteration: 800\t Loss: 1.5258\n",
      "\tIteration: 840\t Loss: 1.4529\n",
      "\tIteration: 880\t Loss: 1.3806\n",
      "\tIteration: 920\t Loss: 1.2959\n",
      "\tIteration: 960\t Loss: 1.3081\n",
      "\tIteration: 1000\t Loss: 1.2583\n",
      "\tIteration: 1040\t Loss: 1.1695\n",
      "\tIteration: 1080\t Loss: 1.0923\n",
      "\tIteration: 1120\t Loss: 1.1146\n",
      "\tIteration: 1160\t Loss: 1.0587\n",
      "\tIteration: 1200\t Loss: 0.9888\n",
      "\tIteration: 1240\t Loss: 0.9521\n",
      "\tIteration: 1280\t Loss: 0.9027\n",
      "\tIteration: 1320\t Loss: 0.9258\n",
      "\tIteration: 1360\t Loss: 0.9021\n",
      "\tIteration: 1400\t Loss: 0.8434\n",
      "\tIteration: 1440\t Loss: 0.8253\n",
      "\tIteration: 1480\t Loss: 0.7836\n",
      "\tIteration: 1520\t Loss: 0.7946\n",
      "\tIteration: 1560\t Loss: 0.7367\n",
      "\tIteration: 1600\t Loss: 0.7452\n",
      "\tIteration: 1640\t Loss: 0.6925\n",
      "\tIteration: 1680\t Loss: 0.6986\n",
      "\tIteration: 1720\t Loss: 0.7096\n",
      "\tIteration: 1760\t Loss: 0.6701\n",
      "\tIteration: 1800\t Loss: 0.6992\n",
      "\tIteration: 1840\t Loss: 0.6534\n",
      "\tIteration: 1880\t Loss: 0.6300\n",
      "\tIteration: 1920\t Loss: 0.6190\n",
      "\tIteration: 1960\t Loss: 0.6186\n",
      "\tIteration: 2000\t Loss: 0.6119\n",
      "\tIteration: 2040\t Loss: 0.5823\n",
      "\tIteration: 2080\t Loss: 0.5779\n",
      "\tIteration: 2120\t Loss: 0.5817\n",
      "\tIteration: 2160\t Loss: 0.5765\n",
      "\tIteration: 2200\t Loss: 0.6040\n",
      "\tIteration: 2240\t Loss: 0.5645\n",
      "\tIteration: 2280\t Loss: 0.5270\n",
      "\tIteration: 2320\t Loss: 0.5373\n",
      "\tIteration: 2360\t Loss: 0.5403\n",
      "\tIteration: 2400\t Loss: 0.5541\n",
      "\tIteration: 2440\t Loss: 0.5348\n",
      "\tIteration: 2480\t Loss: 0.4662\n",
      "\tIteration: 2520\t Loss: 0.4807\n",
      "\tIteration: 2560\t Loss: 0.5391\n",
      "\tIteration: 2600\t Loss: 0.5358\n",
      "\tIteration: 2640\t Loss: 0.4767\n",
      "\tIteration: 2680\t Loss: 0.5459\n",
      "\tIteration: 2720\t Loss: 0.4858\n",
      "\tIteration: 2760\t Loss: 0.4668\n",
      "\tIteration: 2800\t Loss: 0.4612\n",
      "\tIteration: 2840\t Loss: 0.4698\n",
      "\tIteration: 2880\t Loss: 0.4712\n",
      "\tIteration: 2920\t Loss: 0.5419\n",
      "\tIteration: 2960\t Loss: 0.4457\n",
      "\tIteration: 3000\t Loss: 0.3961\n",
      "\tIteration: 3040\t Loss: 0.5136\n",
      "\tIteration: 3080\t Loss: 0.4521\n",
      "\tIteration: 3120\t Loss: 0.3783\n",
      "\tIteration: 3160\t Loss: 0.4663\n",
      "\tIteration: 3200\t Loss: 0.4895\n",
      "\tIteration: 3240\t Loss: 0.4496\n",
      "\tIteration: 3280\t Loss: 0.4831\n",
      "\tIteration: 3320\t Loss: 0.4122\n",
      "\tIteration: 3360\t Loss: 0.5090\n",
      "\tIteration: 3400\t Loss: 0.4456\n",
      "\tIteration: 3440\t Loss: 0.4521\n",
      "\tIteration: 3480\t Loss: 0.4633\n",
      "\tIteration: 3520\t Loss: 0.4847\n",
      "\tIteration: 3560\t Loss: 0.4112\n",
      "\tIteration: 3600\t Loss: 0.4260\n",
      "\tIteration: 3640\t Loss: 0.4032\n",
      "\tIteration: 3680\t Loss: 0.4106\n",
      "\tIteration: 3720\t Loss: 0.4016\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0039\n",
      "\tIteration: 40\t Loss: 0.4050\n",
      "\tIteration: 80\t Loss: 0.4890\n",
      "\tIteration: 120\t Loss: 0.3983\n",
      "\tIteration: 160\t Loss: 0.4395\n",
      "\tIteration: 200\t Loss: 0.4549\n",
      "\tIteration: 240\t Loss: 0.3831\n",
      "\tIteration: 280\t Loss: 0.4155\n",
      "\tIteration: 320\t Loss: 0.4529\n",
      "\tIteration: 360\t Loss: 0.3587\n",
      "\tIteration: 400\t Loss: 0.3808\n",
      "\tIteration: 440\t Loss: 0.3403\n",
      "\tIteration: 480\t Loss: 0.4209\n",
      "\tIteration: 520\t Loss: 0.3828\n",
      "\tIteration: 560\t Loss: 0.4108\n",
      "\tIteration: 600\t Loss: 0.3863\n",
      "\tIteration: 640\t Loss: 0.4287\n",
      "\tIteration: 680\t Loss: 0.3377\n",
      "\tIteration: 720\t Loss: 0.3890\n",
      "\tIteration: 760\t Loss: 0.4042\n",
      "\tIteration: 800\t Loss: 0.3472\n",
      "\tIteration: 840\t Loss: 0.4135\n",
      "\tIteration: 880\t Loss: 0.3928\n",
      "\tIteration: 920\t Loss: 0.4565\n",
      "\tIteration: 960\t Loss: 0.3775\n",
      "\tIteration: 1000\t Loss: 0.3481\n",
      "\tIteration: 1040\t Loss: 0.4352\n",
      "\tIteration: 1080\t Loss: 0.4468\n",
      "\tIteration: 1120\t Loss: 0.4009\n",
      "\tIteration: 1160\t Loss: 0.3112\n",
      "\tIteration: 1200\t Loss: 0.3350\n",
      "\tIteration: 1240\t Loss: 0.4213\n",
      "\tIteration: 1280\t Loss: 0.4632\n",
      "\tIteration: 1320\t Loss: 0.3578\n",
      "\tIteration: 1360\t Loss: 0.3829\n",
      "\tIteration: 1400\t Loss: 0.3829\n",
      "\tIteration: 1440\t Loss: 0.3893\n",
      "\tIteration: 1480\t Loss: 0.3752\n",
      "\tIteration: 1520\t Loss: 0.3748\n",
      "\tIteration: 1560\t Loss: 0.3609\n",
      "\tIteration: 1600\t Loss: 0.4301\n",
      "\tIteration: 1640\t Loss: 0.4134\n",
      "\tIteration: 1680\t Loss: 0.4390\n",
      "\tIteration: 1720\t Loss: 0.3528\n",
      "\tIteration: 1760\t Loss: 0.3421\n",
      "\tIteration: 1800\t Loss: 0.3185\n",
      "\tIteration: 1840\t Loss: 0.3844\n",
      "\tIteration: 1880\t Loss: 0.3581\n",
      "\tIteration: 1920\t Loss: 0.3785\n",
      "\tIteration: 1960\t Loss: 0.3444\n",
      "\tIteration: 2000\t Loss: 0.3542\n",
      "\tIteration: 2040\t Loss: 0.3629\n",
      "\tIteration: 2080\t Loss: 0.3491\n",
      "\tIteration: 2120\t Loss: 0.3194\n",
      "\tIteration: 2160\t Loss: 0.3544\n",
      "\tIteration: 2200\t Loss: 0.3619\n",
      "\tIteration: 2240\t Loss: 0.2996\n",
      "\tIteration: 2280\t Loss: 0.3344\n",
      "\tIteration: 2320\t Loss: 0.3210\n",
      "\tIteration: 2360\t Loss: 0.3161\n",
      "\tIteration: 2400\t Loss: 0.3858\n",
      "\tIteration: 2440\t Loss: 0.3774\n",
      "\tIteration: 2480\t Loss: 0.3879\n",
      "\tIteration: 2520\t Loss: 0.3035\n",
      "\tIteration: 2560\t Loss: 0.3516\n",
      "\tIteration: 2600\t Loss: 0.3660\n",
      "\tIteration: 2640\t Loss: 0.3706\n",
      "\tIteration: 2680\t Loss: 0.3645\n",
      "\tIteration: 2720\t Loss: 0.4043\n",
      "\tIteration: 2760\t Loss: 0.4306\n",
      "\tIteration: 2800\t Loss: 0.3419\n",
      "\tIteration: 2840\t Loss: 0.3388\n",
      "\tIteration: 2880\t Loss: 0.3139\n",
      "\tIteration: 2920\t Loss: 0.3479\n",
      "\tIteration: 2960\t Loss: 0.3510\n",
      "\tIteration: 3000\t Loss: 0.3993\n",
      "\tIteration: 3040\t Loss: 0.3430\n",
      "\tIteration: 3080\t Loss: 0.4329\n",
      "\tIteration: 3120\t Loss: 0.2951\n",
      "\tIteration: 3160\t Loss: 0.3190\n",
      "\tIteration: 3200\t Loss: 0.3496\n",
      "\tIteration: 3240\t Loss: 0.3412\n",
      "\tIteration: 3280\t Loss: 0.3478\n",
      "\tIteration: 3320\t Loss: 0.3647\n",
      "\tIteration: 3360\t Loss: 0.3210\n",
      "\tIteration: 3400\t Loss: 0.2574\n",
      "\tIteration: 3440\t Loss: 0.3792\n",
      "\tIteration: 3480\t Loss: 0.3546\n",
      "\tIteration: 3520\t Loss: 0.3487\n",
      "\tIteration: 3560\t Loss: 0.3055\n",
      "\tIteration: 3600\t Loss: 0.3070\n",
      "\tIteration: 3640\t Loss: 0.3179\n",
      "\tIteration: 3680\t Loss: 0.2618\n",
      "\tIteration: 3720\t Loss: 0.3683\n",
      "Epoch: 3/3\n",
      "\tIteration: 0\t Loss: 0.0019\n",
      "\tIteration: 40\t Loss: 0.3340\n",
      "\tIteration: 80\t Loss: 0.3228\n",
      "\tIteration: 120\t Loss: 0.3324\n",
      "\tIteration: 160\t Loss: 0.3764\n",
      "\tIteration: 200\t Loss: 0.2613\n",
      "\tIteration: 240\t Loss: 0.3224\n",
      "\tIteration: 280\t Loss: 0.3422\n",
      "\tIteration: 320\t Loss: 0.2901\n",
      "\tIteration: 360\t Loss: 0.2846\n",
      "\tIteration: 400\t Loss: 0.3004\n",
      "\tIteration: 440\t Loss: 0.2985\n",
      "\tIteration: 480\t Loss: 0.3366\n",
      "\tIteration: 520\t Loss: 0.2911\n",
      "\tIteration: 560\t Loss: 0.3247\n",
      "\tIteration: 600\t Loss: 0.3292\n",
      "\tIteration: 640\t Loss: 0.3662\n",
      "\tIteration: 680\t Loss: 0.3583\n",
      "\tIteration: 720\t Loss: 0.3156\n",
      "\tIteration: 760\t Loss: 0.3060\n",
      "\tIteration: 800\t Loss: 0.2959\n",
      "\tIteration: 840\t Loss: 0.2871\n",
      "\tIteration: 880\t Loss: 0.3391\n",
      "\tIteration: 920\t Loss: 0.2927\n",
      "\tIteration: 960\t Loss: 0.3670\n",
      "\tIteration: 1000\t Loss: 0.3901\n",
      "\tIteration: 1040\t Loss: 0.2993\n",
      "\tIteration: 1080\t Loss: 0.3280\n",
      "\tIteration: 1120\t Loss: 0.3131\n",
      "\tIteration: 1160\t Loss: 0.3420\n",
      "\tIteration: 1200\t Loss: 0.3047\n",
      "\tIteration: 1240\t Loss: 0.2808\n",
      "\tIteration: 1280\t Loss: 0.3207\n",
      "\tIteration: 1320\t Loss: 0.3568\n",
      "\tIteration: 1360\t Loss: 0.3081\n",
      "\tIteration: 1400\t Loss: 0.3496\n",
      "\tIteration: 1440\t Loss: 0.2994\n",
      "\tIteration: 1480\t Loss: 0.2711\n",
      "\tIteration: 1520\t Loss: 0.2892\n",
      "\tIteration: 1560\t Loss: 0.3562\n",
      "\tIteration: 1600\t Loss: 0.3376\n",
      "\tIteration: 1640\t Loss: 0.3107\n",
      "\tIteration: 1680\t Loss: 0.3281\n",
      "\tIteration: 1720\t Loss: 0.2700\n",
      "\tIteration: 1760\t Loss: 0.3036\n",
      "\tIteration: 1800\t Loss: 0.2795\n",
      "\tIteration: 1840\t Loss: 0.2811\n",
      "\tIteration: 1880\t Loss: 0.3232\n",
      "\tIteration: 1920\t Loss: 0.2837\n",
      "\tIteration: 1960\t Loss: 0.3221\n",
      "\tIteration: 2000\t Loss: 0.3145\n",
      "\tIteration: 2040\t Loss: 0.3358\n",
      "\tIteration: 2080\t Loss: 0.3016\n",
      "\tIteration: 2120\t Loss: 0.4205\n",
      "\tIteration: 2160\t Loss: 0.3273\n",
      "\tIteration: 2200\t Loss: 0.2875\n",
      "\tIteration: 2240\t Loss: 0.3052\n",
      "\tIteration: 2280\t Loss: 0.3265\n",
      "\tIteration: 2320\t Loss: 0.2971\n",
      "\tIteration: 2360\t Loss: 0.3109\n",
      "\tIteration: 2400\t Loss: 0.2868\n",
      "\tIteration: 2440\t Loss: 0.2745\n",
      "\tIteration: 2480\t Loss: 0.2944\n",
      "\tIteration: 2520\t Loss: 0.3115\n",
      "\tIteration: 2560\t Loss: 0.3386\n",
      "\tIteration: 2600\t Loss: 0.2954\n",
      "\tIteration: 2640\t Loss: 0.3036\n",
      "\tIteration: 2680\t Loss: 0.3423\n",
      "\tIteration: 2720\t Loss: 0.3127\n",
      "\tIteration: 2760\t Loss: 0.3163\n",
      "\tIteration: 2800\t Loss: 0.3295\n",
      "\tIteration: 2840\t Loss: 0.2733\n",
      "\tIteration: 2880\t Loss: 0.2752\n",
      "\tIteration: 2920\t Loss: 0.2956\n",
      "\tIteration: 2960\t Loss: 0.3060\n",
      "\tIteration: 3000\t Loss: 0.3238\n",
      "\tIteration: 3040\t Loss: 0.3011\n",
      "\tIteration: 3080\t Loss: 0.2961\n",
      "\tIteration: 3120\t Loss: 0.3503\n",
      "\tIteration: 3160\t Loss: 0.3371\n",
      "\tIteration: 3200\t Loss: 0.2666\n",
      "\tIteration: 3240\t Loss: 0.2882\n",
      "\tIteration: 3280\t Loss: 0.2567\n",
      "\tIteration: 3320\t Loss: 0.3060\n",
      "\tIteration: 3360\t Loss: 0.2760\n",
      "\tIteration: 3400\t Loss: 0.2334\n",
      "\tIteration: 3440\t Loss: 0.3369\n",
      "\tIteration: 3480\t Loss: 0.2529\n",
      "\tIteration: 3520\t Loss: 0.2800\n",
      "\tIteration: 3560\t Loss: 0.3060\n",
      "\tIteration: 3600\t Loss: 0.3204\n",
      "\tIteration: 3640\t Loss: 0.2894\n",
      "\tIteration: 3680\t Loss: 0.3466\n",
      "\tIteration: 3720\t Loss: 0.3115\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:30:00.206666Z",
     "start_time": "2021-05-26T22:29:59.954325Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x648 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 349,
       "height": 195
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our network is brilliant. It can accurately predict the digits in our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">EMNIST Classification: Exercise</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 1:</h3>\n",
    "  <p>Now it's your turn to build a simple network, use any method I've covered so far. In the next notebook, you'll learn how to train a network so it can make good predictions.</p>\n",
    "  <p>Build a network to classify the MNIST images with 3 hidden layers. Use 16 units in the first hidden layer, 32 units in the second layer, and 8 units in the third layer. Each hidden layer should have a ReLU activation function, and use softmax on the output layer.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Exercise_net(\n",
       "  (fc1): Linear(in_features=784, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=8, bias=True)\n",
       "  (fc4): Linear(in_features=8, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "## TODO: Your network here\n",
    "\n",
    "class Exercise_net(nn.Module):\n",
    "    \n",
    "    # Defining the layers, 128, 64, 10 units each\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.fc1 = nn.Linear(784, 16)\n",
    "        # first hidden layer, size = 16\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        #  # second hidden layer, size = 32\n",
    "        self.fc3 = nn.Linear(32, 8)\n",
    "        # third hidden layer, size = 8\n",
    "        self.fc4 = nn.Linear(8, 10)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "\n",
    "        #  input layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        # first hidden layer and activation\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        # second hidden layer and activation\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        # third hidden layer and output layer\n",
    "        x = self.fc4(x)\n",
    "        # x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = Exercise_net()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x648 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 349,
       "height": 195
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Run this cell with your model to make sure it works\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 2:</h3>\n",
    "  <p>Train your network implementing the Pytorch training loop and <strong style=\"color:#01ff84\">after each epoch, use the model for predicting the test (validation) MNIST data.</strong></p>\n",
    "  <p>Note: If your model does not fit with the final softmax layer, you can remove this layer.</p>\n",
    "  <p>Hint: <a href=\"https://discuss.pytorch.org/t/training-loop-checking-validation-accuracy/78399\">Training loop checking validation accuracy\n",
    "</a></p>\n",
    "  <p>Research about <code>model.train()</code>, <code>model.eval()</code> and <code>with torch.no_grad()</code> in Pytorch.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial weights -  Parameter containing:\ntensor([[-0.0140,  0.0130, -0.0026,  ..., -0.0223,  0.0247,  0.0206],\n        [ 0.0213, -0.0213, -0.0314,  ..., -0.0052,  0.0315,  0.0045],\n        [ 0.0354,  0.0253, -0.0091,  ..., -0.0117, -0.0232,  0.0153],\n        ...,\n        [ 0.0260, -0.0208,  0.0226,  ..., -0.0340,  0.0161,  0.0113],\n        [ 0.0220,  0.0291, -0.0211,  ..., -0.0048, -0.0184,  0.0139],\n        [-0.0243,  0.0306,  0.0135,  ...,  0.0309,  0.0144,  0.0311]],\n       requires_grad=True)\nGradient - tensor([[-0.0150, -0.0150, -0.0150,  ..., -0.0150, -0.0150, -0.0150],\n        [-0.0034, -0.0034, -0.0034,  ..., -0.0034, -0.0034, -0.0034],\n        [ 0.0059,  0.0059,  0.0059,  ...,  0.0059,  0.0059,  0.0059],\n        ...,\n        [-0.0046, -0.0046, -0.0046,  ..., -0.0046, -0.0046, -0.0046],\n        [-0.0045, -0.0045, -0.0045,  ..., -0.0045, -0.0045, -0.0045],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# First step: defining criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "# first step for testing pourposes\n",
    "\n",
    "print('Initial weights - ', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(trainloader.batch_size, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(test_loader, model):\n",
    "    acc_list = []\n",
    "    y_preds_list = []\n",
    "    y_true_list = []\n",
    "    for i, (images_test, y_true) in enumerate(iter(test_loader)):\n",
    "        y_preds = []\n",
    "\n",
    "        # Flatten EMNIST images into a 784 long vector\n",
    "        images_test.resize_(images_test.size()[0], 784)\n",
    "        logits = model.forward(images_test)\n",
    "        output_preds = F.softmax(logits, dim=1)\n",
    "        for p in output_preds:\n",
    "            y_preds.append(p.argmax())\n",
    "        \n",
    "        y_preds = np.array(y_preds)\n",
    "        y_preds = torch.tensor(y_preds)\n",
    "\n",
    "        for i in range(y_preds.size(0)):\n",
    "            y_preds_list.append(y_preds[i].item())\n",
    "            y_true_list.append(y_true[i].item())\n",
    "\n",
    "    accuracy = (np.array(y_preds_list) == np.array(y_true_list)).sum()/len(y_preds_list)\n",
    "    print(accuracy)\n",
    "\n",
    "    return accuracy, y_preds_list, y_true_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\t Loss: 2.2870\n",
      "\tIteration: 440\t Loss: 2.2925\n",
      "\tIteration: 480\t Loss: 2.2881\n",
      "\tIteration: 520\t Loss: 2.2823\n",
      "\tIteration: 560\t Loss: 2.2779\n",
      "\tIteration: 600\t Loss: 2.2816\n",
      "\tIteration: 640\t Loss: 2.2850\n",
      "\tIteration: 680\t Loss: 2.2714\n",
      "\tIteration: 720\t Loss: 2.2607\n",
      "\tIteration: 760\t Loss: 2.2527\n",
      "\tIteration: 800\t Loss: 2.2599\n",
      "\tIteration: 840\t Loss: 2.2329\n",
      "\tIteration: 880\t Loss: 2.2466\n",
      "\tIteration: 920\t Loss: 2.2302\n",
      "\tIteration: 960\t Loss: 2.2159\n",
      "\tIteration: 1000\t Loss: 2.2093\n",
      "\tIteration: 1040\t Loss: 2.2180\n",
      "\tIteration: 1080\t Loss: 2.1927\n",
      "\tIteration: 1120\t Loss: 2.1855\n",
      "\tIteration: 1160\t Loss: 2.1696\n",
      "\tIteration: 1200\t Loss: 2.1712\n",
      "\tIteration: 1240\t Loss: 2.1786\n",
      "\tIteration: 1280\t Loss: 2.1607\n",
      "\tIteration: 1320\t Loss: 2.1476\n",
      "\tIteration: 1360\t Loss: 2.1260\n",
      "\tIteration: 1400\t Loss: 2.1161\n",
      "\tIteration: 1440\t Loss: 2.0645\n",
      "\tIteration: 1480\t Loss: 2.1311\n",
      "\tIteration: 1520\t Loss: 2.0718\n",
      "\tIteration: 1560\t Loss: 2.0411\n",
      "\tIteration: 1600\t Loss: 2.0223\n",
      "\tIteration: 1640\t Loss: 1.9849\n",
      "\tIteration: 1680\t Loss: 1.9610\n",
      "\tIteration: 1720\t Loss: 1.9430\n",
      "\tIteration: 1760\t Loss: 1.9399\n",
      "\tIteration: 1800\t Loss: 1.9242\n",
      "\tIteration: 1840\t Loss: 1.8842\n",
      "\tIteration: 1880\t Loss: 1.8354\n",
      "\tIteration: 1920\t Loss: 1.8595\n",
      "\tIteration: 1960\t Loss: 1.7579\n",
      "\tIteration: 2000\t Loss: 1.7676\n",
      "\tIteration: 2040\t Loss: 1.7355\n",
      "\tIteration: 2080\t Loss: 1.6946\n",
      "\tIteration: 2120\t Loss: 1.6368\n",
      "\tIteration: 2160\t Loss: 1.6397\n",
      "\tIteration: 2200\t Loss: 1.5905\n",
      "\tIteration: 2240\t Loss: 1.6200\n",
      "\tIteration: 2280\t Loss: 1.5741\n",
      "\tIteration: 2320\t Loss: 1.5100\n",
      "\tIteration: 2360\t Loss: 1.4773\n",
      "\tIteration: 2400\t Loss: 1.4505\n",
      "\tIteration: 2440\t Loss: 1.4632\n",
      "\tIteration: 2480\t Loss: 1.4761\n",
      "\tIteration: 2520\t Loss: 1.3945\n",
      "\tIteration: 2560\t Loss: 1.3898\n",
      "\tIteration: 2600\t Loss: 1.3080\n",
      "\tIteration: 2640\t Loss: 1.2888\n",
      "\tIteration: 2680\t Loss: 1.3202\n",
      "\tIteration: 2720\t Loss: 1.2328\n",
      "\tIteration: 2760\t Loss: 1.2308\n",
      "\tIteration: 2800\t Loss: 1.2415\n",
      "\tIteration: 2840\t Loss: 1.1829\n",
      "\tIteration: 2880\t Loss: 1.1428\n",
      "\tIteration: 2920\t Loss: 1.0624\n",
      "\tIteration: 2960\t Loss: 1.0944\n",
      "\tIteration: 3000\t Loss: 1.0327\n",
      "\tIteration: 3040\t Loss: 1.0027\n",
      "\tIteration: 3080\t Loss: 1.0022\n",
      "\tIteration: 3120\t Loss: 1.0818\n",
      "\tIteration: 3160\t Loss: 0.9770\n",
      "\tIteration: 3200\t Loss: 0.9786\n",
      "\tIteration: 3240\t Loss: 0.9064\n",
      "\tIteration: 3280\t Loss: 0.9633\n",
      "\tIteration: 3320\t Loss: 0.9153\n",
      "\tIteration: 3360\t Loss: 0.9043\n",
      "\tIteration: 3400\t Loss: 0.9238\n",
      "\tIteration: 3440\t Loss: 0.8639\n",
      "\tIteration: 3480\t Loss: 0.8585\n",
      "\tIteration: 3520\t Loss: 0.7892\n",
      "\tIteration: 3560\t Loss: 0.7820\n",
      "\tIteration: 3600\t Loss: 0.7833\n",
      "\tIteration: 3640\t Loss: 0.7728\n",
      "\tIteration: 3680\t Loss: 0.8479\n",
      "\tIteration: 3720\t Loss: 0.8292\n",
      "0.7398\n",
      "Epoch: 2/7\n",
      "\tIteration: 0\t Loss: 0.0224\n",
      "\tIteration: 40\t Loss: 0.8548\n",
      "\tIteration: 80\t Loss: 0.8087\n",
      "\tIteration: 120\t Loss: 0.7868\n",
      "\tIteration: 160\t Loss: 0.7819\n",
      "\tIteration: 200\t Loss: 0.7163\n",
      "\tIteration: 240\t Loss: 0.8182\n",
      "\tIteration: 280\t Loss: 0.7368\n",
      "\tIteration: 320\t Loss: 0.6943\n",
      "\tIteration: 360\t Loss: 0.6828\n",
      "\tIteration: 400\t Loss: 0.7610\n",
      "\tIteration: 440\t Loss: 0.6818\n",
      "\tIteration: 480\t Loss: 0.6433\n",
      "\tIteration: 520\t Loss: 0.6877\n",
      "\tIteration: 560\t Loss: 0.7618\n",
      "\tIteration: 600\t Loss: 0.6587\n",
      "\tIteration: 640\t Loss: 0.6989\n",
      "\tIteration: 680\t Loss: 0.7411\n",
      "\tIteration: 720\t Loss: 0.6330\n",
      "\tIteration: 760\t Loss: 0.6506\n",
      "\tIteration: 800\t Loss: 0.6840\n",
      "\tIteration: 840\t Loss: 0.6723\n",
      "\tIteration: 880\t Loss: 0.6550\n",
      "\tIteration: 920\t Loss: 0.6779\n",
      "\tIteration: 960\t Loss: 0.6983\n",
      "\tIteration: 1000\t Loss: 0.6625\n",
      "\tIteration: 1040\t Loss: 0.6210\n",
      "\tIteration: 1080\t Loss: 0.6047\n",
      "\tIteration: 1120\t Loss: 0.7312\n",
      "\tIteration: 1160\t Loss: 0.6558\n",
      "\tIteration: 1200\t Loss: 0.6402\n",
      "\tIteration: 1240\t Loss: 0.6194\n",
      "\tIteration: 1280\t Loss: 0.6395\n",
      "\tIteration: 1320\t Loss: 0.6317\n",
      "\tIteration: 1360\t Loss: 0.5523\n",
      "\tIteration: 1400\t Loss: 0.6343\n",
      "\tIteration: 1440\t Loss: 0.6547\n",
      "\tIteration: 1480\t Loss: 0.6458\n",
      "\tIteration: 1520\t Loss: 0.6227\n",
      "\tIteration: 1560\t Loss: 0.6559\n",
      "\tIteration: 1600\t Loss: 0.6225\n",
      "\tIteration: 1640\t Loss: 0.6782\n",
      "\tIteration: 1680\t Loss: 0.6911\n",
      "\tIteration: 1720\t Loss: 0.5869\n",
      "\tIteration: 1760\t Loss: 0.5767\n",
      "\tIteration: 1800\t Loss: 0.6461\n",
      "\tIteration: 1840\t Loss: 0.6769\n",
      "\tIteration: 1880\t Loss: 0.5184\n",
      "\tIteration: 1920\t Loss: 0.5600\n",
      "\tIteration: 1960\t Loss: 0.6079\n",
      "\tIteration: 2000\t Loss: 0.6224\n",
      "\tIteration: 2040\t Loss: 0.5076\n",
      "\tIteration: 2080\t Loss: 0.6819\n",
      "\tIteration: 2120\t Loss: 0.5689\n",
      "\tIteration: 2160\t Loss: 0.5562\n",
      "\tIteration: 2200\t Loss: 0.5655\n",
      "\tIteration: 2240\t Loss: 0.5825\n",
      "\tIteration: 2280\t Loss: 0.5258\n",
      "\tIteration: 2320\t Loss: 0.6051\n",
      "\tIteration: 2360\t Loss: 0.5396\n",
      "\tIteration: 2400\t Loss: 0.5586\n",
      "\tIteration: 2440\t Loss: 0.5780\n",
      "\tIteration: 2480\t Loss: 0.5721\n",
      "\tIteration: 2520\t Loss: 0.6099\n",
      "\tIteration: 2560\t Loss: 0.6437\n",
      "\tIteration: 2600\t Loss: 0.5642\n",
      "\tIteration: 2640\t Loss: 0.5280\n",
      "\tIteration: 2680\t Loss: 0.5608\n",
      "\tIteration: 2720\t Loss: 0.5960\n",
      "\tIteration: 2760\t Loss: 0.5010\n",
      "\tIteration: 2800\t Loss: 0.6278\n",
      "\tIteration: 2840\t Loss: 0.5515\n",
      "\tIteration: 2880\t Loss: 0.5505\n",
      "\tIteration: 2920\t Loss: 0.5269\n",
      "\tIteration: 2960\t Loss: 0.5854\n",
      "\tIteration: 3000\t Loss: 0.5414\n",
      "\tIteration: 3040\t Loss: 0.6179\n",
      "\tIteration: 3080\t Loss: 0.5189\n",
      "\tIteration: 3120\t Loss: 0.5491\n",
      "\tIteration: 3160\t Loss: 0.5242\n",
      "\tIteration: 3200\t Loss: 0.5374\n",
      "\tIteration: 3240\t Loss: 0.5369\n",
      "\tIteration: 3280\t Loss: 0.5083\n",
      "\tIteration: 3320\t Loss: 0.5003\n",
      "\tIteration: 3360\t Loss: 0.5039\n",
      "\tIteration: 3400\t Loss: 0.5073\n",
      "\tIteration: 3440\t Loss: 0.4869\n",
      "\tIteration: 3480\t Loss: 0.4578\n",
      "\tIteration: 3520\t Loss: 0.4985\n",
      "\tIteration: 3560\t Loss: 0.4684\n",
      "\tIteration: 3600\t Loss: 0.4812\n",
      "\tIteration: 3640\t Loss: 0.5517\n",
      "\tIteration: 3680\t Loss: 0.5315\n",
      "\tIteration: 3720\t Loss: 0.5454\n",
      "0.8506\n",
      "Epoch: 3/7\n",
      "\tIteration: 0\t Loss: 0.0046\n",
      "\tIteration: 40\t Loss: 0.4723\n",
      "\tIteration: 80\t Loss: 0.5352\n",
      "\tIteration: 120\t Loss: 0.5016\n",
      "\tIteration: 160\t Loss: 0.4971\n",
      "\tIteration: 200\t Loss: 0.6044\n",
      "\tIteration: 240\t Loss: 0.5639\n",
      "\tIteration: 280\t Loss: 0.5590\n",
      "\tIteration: 320\t Loss: 0.5019\n",
      "\tIteration: 360\t Loss: 0.5706\n",
      "\tIteration: 400\t Loss: 0.4650\n",
      "\tIteration: 440\t Loss: 0.5297\n",
      "\tIteration: 480\t Loss: 0.5005\n",
      "\tIteration: 520\t Loss: 0.5142\n",
      "\tIteration: 560\t Loss: 0.4527\n",
      "\tIteration: 600\t Loss: 0.5447\n",
      "\tIteration: 640\t Loss: 0.5005\n",
      "\tIteration: 680\t Loss: 0.5220\n",
      "\tIteration: 720\t Loss: 0.5412\n",
      "\tIteration: 760\t Loss: 0.4818\n",
      "\tIteration: 800\t Loss: 0.4443\n",
      "\tIteration: 840\t Loss: 0.4330\n",
      "\tIteration: 880\t Loss: 0.5281\n",
      "\tIteration: 920\t Loss: 0.5337\n",
      "\tIteration: 960\t Loss: 0.4552\n",
      "\tIteration: 1000\t Loss: 0.5187\n",
      "\tIteration: 1040\t Loss: 0.5318\n",
      "\tIteration: 1080\t Loss: 0.5260\n",
      "\tIteration: 1120\t Loss: 0.5701\n",
      "\tIteration: 1160\t Loss: 0.4445\n",
      "\tIteration: 1200\t Loss: 0.4826\n",
      "\tIteration: 1240\t Loss: 0.5176\n",
      "\tIteration: 1280\t Loss: 0.5135\n",
      "\tIteration: 1320\t Loss: 0.4738\n",
      "\tIteration: 1360\t Loss: 0.5460\n",
      "\tIteration: 1400\t Loss: 0.5000\n",
      "\tIteration: 1440\t Loss: 0.5153\n",
      "\tIteration: 1480\t Loss: 0.4835\n",
      "\tIteration: 1520\t Loss: 0.5586\n",
      "\tIteration: 1560\t Loss: 0.4872\n",
      "\tIteration: 1600\t Loss: 0.4336\n",
      "\tIteration: 1640\t Loss: 0.4557\n",
      "\tIteration: 1680\t Loss: 0.4407\n",
      "\tIteration: 1720\t Loss: 0.4420\n",
      "\tIteration: 1760\t Loss: 0.3900\n",
      "\tIteration: 1800\t Loss: 0.5518\n",
      "\tIteration: 1840\t Loss: 0.4588\n",
      "\tIteration: 1880\t Loss: 0.4855\n",
      "\tIteration: 1920\t Loss: 0.4512\n",
      "\tIteration: 1960\t Loss: 0.4487\n",
      "\tIteration: 2000\t Loss: 0.3856\n",
      "\tIteration: 2040\t Loss: 0.5442\n",
      "\tIteration: 2080\t Loss: 0.4883\n",
      "\tIteration: 2120\t Loss: 0.3808\n",
      "\tIteration: 2160\t Loss: 0.4207\n",
      "\tIteration: 2200\t Loss: 0.4508\n",
      "\tIteration: 2240\t Loss: 0.4240\n",
      "\tIteration: 2280\t Loss: 0.5055\n",
      "\tIteration: 2320\t Loss: 0.4112\n",
      "\tIteration: 2360\t Loss: 0.4523\n",
      "\tIteration: 2400\t Loss: 0.4363\n",
      "\tIteration: 2440\t Loss: 0.3909\n",
      "\tIteration: 2480\t Loss: 0.4934\n",
      "\tIteration: 2520\t Loss: 0.4631\n",
      "\tIteration: 2560\t Loss: 0.5276\n",
      "\tIteration: 2600\t Loss: 0.4426\n",
      "\tIteration: 2640\t Loss: 0.4513\n",
      "\tIteration: 2680\t Loss: 0.4475\n",
      "\tIteration: 2720\t Loss: 0.6194\n",
      "\tIteration: 2760\t Loss: 0.4435\n",
      "\tIteration: 2800\t Loss: 0.4240\n",
      "\tIteration: 2840\t Loss: 0.4302\n",
      "\tIteration: 2880\t Loss: 0.4304\n",
      "\tIteration: 2920\t Loss: 0.4392\n",
      "\tIteration: 2960\t Loss: 0.4353\n",
      "\tIteration: 3000\t Loss: 0.4264\n",
      "\tIteration: 3040\t Loss: 0.5060\n",
      "\tIteration: 3080\t Loss: 0.5082\n",
      "\tIteration: 3120\t Loss: 0.4635\n",
      "\tIteration: 3160\t Loss: 0.5293\n",
      "\tIteration: 3200\t Loss: 0.3993\n",
      "\tIteration: 3240\t Loss: 0.4871\n",
      "\tIteration: 3280\t Loss: 0.4752\n",
      "\tIteration: 3320\t Loss: 0.4226\n",
      "\tIteration: 3360\t Loss: 0.4346\n",
      "\tIteration: 3400\t Loss: 0.3813\n",
      "\tIteration: 3440\t Loss: 0.4648\n",
      "\tIteration: 3480\t Loss: 0.3823\n",
      "\tIteration: 3520\t Loss: 0.4392\n",
      "\tIteration: 3560\t Loss: 0.5511\n",
      "\tIteration: 3600\t Loss: 0.4762\n",
      "\tIteration: 3640\t Loss: 0.4029\n",
      "\tIteration: 3680\t Loss: 0.5008\n",
      "\tIteration: 3720\t Loss: 0.4665\n",
      "0.8802\n",
      "Epoch: 4/7\n",
      "\tIteration: 0\t Loss: 0.0267\n",
      "\tIteration: 40\t Loss: 0.4375\n",
      "\tIteration: 80\t Loss: 0.4796\n",
      "\tIteration: 120\t Loss: 0.4296\n",
      "\tIteration: 160\t Loss: 0.4544\n",
      "\tIteration: 200\t Loss: 0.4334\n",
      "\tIteration: 240\t Loss: 0.3485\n",
      "\tIteration: 280\t Loss: 0.4293\n",
      "\tIteration: 320\t Loss: 0.4255\n",
      "\tIteration: 360\t Loss: 0.4079\n",
      "\tIteration: 400\t Loss: 0.5146\n",
      "\tIteration: 440\t Loss: 0.4766\n",
      "\tIteration: 480\t Loss: 0.4150\n",
      "\tIteration: 520\t Loss: 0.4291\n",
      "\tIteration: 560\t Loss: 0.4193\n",
      "\tIteration: 600\t Loss: 0.4933\n",
      "\tIteration: 640\t Loss: 0.3916\n",
      "\tIteration: 680\t Loss: 0.4638\n",
      "\tIteration: 720\t Loss: 0.4509\n",
      "\tIteration: 760\t Loss: 0.4487\n",
      "\tIteration: 800\t Loss: 0.4617\n",
      "\tIteration: 840\t Loss: 0.3957\n",
      "\tIteration: 880\t Loss: 0.3663\n",
      "\tIteration: 920\t Loss: 0.4644\n",
      "\tIteration: 960\t Loss: 0.4364\n",
      "\tIteration: 1000\t Loss: 0.4599\n",
      "\tIteration: 1040\t Loss: 0.4240\n",
      "\tIteration: 1080\t Loss: 0.3797\n",
      "\tIteration: 1120\t Loss: 0.4290\n",
      "\tIteration: 1160\t Loss: 0.3956\n",
      "\tIteration: 1200\t Loss: 0.4391\n",
      "\tIteration: 1240\t Loss: 0.4540\n",
      "\tIteration: 1280\t Loss: 0.5082\n",
      "\tIteration: 1320\t Loss: 0.4504\n",
      "\tIteration: 1360\t Loss: 0.4479\n",
      "\tIteration: 1400\t Loss: 0.3571\n",
      "\tIteration: 1440\t Loss: 0.3860\n",
      "\tIteration: 1480\t Loss: 0.4123\n",
      "\tIteration: 1520\t Loss: 0.4092\n",
      "\tIteration: 1560\t Loss: 0.3587\n",
      "\tIteration: 1600\t Loss: 0.3805\n",
      "\tIteration: 1640\t Loss: 0.4242\n",
      "\tIteration: 1680\t Loss: 0.4881\n",
      "\tIteration: 1720\t Loss: 0.4099\n",
      "\tIteration: 1760\t Loss: 0.3259\n",
      "\tIteration: 1800\t Loss: 0.4867\n",
      "\tIteration: 1840\t Loss: 0.4859\n",
      "\tIteration: 1880\t Loss: 0.3798\n",
      "\tIteration: 1920\t Loss: 0.4144\n",
      "\tIteration: 1960\t Loss: 0.5028\n",
      "\tIteration: 2000\t Loss: 0.4554\n",
      "\tIteration: 2040\t Loss: 0.4788\n",
      "\tIteration: 2080\t Loss: 0.5175\n",
      "\tIteration: 2120\t Loss: 0.3898\n",
      "\tIteration: 2160\t Loss: 0.4110\n",
      "\tIteration: 2200\t Loss: 0.3945\n",
      "\tIteration: 2240\t Loss: 0.3790\n",
      "\tIteration: 2280\t Loss: 0.3875\n",
      "\tIteration: 2320\t Loss: 0.4018\n",
      "\tIteration: 2360\t Loss: 0.4075\n",
      "\tIteration: 2400\t Loss: 0.4214\n",
      "\tIteration: 2440\t Loss: 0.4321\n",
      "\tIteration: 2480\t Loss: 0.3981\n",
      "\tIteration: 2520\t Loss: 0.3921\n",
      "\tIteration: 2560\t Loss: 0.3693\n",
      "\tIteration: 2600\t Loss: 0.3693\n",
      "\tIteration: 2640\t Loss: 0.4169\n",
      "\tIteration: 2680\t Loss: 0.4079\n",
      "\tIteration: 2720\t Loss: 0.4078\n",
      "\tIteration: 2760\t Loss: 0.3984\n",
      "\tIteration: 2800\t Loss: 0.3649\n",
      "\tIteration: 2840\t Loss: 0.3875\n",
      "\tIteration: 2880\t Loss: 0.4530\n",
      "\tIteration: 2920\t Loss: 0.4257\n",
      "\tIteration: 2960\t Loss: 0.3583\n",
      "\tIteration: 3000\t Loss: 0.3738\n",
      "\tIteration: 3040\t Loss: 0.3978\n",
      "\tIteration: 3080\t Loss: 0.3851\n",
      "\tIteration: 3120\t Loss: 0.4361\n",
      "\tIteration: 3160\t Loss: 0.3824\n",
      "\tIteration: 3200\t Loss: 0.4080\n",
      "\tIteration: 3240\t Loss: 0.3985\n",
      "\tIteration: 3280\t Loss: 0.3673\n",
      "\tIteration: 3320\t Loss: 0.4229\n",
      "\tIteration: 3360\t Loss: 0.3728\n",
      "\tIteration: 3400\t Loss: 0.4079\n",
      "\tIteration: 3440\t Loss: 0.3345\n",
      "\tIteration: 3480\t Loss: 0.3774\n",
      "\tIteration: 3520\t Loss: 0.3528\n",
      "\tIteration: 3560\t Loss: 0.3678\n",
      "\tIteration: 3600\t Loss: 0.3786\n",
      "\tIteration: 3640\t Loss: 0.3470\n",
      "\tIteration: 3680\t Loss: 0.3861\n",
      "\tIteration: 3720\t Loss: 0.4844\n",
      "0.8901\n",
      "Epoch: 5/7\n",
      "\tIteration: 0\t Loss: 0.0069\n",
      "\tIteration: 40\t Loss: 0.3624\n",
      "\tIteration: 80\t Loss: 0.4073\n",
      "\tIteration: 120\t Loss: 0.3389\n",
      "\tIteration: 160\t Loss: 0.3987\n",
      "\tIteration: 200\t Loss: 0.3573\n",
      "\tIteration: 240\t Loss: 0.3782\n",
      "\tIteration: 280\t Loss: 0.3557\n",
      "\tIteration: 320\t Loss: 0.4038\n",
      "\tIteration: 360\t Loss: 0.4612\n",
      "\tIteration: 400\t Loss: 0.3864\n",
      "\tIteration: 440\t Loss: 0.3117\n",
      "\tIteration: 480\t Loss: 0.3770\n",
      "\tIteration: 520\t Loss: 0.3675\n",
      "\tIteration: 560\t Loss: 0.3930\n",
      "\tIteration: 600\t Loss: 0.3649\n",
      "\tIteration: 640\t Loss: 0.4515\n",
      "\tIteration: 680\t Loss: 0.3782\n",
      "\tIteration: 720\t Loss: 0.4508\n",
      "\tIteration: 760\t Loss: 0.3997\n",
      "\tIteration: 800\t Loss: 0.4505\n",
      "\tIteration: 840\t Loss: 0.3676\n",
      "\tIteration: 880\t Loss: 0.4031\n",
      "\tIteration: 920\t Loss: 0.3966\n",
      "\tIteration: 960\t Loss: 0.4008\n",
      "\tIteration: 1000\t Loss: 0.3491\n",
      "\tIteration: 1040\t Loss: 0.4169\n",
      "\tIteration: 1080\t Loss: 0.3637\n",
      "\tIteration: 1120\t Loss: 0.3286\n",
      "\tIteration: 1160\t Loss: 0.3912\n",
      "\tIteration: 1200\t Loss: 0.4021\n",
      "\tIteration: 1240\t Loss: 0.4171\n",
      "\tIteration: 1280\t Loss: 0.4143\n",
      "\tIteration: 1320\t Loss: 0.3380\n",
      "\tIteration: 1360\t Loss: 0.3209\n",
      "\tIteration: 1400\t Loss: 0.3579\n",
      "\tIteration: 1440\t Loss: 0.3504\n",
      "\tIteration: 1480\t Loss: 0.4077\n",
      "\tIteration: 1520\t Loss: 0.3471\n",
      "\tIteration: 1560\t Loss: 0.3698\n",
      "\tIteration: 1600\t Loss: 0.4182\n",
      "\tIteration: 1640\t Loss: 0.3512\n",
      "\tIteration: 1680\t Loss: 0.3229\n",
      "\tIteration: 1720\t Loss: 0.3979\n",
      "\tIteration: 1760\t Loss: 0.3838\n",
      "\tIteration: 1800\t Loss: 0.3890\n",
      "\tIteration: 1840\t Loss: 0.3623\n",
      "\tIteration: 1880\t Loss: 0.3682\n",
      "\tIteration: 1920\t Loss: 0.4837\n",
      "\tIteration: 1960\t Loss: 0.3590\n",
      "\tIteration: 2000\t Loss: 0.3307\n",
      "\tIteration: 2040\t Loss: 0.3570\n",
      "\tIteration: 2080\t Loss: 0.3377\n",
      "\tIteration: 2120\t Loss: 0.3825\n",
      "\tIteration: 2160\t Loss: 0.3653\n",
      "\tIteration: 2200\t Loss: 0.2642\n",
      "\tIteration: 2240\t Loss: 0.3581\n",
      "\tIteration: 2280\t Loss: 0.3992\n",
      "\tIteration: 2320\t Loss: 0.4187\n",
      "\tIteration: 2360\t Loss: 0.3707\n",
      "\tIteration: 2400\t Loss: 0.3321\n",
      "\tIteration: 2440\t Loss: 0.3374\n",
      "\tIteration: 2480\t Loss: 0.3633\n",
      "\tIteration: 2520\t Loss: 0.3342\n",
      "\tIteration: 2560\t Loss: 0.3968\n",
      "\tIteration: 2600\t Loss: 0.3799\n",
      "\tIteration: 2640\t Loss: 0.4036\n",
      "\tIteration: 2680\t Loss: 0.3236\n",
      "\tIteration: 2720\t Loss: 0.3325\n",
      "\tIteration: 2760\t Loss: 0.3419\n",
      "\tIteration: 2800\t Loss: 0.3381\n",
      "\tIteration: 2840\t Loss: 0.4157\n",
      "\tIteration: 2880\t Loss: 0.3942\n",
      "\tIteration: 2920\t Loss: 0.3258\n",
      "\tIteration: 2960\t Loss: 0.3810\n",
      "\tIteration: 3000\t Loss: 0.4114\n",
      "\tIteration: 3040\t Loss: 0.3188\n",
      "\tIteration: 3080\t Loss: 0.3083\n",
      "\tIteration: 3120\t Loss: 0.3182\n",
      "\tIteration: 3160\t Loss: 0.3367\n",
      "\tIteration: 3200\t Loss: 0.3596\n",
      "\tIteration: 3240\t Loss: 0.3334\n",
      "\tIteration: 3280\t Loss: 0.3335\n",
      "\tIteration: 3320\t Loss: 0.4012\n",
      "\tIteration: 3360\t Loss: 0.3726\n",
      "\tIteration: 3400\t Loss: 0.2741\n",
      "\tIteration: 3440\t Loss: 0.4068\n",
      "\tIteration: 3480\t Loss: 0.3402\n",
      "\tIteration: 3520\t Loss: 0.4378\n",
      "\tIteration: 3560\t Loss: 0.3575\n",
      "\tIteration: 3600\t Loss: 0.4009\n",
      "\tIteration: 3640\t Loss: 0.3820\n",
      "\tIteration: 3680\t Loss: 0.3602\n",
      "\tIteration: 3720\t Loss: 0.3477\n",
      "0.9033\n",
      "Epoch: 6/7\n",
      "\tIteration: 0\t Loss: 0.0129\n",
      "\tIteration: 40\t Loss: 0.3974\n",
      "\tIteration: 80\t Loss: 0.3036\n",
      "\tIteration: 120\t Loss: 0.3458\n",
      "\tIteration: 160\t Loss: 0.3271\n",
      "\tIteration: 200\t Loss: 0.3533\n",
      "\tIteration: 240\t Loss: 0.3403\n",
      "\tIteration: 280\t Loss: 0.3714\n",
      "\tIteration: 320\t Loss: 0.3436\n",
      "\tIteration: 360\t Loss: 0.3596\n",
      "\tIteration: 400\t Loss: 0.3366\n",
      "\tIteration: 440\t Loss: 0.3847\n",
      "\tIteration: 480\t Loss: 0.3401\n",
      "\tIteration: 520\t Loss: 0.3835\n",
      "\tIteration: 560\t Loss: 0.4156\n",
      "\tIteration: 600\t Loss: 0.3693\n",
      "\tIteration: 640\t Loss: 0.3588\n",
      "\tIteration: 680\t Loss: 0.3075\n",
      "\tIteration: 720\t Loss: 0.3651\n",
      "\tIteration: 760\t Loss: 0.3456\n",
      "\tIteration: 800\t Loss: 0.3021\n",
      "\tIteration: 840\t Loss: 0.3515\n",
      "\tIteration: 880\t Loss: 0.3686\n",
      "\tIteration: 920\t Loss: 0.3576\n",
      "\tIteration: 960\t Loss: 0.3754\n",
      "\tIteration: 1000\t Loss: 0.3411\n",
      "\tIteration: 1040\t Loss: 0.3880\n",
      "\tIteration: 1080\t Loss: 0.2984\n",
      "\tIteration: 1120\t Loss: 0.3093\n",
      "\tIteration: 1160\t Loss: 0.2957\n",
      "\tIteration: 1200\t Loss: 0.3225\n",
      "\tIteration: 1240\t Loss: 0.2766\n",
      "\tIteration: 1280\t Loss: 0.3562\n",
      "\tIteration: 1320\t Loss: 0.3111\n",
      "\tIteration: 1360\t Loss: 0.3221\n",
      "\tIteration: 1400\t Loss: 0.2644\n",
      "\tIteration: 1440\t Loss: 0.3359\n",
      "\tIteration: 1480\t Loss: 0.3282\n",
      "\tIteration: 1520\t Loss: 0.3610\n",
      "\tIteration: 1560\t Loss: 0.4032\n",
      "\tIteration: 1600\t Loss: 0.3196\n",
      "\tIteration: 1640\t Loss: 0.3156\n",
      "\tIteration: 1680\t Loss: 0.3384\n",
      "\tIteration: 1720\t Loss: 0.3883\n",
      "\tIteration: 1760\t Loss: 0.2803\n",
      "\tIteration: 1800\t Loss: 0.3211\n",
      "\tIteration: 1840\t Loss: 0.3222\n",
      "\tIteration: 1880\t Loss: 0.3050\n",
      "\tIteration: 1920\t Loss: 0.2536\n",
      "\tIteration: 1960\t Loss: 0.3622\n",
      "\tIteration: 2000\t Loss: 0.4019\n",
      "\tIteration: 2040\t Loss: 0.3424\n",
      "\tIteration: 2080\t Loss: 0.2667\n",
      "\tIteration: 2120\t Loss: 0.3333\n",
      "\tIteration: 2160\t Loss: 0.3192\n",
      "\tIteration: 2200\t Loss: 0.2952\n",
      "\tIteration: 2240\t Loss: 0.3260\n",
      "\tIteration: 2280\t Loss: 0.3669\n",
      "\tIteration: 2320\t Loss: 0.3150\n",
      "\tIteration: 2360\t Loss: 0.3204\n",
      "\tIteration: 2400\t Loss: 0.3302\n",
      "\tIteration: 2440\t Loss: 0.3531\n",
      "\tIteration: 2480\t Loss: 0.3629\n",
      "\tIteration: 2520\t Loss: 0.3384\n",
      "\tIteration: 2560\t Loss: 0.3316\n",
      "\tIteration: 2600\t Loss: 0.3977\n",
      "\tIteration: 2640\t Loss: 0.2817\n",
      "\tIteration: 2680\t Loss: 0.3197\n",
      "\tIteration: 2720\t Loss: 0.2886\n",
      "\tIteration: 2760\t Loss: 0.3278\n",
      "\tIteration: 2800\t Loss: 0.3465\n",
      "\tIteration: 2840\t Loss: 0.3151\n",
      "\tIteration: 2880\t Loss: 0.3328\n",
      "\tIteration: 2920\t Loss: 0.3344\n",
      "\tIteration: 2960\t Loss: 0.2976\n",
      "\tIteration: 3000\t Loss: 0.3237\n",
      "\tIteration: 3040\t Loss: 0.3783\n",
      "\tIteration: 3080\t Loss: 0.3065\n",
      "\tIteration: 3120\t Loss: 0.3001\n",
      "\tIteration: 3160\t Loss: 0.4224\n",
      "\tIteration: 3200\t Loss: 0.3272\n",
      "\tIteration: 3240\t Loss: 0.3319\n",
      "\tIteration: 3280\t Loss: 0.3990\n",
      "\tIteration: 3320\t Loss: 0.3649\n",
      "\tIteration: 3360\t Loss: 0.3653\n",
      "\tIteration: 3400\t Loss: 0.2570\n",
      "\tIteration: 3440\t Loss: 0.3525\n",
      "\tIteration: 3480\t Loss: 0.3303\n",
      "\tIteration: 3520\t Loss: 0.2387\n",
      "\tIteration: 3560\t Loss: 0.3306\n",
      "\tIteration: 3600\t Loss: 0.3756\n",
      "\tIteration: 3640\t Loss: 0.2861\n",
      "\tIteration: 3680\t Loss: 0.2916\n",
      "\tIteration: 3720\t Loss: 0.3298\n",
      "0.9094\n",
      "Epoch: 7/7\n",
      "\tIteration: 0\t Loss: 0.0044\n",
      "\tIteration: 40\t Loss: 0.3496\n",
      "\tIteration: 80\t Loss: 0.3250\n",
      "\tIteration: 120\t Loss: 0.2774\n",
      "\tIteration: 160\t Loss: 0.2834\n",
      "\tIteration: 200\t Loss: 0.2981\n",
      "\tIteration: 240\t Loss: 0.2944\n",
      "\tIteration: 280\t Loss: 0.2983\n",
      "\tIteration: 320\t Loss: 0.3116\n",
      "\tIteration: 360\t Loss: 0.3190\n",
      "\tIteration: 400\t Loss: 0.3037\n",
      "\tIteration: 440\t Loss: 0.3099\n",
      "\tIteration: 480\t Loss: 0.3149\n",
      "\tIteration: 520\t Loss: 0.2883\n",
      "\tIteration: 560\t Loss: 0.2866\n",
      "\tIteration: 600\t Loss: 0.2939\n",
      "\tIteration: 640\t Loss: 0.3619\n",
      "\tIteration: 680\t Loss: 0.3169\n",
      "\tIteration: 720\t Loss: 0.3330\n",
      "\tIteration: 760\t Loss: 0.2768\n",
      "\tIteration: 800\t Loss: 0.3386\n",
      "\tIteration: 840\t Loss: 0.3205\n",
      "\tIteration: 880\t Loss: 0.3225\n",
      "\tIteration: 920\t Loss: 0.3139\n",
      "\tIteration: 960\t Loss: 0.3852\n",
      "\tIteration: 1000\t Loss: 0.3238\n",
      "\tIteration: 1040\t Loss: 0.2568\n",
      "\tIteration: 1080\t Loss: 0.2995\n",
      "\tIteration: 1120\t Loss: 0.3069\n",
      "\tIteration: 1160\t Loss: 0.2941\n",
      "\tIteration: 1200\t Loss: 0.3166\n",
      "\tIteration: 1240\t Loss: 0.2896\n",
      "\tIteration: 1280\t Loss: 0.2802\n",
      "\tIteration: 1320\t Loss: 0.3491\n",
      "\tIteration: 1360\t Loss: 0.2690\n",
      "\tIteration: 1400\t Loss: 0.3092\n",
      "\tIteration: 1440\t Loss: 0.2954\n",
      "\tIteration: 1480\t Loss: 0.3432\n",
      "\tIteration: 1520\t Loss: 0.3333\n",
      "\tIteration: 1560\t Loss: 0.3295\n",
      "\tIteration: 1600\t Loss: 0.2645\n",
      "\tIteration: 1640\t Loss: 0.2800\n",
      "\tIteration: 1680\t Loss: 0.3385\n",
      "\tIteration: 1720\t Loss: 0.2808\n",
      "\tIteration: 1760\t Loss: 0.2770\n",
      "\tIteration: 1800\t Loss: 0.2791\n",
      "\tIteration: 1840\t Loss: 0.3132\n",
      "\tIteration: 1880\t Loss: 0.3440\n",
      "\tIteration: 1920\t Loss: 0.3079\n",
      "\tIteration: 1960\t Loss: 0.2758\n",
      "\tIteration: 2000\t Loss: 0.2737\n",
      "\tIteration: 2040\t Loss: 0.2937\n",
      "\tIteration: 2080\t Loss: 0.2695\n",
      "\tIteration: 2120\t Loss: 0.2611\n",
      "\tIteration: 2160\t Loss: 0.3158\n",
      "\tIteration: 2200\t Loss: 0.2742\n",
      "\tIteration: 2240\t Loss: 0.3369\n",
      "\tIteration: 2280\t Loss: 0.2983\n",
      "\tIteration: 2320\t Loss: 0.3220\n",
      "\tIteration: 2360\t Loss: 0.3270\n",
      "\tIteration: 2400\t Loss: 0.3099\n",
      "\tIteration: 2440\t Loss: 0.3542\n",
      "\tIteration: 2480\t Loss: 0.2789\n",
      "\tIteration: 2520\t Loss: 0.3419\n",
      "\tIteration: 2560\t Loss: 0.2909\n",
      "\tIteration: 2600\t Loss: 0.3115\n",
      "\tIteration: 2640\t Loss: 0.3191\n",
      "\tIteration: 2680\t Loss: 0.3078\n",
      "\tIteration: 2720\t Loss: 0.3232\n",
      "\tIteration: 2760\t Loss: 0.2897\n",
      "\tIteration: 2800\t Loss: 0.3655\n",
      "\tIteration: 2840\t Loss: 0.2749\n",
      "\tIteration: 2880\t Loss: 0.2476\n",
      "\tIteration: 2920\t Loss: 0.3438\n",
      "\tIteration: 2960\t Loss: 0.3241\n",
      "\tIteration: 3000\t Loss: 0.2943\n",
      "\tIteration: 3040\t Loss: 0.3326\n",
      "\tIteration: 3080\t Loss: 0.2870\n",
      "\tIteration: 3120\t Loss: 0.2810\n",
      "\tIteration: 3160\t Loss: 0.3330\n",
      "\tIteration: 3200\t Loss: 0.3126\n",
      "\tIteration: 3240\t Loss: 0.3115\n",
      "\tIteration: 3280\t Loss: 0.3138\n",
      "\tIteration: 3320\t Loss: 0.3472\n",
      "\tIteration: 3360\t Loss: 0.2958\n",
      "\tIteration: 3400\t Loss: 0.2790\n",
      "\tIteration: 3440\t Loss: 0.2635\n",
      "\tIteration: 3480\t Loss: 0.3218\n",
      "\tIteration: 3520\t Loss: 0.3090\n",
      "\tIteration: 3560\t Loss: 0.3481\n",
      "\tIteration: 3600\t Loss: 0.2856\n",
      "\tIteration: 3640\t Loss: 0.3160\n",
      "\tIteration: 3680\t Loss: 0.2758\n",
      "\tIteration: 3720\t Loss: 0.2338\n",
      "0.9178\n"
     ]
    }
   ],
   "source": [
    "## TODO: Your training loop here\n",
    "\n",
    "'''\n",
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True)\n",
    "'''\n",
    "\n",
    "epochs = 7\n",
    "print_every = 40\n",
    "accs_test = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten EMNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        acc, y_pred, y_true = check_accuracy(testloader, model)\n",
    "        accs_test.append(acc)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.7398, 0.8506, 0.8802, 0.8901, 0.9033, 0.9094, 0.9178]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "accs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x648 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 349,
       "height": 195
      },
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Run this cell with your model to make sure it works and predicts well for the validation data\n",
    "images, labels = next(iter(testloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = F.softmax(model.forward(images[0,:]), dim=1)\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 3:</h3>\n",
    "  <p>Write the code for adding <strong style=\"color:#01ff84\">Early Stopping with patience = 2</strong> to the training loop from scratch.</p>\n",
    "  <p><strong style=\"color:#01ff84\">Hint:</strong> Monitor the Validation loss every epoch, and if in 2 epochs, the validation loss does not improve, stop the training loop with <code>break</code>.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initial weights -  Parameter containing:\ntensor([[ 0.0228, -0.0196,  0.0173,  ...,  0.0156, -0.0271, -0.0253],\n        [-0.0248, -0.0218, -0.0295,  ...,  0.0113, -0.0344, -0.0208],\n        [ 0.0286,  0.0266,  0.0355,  ..., -0.0199, -0.0014, -0.0195],\n        ...,\n        [-0.0174, -0.0344,  0.0190,  ..., -0.0158,  0.0049,  0.0092],\n        [-0.0149, -0.0051,  0.0154,  ...,  0.0307, -0.0280,  0.0353],\n        [ 0.0098,  0.0327,  0.0351,  ...,  0.0166, -0.0267, -0.0189]],\n       requires_grad=True)\nGradient - tensor([[-0.1076, -0.1076, -0.1076,  ..., -0.1076, -0.1076, -0.1076],\n        [ 0.0504,  0.0504,  0.0504,  ...,  0.0504,  0.0504,  0.0504],\n        [ 0.0191,  0.0191,  0.0191,  ...,  0.0191,  0.0191,  0.0191],\n        ...,\n        [-0.0011, -0.0011, -0.0011,  ..., -0.0011, -0.0011, -0.0011],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0677,  0.0677,  0.0677,  ...,  0.0677,  0.0677,  0.0677]])\n"
     ]
    }
   ],
   "source": [
    "model2 = Exercise_net()\n",
    "\n",
    "# First step: defining criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "# first step for testing pourposes\n",
    "\n",
    "print('Initial weights - ', model2.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(trainloader.batch_size, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model2.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ss: 2.2983\n",
      "\tIteration: 440\t Loss: 2.2866\n",
      "\tIteration: 480\t Loss: 2.2782\n",
      "\tIteration: 520\t Loss: 2.2784\n",
      "\tIteration: 560\t Loss: 2.2709\n",
      "\tIteration: 600\t Loss: 2.2592\n",
      "\tIteration: 640\t Loss: 2.2623\n",
      "\tIteration: 680\t Loss: 2.2439\n",
      "\tIteration: 720\t Loss: 2.2470\n",
      "\tIteration: 760\t Loss: 2.2249\n",
      "\tIteration: 800\t Loss: 2.2233\n",
      "\tIteration: 840\t Loss: 2.2064\n",
      "\tIteration: 880\t Loss: 2.2108\n",
      "\tIteration: 920\t Loss: 2.1946\n",
      "\tIteration: 960\t Loss: 2.1641\n",
      "\tIteration: 1000\t Loss: 2.1701\n",
      "\tIteration: 1040\t Loss: 2.1396\n",
      "\tIteration: 1080\t Loss: 2.1551\n",
      "\tIteration: 1120\t Loss: 2.1156\n",
      "\tIteration: 1160\t Loss: 2.1106\n",
      "\tIteration: 1200\t Loss: 2.0658\n",
      "\tIteration: 1240\t Loss: 2.0778\n",
      "\tIteration: 1280\t Loss: 2.0626\n",
      "\tIteration: 1320\t Loss: 2.0572\n",
      "\tIteration: 1360\t Loss: 2.0044\n",
      "\tIteration: 1400\t Loss: 1.9932\n",
      "\tIteration: 1440\t Loss: 1.9970\n",
      "\tIteration: 1480\t Loss: 1.9682\n",
      "\tIteration: 1520\t Loss: 1.9659\n",
      "\tIteration: 1560\t Loss: 1.9469\n",
      "\tIteration: 1600\t Loss: 1.9532\n",
      "\tIteration: 1640\t Loss: 1.9126\n",
      "\tIteration: 1680\t Loss: 1.9300\n",
      "\tIteration: 1720\t Loss: 1.9125\n",
      "\tIteration: 1760\t Loss: 1.9161\n",
      "\tIteration: 1800\t Loss: 1.8616\n",
      "\tIteration: 1840\t Loss: 1.8123\n",
      "\tIteration: 1880\t Loss: 1.8303\n",
      "\tIteration: 1920\t Loss: 1.8430\n",
      "\tIteration: 1960\t Loss: 1.7970\n",
      "\tIteration: 2000\t Loss: 1.8005\n",
      "\tIteration: 2040\t Loss: 1.7665\n",
      "\tIteration: 2080\t Loss: 1.7437\n",
      "\tIteration: 2120\t Loss: 1.7290\n",
      "\tIteration: 2160\t Loss: 1.6864\n",
      "\tIteration: 2200\t Loss: 1.7376\n",
      "\tIteration: 2240\t Loss: 1.7318\n",
      "\tIteration: 2280\t Loss: 1.6973\n",
      "\tIteration: 2320\t Loss: 1.6443\n",
      "\tIteration: 2360\t Loss: 1.6425\n",
      "\tIteration: 2400\t Loss: 1.6552\n",
      "\tIteration: 2440\t Loss: 1.5962\n",
      "\tIteration: 2480\t Loss: 1.6124\n",
      "\tIteration: 2520\t Loss: 1.5999\n",
      "\tIteration: 2560\t Loss: 1.6234\n",
      "\tIteration: 2600\t Loss: 1.5944\n",
      "\tIteration: 2640\t Loss: 1.5947\n",
      "\tIteration: 2680\t Loss: 1.5432\n",
      "\tIteration: 2720\t Loss: 1.4913\n",
      "\tIteration: 2760\t Loss: 1.5065\n",
      "\tIteration: 2800\t Loss: 1.4798\n",
      "\tIteration: 2840\t Loss: 1.5318\n",
      "\tIteration: 2880\t Loss: 1.4910\n",
      "\tIteration: 2920\t Loss: 1.4282\n",
      "\tIteration: 2960\t Loss: 1.4496\n",
      "\tIteration: 3000\t Loss: 1.4432\n",
      "\tIteration: 3040\t Loss: 1.3757\n",
      "\tIteration: 3080\t Loss: 1.4546\n",
      "\tIteration: 3120\t Loss: 1.4084\n",
      "\tIteration: 3160\t Loss: 1.4238\n",
      "\tIteration: 3200\t Loss: 1.4062\n",
      "\tIteration: 3240\t Loss: 1.4033\n",
      "\tIteration: 3280\t Loss: 1.3907\n",
      "\tIteration: 3320\t Loss: 1.4061\n",
      "\tIteration: 3360\t Loss: 1.3264\n",
      "\tIteration: 3400\t Loss: 1.3236\n",
      "\tIteration: 3440\t Loss: 1.3267\n",
      "\tIteration: 3480\t Loss: 1.3199\n",
      "\tIteration: 3520\t Loss: 1.3436\n",
      "\tIteration: 3560\t Loss: 1.2915\n",
      "\tIteration: 3600\t Loss: 1.2214\n",
      "\tIteration: 3640\t Loss: 1.2084\n",
      "\tIteration: 3680\t Loss: 1.2594\n",
      "\tIteration: 3720\t Loss: 1.2600\n",
      "0.5829\n",
      "Epoch: 2/10\n",
      "\tIteration: 0\t Loss: 0.0381\n",
      "\tIteration: 40\t Loss: 1.0853\n",
      "\tIteration: 80\t Loss: 1.1238\n",
      "\tIteration: 120\t Loss: 1.1970\n",
      "\tIteration: 160\t Loss: 1.1726\n",
      "\tIteration: 200\t Loss: 1.2205\n",
      "\tIteration: 240\t Loss: 1.1431\n",
      "\tIteration: 280\t Loss: 1.1330\n",
      "\tIteration: 320\t Loss: 1.0930\n",
      "\tIteration: 360\t Loss: 1.1333\n",
      "\tIteration: 400\t Loss: 1.0909\n",
      "\tIteration: 440\t Loss: 1.0641\n",
      "\tIteration: 480\t Loss: 1.0627\n",
      "\tIteration: 520\t Loss: 0.9799\n",
      "\tIteration: 560\t Loss: 1.0724\n",
      "\tIteration: 600\t Loss: 0.9674\n",
      "\tIteration: 640\t Loss: 0.9789\n",
      "\tIteration: 680\t Loss: 0.9706\n",
      "\tIteration: 720\t Loss: 1.0286\n",
      "\tIteration: 760\t Loss: 0.9538\n",
      "\tIteration: 800\t Loss: 0.9151\n",
      "\tIteration: 840\t Loss: 1.0461\n",
      "\tIteration: 880\t Loss: 0.9092\n",
      "\tIteration: 920\t Loss: 0.9400\n",
      "\tIteration: 960\t Loss: 0.9323\n",
      "\tIteration: 1000\t Loss: 0.9686\n",
      "\tIteration: 1040\t Loss: 0.9260\n",
      "\tIteration: 1080\t Loss: 0.9096\n",
      "\tIteration: 1120\t Loss: 0.9767\n",
      "\tIteration: 1160\t Loss: 0.9362\n",
      "\tIteration: 1200\t Loss: 0.9341\n",
      "\tIteration: 1240\t Loss: 0.9081\n",
      "\tIteration: 1280\t Loss: 0.8868\n",
      "\tIteration: 1320\t Loss: 0.9040\n",
      "\tIteration: 1360\t Loss: 0.8352\n",
      "\tIteration: 1400\t Loss: 0.8588\n",
      "\tIteration: 1440\t Loss: 0.8142\n",
      "\tIteration: 1480\t Loss: 0.8577\n",
      "\tIteration: 1520\t Loss: 0.8573\n",
      "\tIteration: 1560\t Loss: 0.8389\n",
      "\tIteration: 1600\t Loss: 0.8922\n",
      "\tIteration: 1640\t Loss: 0.8425\n",
      "\tIteration: 1680\t Loss: 0.8170\n",
      "\tIteration: 1720\t Loss: 0.6676\n",
      "\tIteration: 1760\t Loss: 0.7884\n",
      "\tIteration: 1800\t Loss: 0.8117\n",
      "\tIteration: 1840\t Loss: 0.8343\n",
      "\tIteration: 1880\t Loss: 0.7374\n",
      "\tIteration: 1920\t Loss: 0.7085\n",
      "\tIteration: 1960\t Loss: 0.6811\n",
      "\tIteration: 2000\t Loss: 0.7803\n",
      "\tIteration: 2040\t Loss: 0.7741\n",
      "\tIteration: 2080\t Loss: 0.6789\n",
      "\tIteration: 2120\t Loss: 0.6744\n",
      "\tIteration: 2160\t Loss: 0.7047\n",
      "\tIteration: 2200\t Loss: 0.7115\n",
      "\tIteration: 2240\t Loss: 0.7158\n",
      "\tIteration: 2280\t Loss: 0.7000\n",
      "\tIteration: 2320\t Loss: 0.7437\n",
      "\tIteration: 2360\t Loss: 0.7554\n",
      "\tIteration: 2400\t Loss: 0.6752\n",
      "\tIteration: 2440\t Loss: 0.6944\n",
      "\tIteration: 2480\t Loss: 0.7207\n",
      "\tIteration: 2520\t Loss: 0.6351\n",
      "\tIteration: 2560\t Loss: 0.7642\n",
      "\tIteration: 2600\t Loss: 0.7066\n",
      "\tIteration: 2640\t Loss: 0.6508\n",
      "\tIteration: 2680\t Loss: 0.6452\n",
      "\tIteration: 2720\t Loss: 0.5845\n",
      "\tIteration: 2760\t Loss: 0.7171\n",
      "\tIteration: 2800\t Loss: 0.5612\n",
      "\tIteration: 2840\t Loss: 0.6598\n",
      "\tIteration: 2880\t Loss: 0.6477\n",
      "\tIteration: 2920\t Loss: 0.5710\n",
      "\tIteration: 2960\t Loss: 0.6324\n",
      "\tIteration: 3000\t Loss: 0.5815\n",
      "\tIteration: 3040\t Loss: 0.5936\n",
      "\tIteration: 3080\t Loss: 0.6524\n",
      "\tIteration: 3120\t Loss: 0.6331\n",
      "\tIteration: 3160\t Loss: 0.6678\n",
      "\tIteration: 3200\t Loss: 0.6900\n",
      "\tIteration: 3240\t Loss: 0.6442\n",
      "\tIteration: 3280\t Loss: 0.6681\n",
      "\tIteration: 3320\t Loss: 0.5505\n",
      "\tIteration: 3360\t Loss: 0.5808\n",
      "\tIteration: 3400\t Loss: 0.6091\n",
      "\tIteration: 3440\t Loss: 0.5420\n",
      "\tIteration: 3480\t Loss: 0.5342\n",
      "\tIteration: 3520\t Loss: 0.5493\n",
      "\tIteration: 3560\t Loss: 0.5630\n",
      "\tIteration: 3600\t Loss: 0.5772\n",
      "\tIteration: 3640\t Loss: 0.5280\n",
      "\tIteration: 3680\t Loss: 0.5224\n",
      "\tIteration: 3720\t Loss: 0.5502\n",
      "0.8388\n",
      "Epoch: 3/10\n",
      "\tIteration: 0\t Loss: 0.0106\n",
      "\tIteration: 40\t Loss: 0.4879\n",
      "\tIteration: 80\t Loss: 0.5098\n",
      "\tIteration: 120\t Loss: 0.5681\n",
      "\tIteration: 160\t Loss: 0.5307\n",
      "\tIteration: 200\t Loss: 0.5035\n",
      "\tIteration: 240\t Loss: 0.5824\n",
      "\tIteration: 280\t Loss: 0.5041\n",
      "\tIteration: 320\t Loss: 0.5048\n",
      "\tIteration: 360\t Loss: 0.5074\n",
      "\tIteration: 400\t Loss: 0.5302\n",
      "\tIteration: 440\t Loss: 0.5659\n",
      "\tIteration: 480\t Loss: 0.6005\n",
      "\tIteration: 520\t Loss: 0.5061\n",
      "\tIteration: 560\t Loss: 0.5231\n",
      "\tIteration: 600\t Loss: 0.4907\n",
      "\tIteration: 640\t Loss: 0.4966\n",
      "\tIteration: 680\t Loss: 0.5064\n",
      "\tIteration: 720\t Loss: 0.5350\n",
      "\tIteration: 760\t Loss: 0.5085\n",
      "\tIteration: 800\t Loss: 0.4693\n",
      "\tIteration: 840\t Loss: 0.4862\n",
      "\tIteration: 880\t Loss: 0.4543\n",
      "\tIteration: 920\t Loss: 0.4994\n",
      "\tIteration: 960\t Loss: 0.5569\n",
      "\tIteration: 1000\t Loss: 0.5738\n",
      "\tIteration: 1040\t Loss: 0.5270\n",
      "\tIteration: 1080\t Loss: 0.5723\n",
      "\tIteration: 1120\t Loss: 0.5277\n",
      "\tIteration: 1160\t Loss: 0.5509\n",
      "\tIteration: 1200\t Loss: 0.4981\n",
      "\tIteration: 1240\t Loss: 0.4931\n",
      "\tIteration: 1280\t Loss: 0.5177\n",
      "\tIteration: 1320\t Loss: 0.5203\n",
      "\tIteration: 1360\t Loss: 0.5404\n",
      "\tIteration: 1400\t Loss: 0.4541\n",
      "\tIteration: 1440\t Loss: 0.4749\n",
      "\tIteration: 1480\t Loss: 0.4990\n",
      "\tIteration: 1520\t Loss: 0.5220\n",
      "\tIteration: 1560\t Loss: 0.4531\n",
      "\tIteration: 1600\t Loss: 0.5009\n",
      "\tIteration: 1640\t Loss: 0.5531\n",
      "\tIteration: 1680\t Loss: 0.5612\n",
      "\tIteration: 1720\t Loss: 0.4874\n",
      "\tIteration: 1760\t Loss: 0.4071\n",
      "\tIteration: 1800\t Loss: 0.4650\n",
      "\tIteration: 1840\t Loss: 0.4371\n",
      "\tIteration: 1880\t Loss: 0.5137\n",
      "\tIteration: 1920\t Loss: 0.4360\n",
      "\tIteration: 1960\t Loss: 0.4568\n",
      "\tIteration: 2000\t Loss: 0.4704\n",
      "\tIteration: 2040\t Loss: 0.4612\n",
      "\tIteration: 2080\t Loss: 0.4252\n",
      "\tIteration: 2120\t Loss: 0.4338\n",
      "\tIteration: 2160\t Loss: 0.4608\n",
      "\tIteration: 2200\t Loss: 0.4502\n",
      "\tIteration: 2240\t Loss: 0.4764\n",
      "\tIteration: 2280\t Loss: 0.4165\n",
      "\tIteration: 2320\t Loss: 0.4854\n",
      "\tIteration: 2360\t Loss: 0.4482\n",
      "\tIteration: 2400\t Loss: 0.4385\n",
      "\tIteration: 2440\t Loss: 0.4725\n",
      "\tIteration: 2480\t Loss: 0.4982\n",
      "\tIteration: 2520\t Loss: 0.4909\n",
      "\tIteration: 2560\t Loss: 0.5114\n",
      "\tIteration: 2600\t Loss: 0.3842\n",
      "\tIteration: 2640\t Loss: 0.4587\n",
      "\tIteration: 2680\t Loss: 0.4462\n",
      "\tIteration: 2720\t Loss: 0.4278\n",
      "\tIteration: 2760\t Loss: 0.5110\n",
      "\tIteration: 2800\t Loss: 0.4188\n",
      "\tIteration: 2840\t Loss: 0.3376\n",
      "\tIteration: 2880\t Loss: 0.4601\n",
      "\tIteration: 2920\t Loss: 0.4143\n",
      "\tIteration: 2960\t Loss: 0.3977\n",
      "\tIteration: 3000\t Loss: 0.4412\n",
      "\tIteration: 3040\t Loss: 0.4651\n",
      "\tIteration: 3080\t Loss: 0.5111\n",
      "\tIteration: 3120\t Loss: 0.4574\n",
      "\tIteration: 3160\t Loss: 0.4260\n",
      "\tIteration: 3200\t Loss: 0.5341\n",
      "\tIteration: 3240\t Loss: 0.4112\n",
      "\tIteration: 3280\t Loss: 0.3892\n",
      "\tIteration: 3320\t Loss: 0.4558\n",
      "\tIteration: 3360\t Loss: 0.4561\n",
      "\tIteration: 3400\t Loss: 0.4007\n",
      "\tIteration: 3440\t Loss: 0.4243\n",
      "\tIteration: 3480\t Loss: 0.3902\n",
      "\tIteration: 3520\t Loss: 0.4357\n",
      "\tIteration: 3560\t Loss: 0.4057\n",
      "\tIteration: 3600\t Loss: 0.4811\n",
      "\tIteration: 3640\t Loss: 0.4422\n",
      "\tIteration: 3680\t Loss: 0.4783\n",
      "\tIteration: 3720\t Loss: 0.4378\n",
      "0.8749\n",
      "Epoch: 4/10\n",
      "\tIteration: 0\t Loss: 0.0045\n",
      "\tIteration: 40\t Loss: 0.4293\n",
      "\tIteration: 80\t Loss: 0.3422\n",
      "\tIteration: 120\t Loss: 0.4431\n",
      "\tIteration: 160\t Loss: 0.4417\n",
      "\tIteration: 200\t Loss: 0.4450\n",
      "\tIteration: 240\t Loss: 0.3709\n",
      "\tIteration: 280\t Loss: 0.4280\n",
      "\tIteration: 320\t Loss: 0.4147\n",
      "\tIteration: 360\t Loss: 0.4766\n",
      "\tIteration: 400\t Loss: 0.3595\n",
      "\tIteration: 440\t Loss: 0.4728\n",
      "\tIteration: 480\t Loss: 0.4309\n",
      "\tIteration: 520\t Loss: 0.3749\n",
      "\tIteration: 560\t Loss: 0.3894\n",
      "\tIteration: 600\t Loss: 0.3692\n",
      "\tIteration: 640\t Loss: 0.3633\n",
      "\tIteration: 680\t Loss: 0.4573\n",
      "\tIteration: 720\t Loss: 0.4740\n",
      "\tIteration: 760\t Loss: 0.3623\n",
      "\tIteration: 800\t Loss: 0.3300\n",
      "\tIteration: 840\t Loss: 0.3924\n",
      "\tIteration: 880\t Loss: 0.3904\n",
      "\tIteration: 920\t Loss: 0.4060\n",
      "\tIteration: 960\t Loss: 0.4518\n",
      "\tIteration: 1000\t Loss: 0.3598\n",
      "\tIteration: 1040\t Loss: 0.4682\n",
      "\tIteration: 1080\t Loss: 0.4540\n",
      "\tIteration: 1120\t Loss: 0.3659\n",
      "\tIteration: 1160\t Loss: 0.3821\n",
      "\tIteration: 1200\t Loss: 0.4161\n",
      "\tIteration: 1240\t Loss: 0.3633\n",
      "\tIteration: 1280\t Loss: 0.3728\n",
      "\tIteration: 1320\t Loss: 0.4192\n",
      "\tIteration: 1360\t Loss: 0.4112\n",
      "\tIteration: 1400\t Loss: 0.4119\n",
      "\tIteration: 1440\t Loss: 0.3632\n",
      "\tIteration: 1480\t Loss: 0.4364\n",
      "\tIteration: 1520\t Loss: 0.4153\n",
      "\tIteration: 1560\t Loss: 0.3790\n",
      "\tIteration: 1600\t Loss: 0.4006\n",
      "\tIteration: 1640\t Loss: 0.3666\n",
      "\tIteration: 1680\t Loss: 0.3710\n",
      "\tIteration: 1720\t Loss: 0.3891\n",
      "\tIteration: 1760\t Loss: 0.3569\n",
      "\tIteration: 1800\t Loss: 0.4749\n",
      "\tIteration: 1840\t Loss: 0.3519\n",
      "\tIteration: 1880\t Loss: 0.3960\n",
      "\tIteration: 1920\t Loss: 0.4189\n",
      "\tIteration: 1960\t Loss: 0.3716\n",
      "\tIteration: 2000\t Loss: 0.3431\n",
      "\tIteration: 2040\t Loss: 0.3531\n",
      "\tIteration: 2080\t Loss: 0.3593\n",
      "\tIteration: 2120\t Loss: 0.4394\n",
      "\tIteration: 2160\t Loss: 0.3943\n",
      "\tIteration: 2200\t Loss: 0.4330\n",
      "\tIteration: 2240\t Loss: 0.3052\n",
      "\tIteration: 2280\t Loss: 0.3627\n",
      "\tIteration: 2320\t Loss: 0.4048\n",
      "\tIteration: 2360\t Loss: 0.4212\n",
      "\tIteration: 2400\t Loss: 0.4656\n",
      "\tIteration: 2440\t Loss: 0.4386\n",
      "\tIteration: 2480\t Loss: 0.4398\n",
      "\tIteration: 2520\t Loss: 0.3523\n",
      "\tIteration: 2560\t Loss: 0.3895\n",
      "\tIteration: 2600\t Loss: 0.3068\n",
      "\tIteration: 2640\t Loss: 0.3683\n",
      "\tIteration: 2680\t Loss: 0.3648\n",
      "\tIteration: 2720\t Loss: 0.3237\n",
      "\tIteration: 2760\t Loss: 0.3477\n",
      "\tIteration: 2800\t Loss: 0.3719\n",
      "\tIteration: 2840\t Loss: 0.3745\n",
      "\tIteration: 2880\t Loss: 0.3669\n",
      "\tIteration: 2920\t Loss: 0.4100\n",
      "\tIteration: 2960\t Loss: 0.3915\n",
      "\tIteration: 3000\t Loss: 0.4036\n",
      "\tIteration: 3040\t Loss: 0.4220\n",
      "\tIteration: 3080\t Loss: 0.3737\n",
      "\tIteration: 3120\t Loss: 0.4336\n",
      "\tIteration: 3160\t Loss: 0.3325\n",
      "\tIteration: 3200\t Loss: 0.3532\n",
      "\tIteration: 3240\t Loss: 0.3502\n",
      "\tIteration: 3280\t Loss: 0.3547\n",
      "\tIteration: 3320\t Loss: 0.3429\n",
      "\tIteration: 3360\t Loss: 0.3894\n",
      "\tIteration: 3400\t Loss: 0.3277\n",
      "\tIteration: 3440\t Loss: 0.3686\n",
      "\tIteration: 3480\t Loss: 0.3162\n",
      "\tIteration: 3520\t Loss: 0.3646\n",
      "\tIteration: 3560\t Loss: 0.3678\n",
      "\tIteration: 3600\t Loss: 0.3676\n",
      "\tIteration: 3640\t Loss: 0.3624\n",
      "\tIteration: 3680\t Loss: 0.3950\n",
      "\tIteration: 3720\t Loss: 0.3898\n",
      "0.8951\n",
      "Epoch: 5/10\n",
      "\tIteration: 0\t Loss: 0.0078\n",
      "\tIteration: 40\t Loss: 0.3965\n",
      "\tIteration: 80\t Loss: 0.3397\n",
      "\tIteration: 120\t Loss: 0.2927\n",
      "\tIteration: 160\t Loss: 0.3704\n",
      "\tIteration: 200\t Loss: 0.3785\n",
      "\tIteration: 240\t Loss: 0.3119\n",
      "\tIteration: 280\t Loss: 0.3269\n",
      "\tIteration: 320\t Loss: 0.4303\n",
      "\tIteration: 360\t Loss: 0.3390\n",
      "\tIteration: 400\t Loss: 0.3428\n",
      "\tIteration: 440\t Loss: 0.3551\n",
      "\tIteration: 480\t Loss: 0.3686\n",
      "\tIteration: 520\t Loss: 0.3701\n",
      "\tIteration: 560\t Loss: 0.4161\n",
      "\tIteration: 600\t Loss: 0.4126\n",
      "\tIteration: 640\t Loss: 0.3040\n",
      "\tIteration: 680\t Loss: 0.3633\n",
      "\tIteration: 720\t Loss: 0.3868\n",
      "\tIteration: 760\t Loss: 0.3665\n",
      "\tIteration: 800\t Loss: 0.3243\n",
      "\tIteration: 840\t Loss: 0.2807\n",
      "\tIteration: 880\t Loss: 0.4016\n",
      "\tIteration: 920\t Loss: 0.3547\n",
      "\tIteration: 960\t Loss: 0.4322\n",
      "\tIteration: 1000\t Loss: 0.3181\n",
      "\tIteration: 1040\t Loss: 0.3937\n",
      "\tIteration: 1080\t Loss: 0.3165\n",
      "\tIteration: 1120\t Loss: 0.3789\n",
      "\tIteration: 1160\t Loss: 0.2982\n",
      "\tIteration: 1200\t Loss: 0.3262\n",
      "\tIteration: 1240\t Loss: 0.3340\n",
      "\tIteration: 1280\t Loss: 0.3673\n",
      "\tIteration: 1320\t Loss: 0.3730\n",
      "\tIteration: 1360\t Loss: 0.3587\n",
      "\tIteration: 1400\t Loss: 0.2885\n",
      "\tIteration: 1440\t Loss: 0.4103\n",
      "\tIteration: 1480\t Loss: 0.3020\n",
      "\tIteration: 1520\t Loss: 0.3565\n",
      "\tIteration: 1560\t Loss: 0.3620\n",
      "\tIteration: 1600\t Loss: 0.3335\n",
      "\tIteration: 1640\t Loss: 0.3567\n",
      "\tIteration: 1680\t Loss: 0.3400\n",
      "\tIteration: 1720\t Loss: 0.3844\n",
      "\tIteration: 1760\t Loss: 0.2934\n",
      "\tIteration: 1800\t Loss: 0.3197\n",
      "\tIteration: 1840\t Loss: 0.2940\n",
      "\tIteration: 1880\t Loss: 0.3369\n",
      "\tIteration: 1920\t Loss: 0.3197\n",
      "\tIteration: 1960\t Loss: 0.3491\n",
      "\tIteration: 2000\t Loss: 0.2904\n",
      "\tIteration: 2040\t Loss: 0.3057\n",
      "\tIteration: 2080\t Loss: 0.3314\n",
      "\tIteration: 2120\t Loss: 0.3298\n",
      "\tIteration: 2160\t Loss: 0.3196\n",
      "\tIteration: 2200\t Loss: 0.2566\n",
      "\tIteration: 2240\t Loss: 0.3696\n",
      "\tIteration: 2280\t Loss: 0.3966\n",
      "\tIteration: 2320\t Loss: 0.2982\n",
      "\tIteration: 2360\t Loss: 0.2586\n",
      "\tIteration: 2400\t Loss: 0.2956\n",
      "\tIteration: 2440\t Loss: 0.2887\n",
      "\tIteration: 2480\t Loss: 0.2608\n",
      "\tIteration: 2520\t Loss: 0.2875\n",
      "\tIteration: 2560\t Loss: 0.3453\n",
      "\tIteration: 2600\t Loss: 0.3224\n",
      "\tIteration: 2640\t Loss: 0.2964\n",
      "\tIteration: 2680\t Loss: 0.3003\n",
      "\tIteration: 2720\t Loss: 0.3395\n",
      "\tIteration: 2760\t Loss: 0.3641\n",
      "\tIteration: 2800\t Loss: 0.3322\n",
      "\tIteration: 2840\t Loss: 0.3675\n",
      "\tIteration: 2880\t Loss: 0.3366\n",
      "\tIteration: 2920\t Loss: 0.3162\n",
      "\tIteration: 2960\t Loss: 0.3701\n",
      "\tIteration: 3000\t Loss: 0.3186\n",
      "\tIteration: 3040\t Loss: 0.2770\n",
      "\tIteration: 3080\t Loss: 0.4106\n",
      "\tIteration: 3120\t Loss: 0.3208\n",
      "\tIteration: 3160\t Loss: 0.3277\n",
      "\tIteration: 3200\t Loss: 0.3694\n",
      "\tIteration: 3240\t Loss: 0.3353\n",
      "\tIteration: 3280\t Loss: 0.3013\n",
      "\tIteration: 3320\t Loss: 0.3192\n",
      "\tIteration: 3360\t Loss: 0.3097\n",
      "\tIteration: 3400\t Loss: 0.3225\n",
      "\tIteration: 3440\t Loss: 0.3165\n",
      "\tIteration: 3480\t Loss: 0.3316\n",
      "\tIteration: 3520\t Loss: 0.3256\n",
      "\tIteration: 3560\t Loss: 0.3416\n",
      "\tIteration: 3600\t Loss: 0.2962\n",
      "\tIteration: 3640\t Loss: 0.3125\n",
      "\tIteration: 3680\t Loss: 0.3564\n",
      "\tIteration: 3720\t Loss: 0.3499\n",
      "0.9047\n",
      "Epoch: 6/10\n",
      "\tIteration: 0\t Loss: 0.0062\n",
      "\tIteration: 40\t Loss: 0.3570\n",
      "\tIteration: 80\t Loss: 0.4293\n",
      "\tIteration: 120\t Loss: 0.3539\n",
      "\tIteration: 160\t Loss: 0.3020\n",
      "\tIteration: 200\t Loss: 0.3002\n",
      "\tIteration: 240\t Loss: 0.3416\n",
      "\tIteration: 280\t Loss: 0.2812\n",
      "\tIteration: 320\t Loss: 0.2719\n",
      "\tIteration: 360\t Loss: 0.3540\n",
      "\tIteration: 400\t Loss: 0.3309\n",
      "\tIteration: 440\t Loss: 0.2744\n",
      "\tIteration: 480\t Loss: 0.2844\n",
      "\tIteration: 520\t Loss: 0.3387\n",
      "\tIteration: 560\t Loss: 0.3311\n",
      "\tIteration: 600\t Loss: 0.2739\n",
      "\tIteration: 640\t Loss: 0.3143\n",
      "\tIteration: 680\t Loss: 0.3187\n",
      "\tIteration: 720\t Loss: 0.2586\n",
      "\tIteration: 760\t Loss: 0.3068\n",
      "\tIteration: 800\t Loss: 0.2536\n",
      "\tIteration: 840\t Loss: 0.2806\n",
      "\tIteration: 880\t Loss: 0.3096\n",
      "\tIteration: 920\t Loss: 0.3194\n",
      "\tIteration: 960\t Loss: 0.3157\n",
      "\tIteration: 1000\t Loss: 0.3172\n",
      "\tIteration: 1040\t Loss: 0.2737\n",
      "\tIteration: 1080\t Loss: 0.3343\n",
      "\tIteration: 1120\t Loss: 0.2878\n",
      "\tIteration: 1160\t Loss: 0.2644\n",
      "\tIteration: 1200\t Loss: 0.2743\n",
      "\tIteration: 1240\t Loss: 0.2961\n",
      "\tIteration: 1280\t Loss: 0.3207\n",
      "\tIteration: 1320\t Loss: 0.3014\n",
      "\tIteration: 1360\t Loss: 0.3026\n",
      "\tIteration: 1400\t Loss: 0.2613\n",
      "\tIteration: 1440\t Loss: 0.2881\n",
      "\tIteration: 1480\t Loss: 0.2987\n",
      "\tIteration: 1520\t Loss: 0.2820\n",
      "\tIteration: 1560\t Loss: 0.3062\n",
      "\tIteration: 1600\t Loss: 0.2748\n",
      "\tIteration: 1640\t Loss: 0.3128\n",
      "\tIteration: 1680\t Loss: 0.2922\n",
      "\tIteration: 1720\t Loss: 0.2951\n",
      "\tIteration: 1760\t Loss: 0.2718\n",
      "\tIteration: 1800\t Loss: 0.2702\n",
      "\tIteration: 1840\t Loss: 0.3212\n",
      "\tIteration: 1880\t Loss: 0.2853\n",
      "\tIteration: 1920\t Loss: 0.3680\n",
      "\tIteration: 1960\t Loss: 0.2696\n",
      "\tIteration: 2000\t Loss: 0.3537\n",
      "\tIteration: 2040\t Loss: 0.2312\n",
      "\tIteration: 2080\t Loss: 0.3299\n",
      "\tIteration: 2120\t Loss: 0.2789\n",
      "\tIteration: 2160\t Loss: 0.2502\n",
      "\tIteration: 2200\t Loss: 0.2691\n",
      "\tIteration: 2240\t Loss: 0.2812\n",
      "\tIteration: 2280\t Loss: 0.3239\n",
      "\tIteration: 2320\t Loss: 0.3299\n",
      "\tIteration: 2360\t Loss: 0.2587\n",
      "\tIteration: 2400\t Loss: 0.2735\n",
      "\tIteration: 2440\t Loss: 0.3298\n",
      "\tIteration: 2480\t Loss: 0.3137\n",
      "\tIteration: 2520\t Loss: 0.2429\n",
      "\tIteration: 2560\t Loss: 0.2656\n",
      "\tIteration: 2600\t Loss: 0.2790\n",
      "\tIteration: 2640\t Loss: 0.3234\n",
      "\tIteration: 2680\t Loss: 0.2894\n",
      "\tIteration: 2720\t Loss: 0.2620\n",
      "\tIteration: 2760\t Loss: 0.2704\n",
      "\tIteration: 2800\t Loss: 0.3402\n",
      "\tIteration: 2840\t Loss: 0.2946\n",
      "\tIteration: 2880\t Loss: 0.3036\n",
      "\tIteration: 2920\t Loss: 0.2983\n",
      "\tIteration: 2960\t Loss: 0.3490\n",
      "\tIteration: 3000\t Loss: 0.2796\n",
      "\tIteration: 3040\t Loss: 0.3194\n",
      "\tIteration: 3080\t Loss: 0.2781\n",
      "\tIteration: 3120\t Loss: 0.3517\n",
      "\tIteration: 3160\t Loss: 0.3039\n",
      "\tIteration: 3200\t Loss: 0.2740\n",
      "\tIteration: 3240\t Loss: 0.2494\n",
      "\tIteration: 3280\t Loss: 0.2274\n",
      "\tIteration: 3320\t Loss: 0.2938\n",
      "\tIteration: 3360\t Loss: 0.2992\n",
      "\tIteration: 3400\t Loss: 0.2490\n",
      "\tIteration: 3440\t Loss: 0.3043\n",
      "\tIteration: 3480\t Loss: 0.2479\n",
      "\tIteration: 3520\t Loss: 0.3280\n",
      "\tIteration: 3560\t Loss: 0.3204\n",
      "\tIteration: 3600\t Loss: 0.2498\n",
      "\tIteration: 3640\t Loss: 0.3063\n",
      "\tIteration: 3680\t Loss: 0.2396\n",
      "\tIteration: 3720\t Loss: 0.3205\n",
      "0.9125\n",
      "Epoch: 7/10\n",
      "\tIteration: 0\t Loss: 0.0019\n",
      "\tIteration: 40\t Loss: 0.2868\n",
      "\tIteration: 80\t Loss: 0.2690\n",
      "\tIteration: 120\t Loss: 0.2421\n",
      "\tIteration: 160\t Loss: 0.3199\n",
      "\tIteration: 200\t Loss: 0.2327\n",
      "\tIteration: 240\t Loss: 0.2924\n",
      "\tIteration: 280\t Loss: 0.3160\n",
      "\tIteration: 320\t Loss: 0.2599\n",
      "\tIteration: 360\t Loss: 0.2546\n",
      "\tIteration: 400\t Loss: 0.3129\n",
      "\tIteration: 440\t Loss: 0.2962\n",
      "\tIteration: 480\t Loss: 0.2423\n",
      "\tIteration: 520\t Loss: 0.2402\n",
      "\tIteration: 560\t Loss: 0.2797\n",
      "\tIteration: 600\t Loss: 0.2626\n",
      "\tIteration: 640\t Loss: 0.2482\n",
      "\tIteration: 680\t Loss: 0.2708\n",
      "\tIteration: 720\t Loss: 0.2586\n",
      "\tIteration: 760\t Loss: 0.2462\n",
      "\tIteration: 800\t Loss: 0.2909\n",
      "\tIteration: 840\t Loss: 0.2729\n",
      "\tIteration: 880\t Loss: 0.2660\n",
      "\tIteration: 920\t Loss: 0.3110\n",
      "\tIteration: 960\t Loss: 0.2585\n",
      "\tIteration: 1000\t Loss: 0.2507\n",
      "\tIteration: 1040\t Loss: 0.2717\n",
      "\tIteration: 1080\t Loss: 0.2938\n",
      "\tIteration: 1120\t Loss: 0.2238\n",
      "\tIteration: 1160\t Loss: 0.2830\n",
      "\tIteration: 1200\t Loss: 0.2761\n",
      "\tIteration: 1240\t Loss: 0.2719\n",
      "\tIteration: 1280\t Loss: 0.2931\n",
      "\tIteration: 1320\t Loss: 0.2705\n",
      "\tIteration: 1360\t Loss: 0.2360\n",
      "\tIteration: 1400\t Loss: 0.2692\n",
      "\tIteration: 1440\t Loss: 0.2662\n",
      "\tIteration: 1480\t Loss: 0.2498\n",
      "\tIteration: 1520\t Loss: 0.2794\n",
      "\tIteration: 1560\t Loss: 0.3121\n",
      "\tIteration: 1600\t Loss: 0.2442\n",
      "\tIteration: 1640\t Loss: 0.2425\n",
      "\tIteration: 1680\t Loss: 0.2585\n",
      "\tIteration: 1720\t Loss: 0.2498\n",
      "\tIteration: 1760\t Loss: 0.3124\n",
      "\tIteration: 1800\t Loss: 0.2829\n",
      "\tIteration: 1840\t Loss: 0.3094\n",
      "\tIteration: 1880\t Loss: 0.2462\n",
      "\tIteration: 1920\t Loss: 0.3783\n",
      "\tIteration: 1960\t Loss: 0.2682\n",
      "\tIteration: 2000\t Loss: 0.2750\n",
      "\tIteration: 2040\t Loss: 0.2755\n",
      "\tIteration: 2080\t Loss: 0.3095\n",
      "\tIteration: 2120\t Loss: 0.2299\n",
      "\tIteration: 2160\t Loss: 0.2921\n",
      "\tIteration: 2200\t Loss: 0.2743\n",
      "\tIteration: 2240\t Loss: 0.2154\n",
      "\tIteration: 2280\t Loss: 0.2445\n",
      "\tIteration: 2320\t Loss: 0.2355\n",
      "\tIteration: 2360\t Loss: 0.3009\n",
      "\tIteration: 2400\t Loss: 0.2441\n",
      "\tIteration: 2440\t Loss: 0.2518\n",
      "\tIteration: 2480\t Loss: 0.2194\n",
      "\tIteration: 2520\t Loss: 0.2821\n",
      "\tIteration: 2560\t Loss: 0.2634\n",
      "\tIteration: 2600\t Loss: 0.2295\n",
      "\tIteration: 2640\t Loss: 0.2149\n",
      "\tIteration: 2680\t Loss: 0.2325\n",
      "\tIteration: 2720\t Loss: 0.3291\n",
      "\tIteration: 2760\t Loss: 0.2231\n",
      "\tIteration: 2800\t Loss: 0.2943\n",
      "\tIteration: 2840\t Loss: 0.2334\n",
      "\tIteration: 2880\t Loss: 0.2110\n",
      "\tIteration: 2920\t Loss: 0.2412\n",
      "\tIteration: 2960\t Loss: 0.2564\n",
      "\tIteration: 3000\t Loss: 0.2436\n",
      "\tIteration: 3040\t Loss: 0.2430\n",
      "\tIteration: 3080\t Loss: 0.2209\n",
      "\tIteration: 3120\t Loss: 0.2394\n",
      "\tIteration: 3160\t Loss: 0.2623\n",
      "\tIteration: 3200\t Loss: 0.3183\n",
      "\tIteration: 3240\t Loss: 0.2930\n",
      "\tIteration: 3280\t Loss: 0.2547\n",
      "\tIteration: 3320\t Loss: 0.2842\n",
      "\tIteration: 3360\t Loss: 0.2497\n",
      "\tIteration: 3400\t Loss: 0.2370\n",
      "\tIteration: 3440\t Loss: 0.2672\n",
      "\tIteration: 3480\t Loss: 0.2708\n",
      "\tIteration: 3520\t Loss: 0.2296\n",
      "\tIteration: 3560\t Loss: 0.2090\n",
      "\tIteration: 3600\t Loss: 0.2576\n",
      "\tIteration: 3640\t Loss: 0.2684\n",
      "\tIteration: 3680\t Loss: 0.2622\n",
      "\tIteration: 3720\t Loss: 0.2358\n",
      "0.912\n"
     ]
    }
   ],
   "source": [
    "## TODO: Your training loop here\n",
    "\n",
    "epochs = 10\n",
    "print_every = 40\n",
    "accs_test = []\n",
    "patience = 2\n",
    "epsilon = 0.005\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten EMNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model2.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        acc, y_pred, y_true = check_accuracy(testloader, model2)\n",
    "        accs_test.append(acc)\n",
    "    model2.train()\n",
    "    if len(accs_test) > (patience - 1):\n",
    "        if accs_test[-1] - accs_test[-patience] < epsilon:\n",
    "            print(f'Training stopped after {})\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Optional:</h3>\n",
    "  <p>Don't you want to use MNIST? Try EMNIST instead! Maybe using the first 10 letters of the alphabet!</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:35:26.981584Z",
     "start_time": "2021-05-26T22:35:26.954522Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will need a custom visualization function\n",
    "def view_classify_emnist(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(list(\"abcdefghij\"), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:50:57.571260Z",
     "start_time": "2021-05-26T22:50:57.322172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "def my_collate(batch):\n",
    "    modified_batch = []\n",
    "    for item in batch:\n",
    "        image, label = item\n",
    "        if label < 10: # only the first ten letters\n",
    "            modified_batch.append(item)\n",
    "    return torch.utils.data._utils.collate.default_collate(modified_batch)\n",
    "\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.EMNIST('EMNIST_data/', split=\"letters\", download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, collate_fn=my_collate)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.EMNIST('EMNIST_data/', split=\"letters\", download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:51:02.493175Z",
     "start_time": "2021-05-26T22:51:02.464301Z"
    }
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:51:03.118421Z",
     "start_time": "2021-05-26T22:51:02.978678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[5].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T22:51:06.653639Z",
     "start_time": "2021-05-26T22:51:06.647991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 9, 6, 7, 5, 1, 9, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python395jvsc74a57bd05d8eb55396b26e444e25830983e23d02f9d742a022e83389ffb69c4e4a8d7f54",
   "display_name": "Python 3.9.5 64-bit ('deep_learning': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "5d8eb55396b26e444e25830983e23d02f9d742a022e83389ffb69c4e4a8d7f54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}