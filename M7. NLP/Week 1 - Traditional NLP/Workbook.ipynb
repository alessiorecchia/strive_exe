{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Spacy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# !conda install -c conda-forge spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "## I recommend the one above, because the following is more accurate but less efficient\n",
    "# !python -m spacy download en_core_web_lg"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:42.520625Z",
     "start_time": "2021-03-28T20:32:42.518920Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# You can also load en_core_web_lg that has an higher accuracy but it's less efficient\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:25.501688Z",
     "start_time": "2021-03-31T09:15:24.378850Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "print(nlp.pipeline)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f2fa914f8b0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f2fa910ac70>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f2fa92bce20>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f2fa9110d40>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f2fa90a8c40>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f2fa92bcfa0>)]\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:43.994556Z",
     "start_time": "2021-03-28T20:32:43.991087Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Process sentences 'Hello, world. Antonio is learning Python.' using spaCy\n",
    "doc = nlp(u\"Hello, world. Antonio is learning Python.\")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.015363Z",
     "start_time": "2021-03-28T20:32:43.995410Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      ".\n",
      "Antonio\n",
      "is\n",
      "learning\n",
      "Python\n",
      ".\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 5;\n                var nbb_unformatted_code = \"for token in doc:\\n    print(token.text)\";\n                var nbb_formatted_code = \"for token in doc:\\n    print(token.text)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.021660Z",
     "start_time": "2021-03-28T20:32:44.016337Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello\n",
      "Hello, world.\n",
      "Antonio is learning Python.\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.030363Z",
     "start_time": "2021-03-28T20:32:44.022649Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tokens = nlp(\"Let's go to N.Y.!\")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.043811Z",
     "start_time": "2021-03-28T20:32:44.033898Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for token in tokens:\n",
    "    print(token.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.049403Z",
     "start_time": "2021-03-28T20:32:44.045680Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you have seen, using `nlp`, that comes from `spacy.load(\"en_core_web_sm\")`, you get the tokenized version of the sentence. If you want only the instance of the `Tokenizer` class, you can run:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "type(tokenizer)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "spacy.tokenizer.Tokenizer"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.054850Z",
     "start_time": "2021-03-28T20:32:44.050350Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to instantiate a custom one, with rules and prefixes and so on:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(vocab=nlp.vocab)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.062007Z",
     "start_time": "2021-03-28T20:32:44.055855Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The tokenizer defined above contains only english rules.\n",
    "Let's test it on \"Let's go to N.Y.!\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tokens = tokenizer(\"Let's go to N.Y.!\")\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Let's\n",
      "go\n",
      "to\n",
      "N.Y.!\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.068307Z",
     "start_time": "2021-03-28T20:32:44.063416Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see here, it doesn't handle the exceptions about the dots. So we can add rules for this!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "prefix_re = spacy.util.compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "suffix_re = spacy.util.compile_suffix_regex(nlp.Defaults.prefixes)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.076430Z",
     "start_time": "2021-03-28T20:32:44.069248Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tokenizer = Tokenizer(\n",
    "    vocab=nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.083023Z",
     "start_time": "2021-03-28T20:32:44.077855Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tokens = tokenizer(\"Let's go to N.Y.!\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Let's\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.088977Z",
     "start_time": "2021-03-28T20:32:44.083988Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also check the exceptions the tokenizer can handle:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from spacy.lang.en.tokenizer_exceptions import TOKENIZER_EXCEPTIONS\n",
    "\n",
    "TOKENIZER_EXCEPTIONS.values()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_values([[{65: ' '}], [{65: '\\t'}], [{65: '\\\\t'}], [{65: '\\n'}], [{65: '\\\\n'}], [{65: '—'}], [{65: '\\xa0', 67: '  '}], [{65: \"'\"}], [{65: '\\\\\")'}], [{65: '<space>'}], [{65: \"''\"}], [{65: 'C++'}], [{65: 'a.'}], [{65: 'b.'}], [{65: 'c.'}], [{65: 'd.'}], [{65: 'e.'}], [{65: 'f.'}], [{65: 'g.'}], [{65: 'h.'}], [{65: 'i.'}], [{65: 'j.'}], [{65: 'k.'}], [{65: 'l.'}], [{65: 'm.'}], [{65: 'n.'}], [{65: 'o.'}], [{65: 'p.'}], [{65: 'q.'}], [{65: 'r.'}], [{65: 's.'}], [{65: 't.'}], [{65: 'u.'}], [{65: 'v.'}], [{65: 'w.'}], [{65: 'x.'}], [{65: 'y.'}], [{65: 'z.'}], [{65: 'ä.'}], [{65: 'ö.'}], [{65: 'ü.'}], [{65: 'xD'}], [{65: 'XD'}], [{65: '=]'}], [{65: \":')\"}], [{65: 'xDD'}], [{65: ':-))'}], [{65: ':*'}], [{65: ';)'}], [{65: ':('}], [{65: '(='}], [{65: 'ಠ_ಠ'}], [{65: '-__-'}], [{65: '0_0'}], [{65: ':-3'}], [{65: '=)'}], [{65: 'o.o'}], [{65: ':-P'}], [{65: ';-D'}], [{65: '^_^'}], [{65: ':o'}], [{65: 'ಠ︵ಠ'}], [{65: '^___^'}], [{65: ':|'}], [{65: ':-|'}], [{65: 'XDD'}], [{65: ':))'}], [{65: '8)'}], [{65: ':-X'}], [{65: ':-('}], [{65: ':-/'}], [{65: 'v.v'}], [{65: '[:'}], [{65: ':}'}], [{65: ':O'}], [{65: ';D'}], [{65: 'o_0'}], [{65: ':)'}], [{65: '(^_^)'}], [{65: 'v_v'}], [{65: \":'-)\"}], [{65: '-_-'}], [{65: 'O_O'}], [{65: '^__^'}], [{65: '¯\\\\(ツ)/¯'}], [{65: ':-]'}], [{65: '(._.)'}], [{65: '(-8'}], [{65: ':p'}], [{65: '>:o'}], [{65: ':-p'}], [{65: ':-(('}], [{65: '._.'}], [{65: ':-o'}], [{65: \":'(\"}], [{65: 'O.O'}], [{65: ':(('}], [{65: 'V.V'}], [{65: '>.>'}], [{65: ':-}'}], [{65: '8-)'}], [{65: '(╯°□°）╯︵┻━┻'}], [{65: ']='}], [{65: ':0'}], [{65: '8-D'}], [{65: '=['}], [{65: '=3'}], [{65: ':-*'}], [{65: ':x'}], [{65: ':-)'}], [{65: ':>'}], [{65: ':]'}], [{65: ':o)'}], [{65: '>:('}], [{65: '=/'}], [{65: ':-D'}], [{65: '</3'}], [{65: '0.o'}], [{65: '0_o'}], [{65: '(-_-)'}], [{65: '<3'}], [{65: '=D'}], [{65: '[='}], [{65: 'o_o'}], [{65: ':-)))'}], [{65: '(-:'}], [{65: ';_;'}], [{65: ':->'}], [{65: '<.<'}], [{65: ':((('}], [{65: '=('}], [{65: 'O.o'}], [{65: '(-;'}], [{65: ':-0'}], [{65: ':()'}], [{65: '><(((*>'}], [{65: ':/'}], [{65: 'o_O'}], [{65: ':-O'}], [{65: ':-((('}], [{65: 'V_V'}], [{65: '<33'}], [{65: '(>_<)'}], [{65: ':-x'}], [{65: '(*_*)'}], [{65: \":'-(\"}], [{65: '):'}], [{65: 'O_o'}], [{65: '<333'}], [{65: ':P'}], [{65: '(:'}], [{65: '(o:'}], [{65: ':D'}], [{65: '(;'}], [{65: '=|'}], [{65: ':1'}], [{65: 'o.O'}], [{65: '0.0'}], [{65: 'o.0'}], [{65: ':X'}], [{65: '8D'}], [{65: '[-:'}], [{65: '>.<'}], [{65: '@_@'}], [{65: '(ಠ_ಠ)'}], [{65: ')-:'}], [{65: ':3'}], [{65: ':)))'}], [{65: '(¬_¬)'}], [{65: ';-)'}], [{65: 'i', 67: 'i'}, {65: \"'m\", 67: 'am'}], [{65: 'i', 67: 'i'}, {65: 'm'}], [{65: 'i', 67: 'i'}, {65: \"'m\", 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'i', 67: 'i'}, {65: 'm', 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'I', 67: 'i'}, {65: \"'m\", 67: 'am'}], [{65: 'I', 67: 'i'}, {65: 'm'}], [{65: 'I', 67: 'i'}, {65: \"'m\", 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'I', 67: 'i'}, {65: 'm', 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'i', 67: 'i'}, {65: \"'ll\", 67: 'will'}], [{65: 'i', 67: 'i'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'i', 67: 'i'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'i', 67: 'i'}, {65: \"'d\", 67: \"'d\"}], [{65: 'i', 67: 'i'}, {65: 'd', 67: \"'d\"}], [{65: 'i', 67: 'i'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'i', 67: 'i'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: \"'ll\", 67: 'will'}], [{65: 'I', 67: 'i'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'I', 67: 'i'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: \"'d\", 67: \"'d\"}], [{65: 'I', 67: 'i'}, {65: 'd', 67: \"'d\"}], [{65: 'I', 67: 'i'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'I', 67: 'i'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: \"'ll\", 67: 'will'}], [{65: 'you', 67: 'you'}, {65: 'll', 67: 'will'}], [{65: 'you', 67: 'you'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'you', 67: 'you'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: \"'d\", 67: \"'d\"}], [{65: 'you', 67: 'you'}, {65: 'd', 67: \"'d\"}], [{65: 'you', 67: 'you'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'you', 67: 'you'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: \"'ll\", 67: 'will'}], [{65: 'You', 67: 'you'}, {65: 'll', 67: 'will'}], [{65: 'You', 67: 'you'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'You', 67: 'you'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: \"'d\", 67: \"'d\"}], [{65: 'You', 67: 'you'}, {65: 'd', 67: \"'d\"}], [{65: 'You', 67: 'you'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'You', 67: 'you'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'he', 67: 'he'}, {65: \"'ll\", 67: 'will'}], [{65: 'he', 67: 'he'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'he', 67: 'he'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'he', 67: 'he'}, {65: \"'d\", 67: \"'d\"}], [{65: 'he', 67: 'he'}, {65: 'd', 67: \"'d\"}], [{65: 'he', 67: 'he'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'he', 67: 'he'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'He', 67: 'he'}, {65: \"'ll\", 67: 'will'}], [{65: 'He', 67: 'he'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'He', 67: 'he'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'He', 67: 'he'}, {65: \"'d\", 67: \"'d\"}], [{65: 'He', 67: 'he'}, {65: 'd', 67: \"'d\"}], [{65: 'He', 67: 'he'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'He', 67: 'he'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'she', 67: 'she'}, {65: \"'ll\", 67: 'will'}], [{65: 'she', 67: 'she'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'she', 67: 'she'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'she', 67: 'she'}, {65: \"'d\", 67: \"'d\"}], [{65: 'she', 67: 'she'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'she', 67: 'she'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'She', 67: 'she'}, {65: \"'ll\", 67: 'will'}], [{65: 'She', 67: 'she'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'She', 67: 'she'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'She', 67: 'she'}, {65: \"'d\", 67: \"'d\"}], [{65: 'She', 67: 'she'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'She', 67: 'she'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'it', 67: 'it'}, {65: \"'ll\", 67: 'will'}], [{65: 'it', 67: 'it'}, {65: 'll', 67: 'will'}], [{65: 'it', 67: 'it'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'it', 67: 'it'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'it', 67: 'it'}, {65: \"'d\", 67: \"'d\"}], [{65: 'it', 67: 'it'}, {65: 'd', 67: \"'d\"}], [{65: 'it', 67: 'it'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'it', 67: 'it'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'It', 67: 'it'}, {65: \"'ll\", 67: 'will'}], [{65: 'It', 67: 'it'}, {65: 'll', 67: 'will'}], [{65: 'It', 67: 'it'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'It', 67: 'it'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'It', 67: 'it'}, {65: \"'d\", 67: \"'d\"}], [{65: 'It', 67: 'it'}, {65: 'd', 67: \"'d\"}], [{65: 'It', 67: 'it'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'It', 67: 'it'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: \"'ll\", 67: 'will'}], [{65: 'we', 67: 'we'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'we', 67: 'we'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: \"'d\", 67: \"'d\"}], [{65: 'we', 67: 'we'}, {65: 'd', 67: \"'d\"}], [{65: 'we', 67: 'we'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'we', 67: 'we'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: \"'ll\", 67: 'will'}], [{65: 'We', 67: 'we'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'We', 67: 'we'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: \"'d\", 67: \"'d\"}], [{65: 'We', 67: 'we'}, {65: 'd', 67: \"'d\"}], [{65: 'We', 67: 'we'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'We', 67: 'we'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: \"'ll\", 67: 'will'}], [{65: 'they', 67: 'they'}, {65: 'll', 67: 'will'}], [{65: 'they', 67: 'they'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'they', 67: 'they'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: \"'d\", 67: \"'d\"}], [{65: 'they', 67: 'they'}, {65: 'd', 67: \"'d\"}], [{65: 'they', 67: 'they'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'they', 67: 'they'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: \"'ll\", 67: 'will'}], [{65: 'They', 67: 'they'}, {65: 'll', 67: 'will'}], [{65: 'They', 67: 'they'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'They', 67: 'they'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: \"'d\", 67: \"'d\"}], [{65: 'They', 67: 'they'}, {65: 'd', 67: \"'d\"}], [{65: 'They', 67: 'they'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'They', 67: 'they'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'i', 67: 'i'}, {65: \"'ve\", 67: 'have'}], [{65: 'i', 67: 'i'}, {65: 've', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: \"'ve\", 67: 'have'}], [{65: 'I', 67: 'i'}, {65: 've', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: \"'ve\", 67: 'have'}], [{65: 'you', 67: 'you'}, {65: 've', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: \"'ve\", 67: 'have'}], [{65: 'You', 67: 'you'}, {65: 've', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: \"'ve\", 67: 'have'}], [{65: 'we', 67: 'we'}, {65: 've', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: \"'ve\", 67: 'have'}], [{65: 'We', 67: 'we'}, {65: 've', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: \"'ve\", 67: 'have'}], [{65: 'they', 67: 'they'}, {65: 've', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: \"'ve\", 67: 'have'}], [{65: 'They', 67: 'they'}, {65: 've', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: \"'re\", 67: 'are'}], [{65: 'you', 67: 'you'}, {65: 're', 67: 'are'}], [{65: 'You', 67: 'you'}, {65: \"'re\", 67: 'are'}], [{65: 'You', 67: 'you'}, {65: 're', 67: 'are'}], [{65: 'we', 67: 'we'}, {65: \"'re\", 67: 'are'}], [{65: 'We', 67: 'we'}, {65: \"'re\", 67: 'are'}], [{65: 'they', 67: 'they'}, {65: \"'re\", 67: 'are'}], [{65: 'they', 67: 'they'}, {65: 're', 67: 'are'}], [{65: 'They', 67: 'they'}, {65: \"'re\", 67: 'are'}], [{65: 'They', 67: 'they'}, {65: 're', 67: 'are'}], [{65: 'he', 67: 'he'}, {65: \"'s\", 67: \"'s\"}], [{65: 'he', 67: 'he'}, {65: 's'}], [{65: 'He', 67: 'he'}, {65: \"'s\", 67: \"'s\"}], [{65: 'He', 67: 'he'}, {65: 's'}], [{65: 'she', 67: 'she'}, {65: \"'s\", 67: \"'s\"}], [{65: 'she', 67: 'she'}, {65: 's'}], [{65: 'She', 67: 'she'}, {65: \"'s\", 67: \"'s\"}], [{65: 'She', 67: 'she'}, {65: 's'}], [{65: 'it', 67: 'it'}, {65: \"'s\", 67: \"'s\"}], [{65: 'It', 67: 'it'}, {65: \"'s\", 67: \"'s\"}], [{65: 'who', 67: 'who'}, {65: \"'s\", 67: \"'s\"}], [{65: 'who', 67: 'who'}, {65: 's'}], [{65: 'who', 67: 'who'}, {65: \"'ll\", 67: 'will'}], [{65: 'who', 67: 'who'}, {65: 'll', 67: 'will'}], [{65: 'who', 67: 'who'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'who', 67: 'who'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'who', 67: 'who'}, {65: \"'re\", 67: 'are'}], [{65: 'who', 67: 'who'}, {65: \"'ve\"}], [{65: 'who'}, {65: 've', 67: 'have'}], [{65: 'who', 67: 'who'}, {65: \"'d\", 67: \"'d\"}], [{65: 'who', 67: 'who'}, {65: 'd', 67: \"'d\"}], [{65: 'who', 67: 'who'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'who', 67: 'who'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Who', 67: 'who'}, {65: 's'}], [{65: 'Who', 67: 'who'}, {65: \"'ll\", 67: 'will'}], [{65: 'Who', 67: 'who'}, {65: 'll', 67: 'will'}], [{65: 'Who', 67: 'who'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: \"'re\", 67: 'are'}], [{65: 'Who', 67: 'who'}, {65: \"'ve\"}], [{65: 'Who'}, {65: 've', 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Who', 67: 'who'}, {65: 'd', 67: \"'d\"}], [{65: 'Who', 67: 'who'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'what', 67: 'what'}, {65: \"'s\", 67: \"'s\"}], [{65: 'what', 67: 'what'}, {65: 's'}], [{65: 'what', 67: 'what'}, {65: \"'ll\", 67: 'will'}], [{65: 'what', 67: 'what'}, {65: 'll', 67: 'will'}], [{65: 'what', 67: 'what'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'what', 67: 'what'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'what', 67: 'what'}, {65: \"'re\", 67: 'are'}], [{65: 'what', 67: 'what'}, {65: 're', 67: 'are'}], [{65: 'what', 67: 'what'}, {65: \"'ve\"}], [{65: 'what'}, {65: 've', 67: 'have'}], [{65: 'what', 67: 'what'}, {65: \"'d\", 67: \"'d\"}], [{65: 'what', 67: 'what'}, {65: 'd', 67: \"'d\"}], [{65: 'what', 67: 'what'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'what', 67: 'what'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'What', 67: 'what'}, {65: \"'s\", 67: \"'s\"}], [{65: 'What', 67: 'what'}, {65: 's'}], [{65: 'What', 67: 'what'}, {65: \"'ll\", 67: 'will'}], [{65: 'What', 67: 'what'}, {65: 'll', 67: 'will'}], [{65: 'What', 67: 'what'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'What', 67: 'what'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'What', 67: 'what'}, {65: \"'re\", 67: 'are'}], [{65: 'What', 67: 'what'}, {65: 're', 67: 'are'}], [{65: 'What', 67: 'what'}, {65: \"'ve\"}], [{65: 'What'}, {65: 've', 67: 'have'}], [{65: 'What', 67: 'what'}, {65: \"'d\", 67: \"'d\"}], [{65: 'What', 67: 'what'}, {65: 'd', 67: \"'d\"}], [{65: 'What', 67: 'what'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'What', 67: 'what'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'when', 67: 'when'}, {65: \"'s\", 67: \"'s\"}], [{65: 'when', 67: 'when'}, {65: 's'}], [{65: 'when', 67: 'when'}, {65: \"'ll\", 67: 'will'}], [{65: 'when', 67: 'when'}, {65: 'll', 67: 'will'}], [{65: 'when', 67: 'when'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'when', 67: 'when'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'when', 67: 'when'}, {65: \"'re\", 67: 'are'}], [{65: 'when', 67: 'when'}, {65: 're', 67: 'are'}], [{65: 'when', 67: 'when'}, {65: \"'ve\"}], [{65: 'when'}, {65: 've', 67: 'have'}], [{65: 'when', 67: 'when'}, {65: \"'d\", 67: \"'d\"}], [{65: 'when', 67: 'when'}, {65: 'd', 67: \"'d\"}], [{65: 'when', 67: 'when'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'when', 67: 'when'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'When', 67: 'when'}, {65: \"'s\", 67: \"'s\"}], [{65: 'When', 67: 'when'}, {65: 's'}], [{65: 'When', 67: 'when'}, {65: \"'ll\", 67: 'will'}], [{65: 'When', 67: 'when'}, {65: 'll', 67: 'will'}], [{65: 'When', 67: 'when'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'When', 67: 'when'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'When', 67: 'when'}, {65: \"'re\", 67: 'are'}], [{65: 'When', 67: 'when'}, {65: 're', 67: 'are'}], [{65: 'When', 67: 'when'}, {65: \"'ve\"}], [{65: 'When'}, {65: 've', 67: 'have'}], [{65: 'When', 67: 'when'}, {65: \"'d\", 67: \"'d\"}], [{65: 'When', 67: 'when'}, {65: 'd', 67: \"'d\"}], [{65: 'When', 67: 'when'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'When', 67: 'when'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'where', 67: 'where'}, {65: \"'s\", 67: \"'s\"}], [{65: 'where', 67: 'where'}, {65: 's'}], [{65: 'where', 67: 'where'}, {65: \"'ll\", 67: 'will'}], [{65: 'where', 67: 'where'}, {65: 'll', 67: 'will'}], [{65: 'where', 67: 'where'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'where', 67: 'where'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'where', 67: 'where'}, {65: \"'re\", 67: 'are'}], [{65: 'where', 67: 'where'}, {65: 're', 67: 'are'}], [{65: 'where', 67: 'where'}, {65: \"'ve\"}], [{65: 'where'}, {65: 've', 67: 'have'}], [{65: 'where', 67: 'where'}, {65: \"'d\", 67: \"'d\"}], [{65: 'where', 67: 'where'}, {65: 'd', 67: \"'d\"}], [{65: 'where', 67: 'where'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'where', 67: 'where'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Where', 67: 'where'}, {65: 's'}], [{65: 'Where', 67: 'where'}, {65: \"'ll\", 67: 'will'}], [{65: 'Where', 67: 'where'}, {65: 'll', 67: 'will'}], [{65: 'Where', 67: 'where'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: \"'re\", 67: 'are'}], [{65: 'Where', 67: 'where'}, {65: 're', 67: 'are'}], [{65: 'Where', 67: 'where'}, {65: \"'ve\"}], [{65: 'Where'}, {65: 've', 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Where', 67: 'where'}, {65: 'd', 67: \"'d\"}], [{65: 'Where', 67: 'where'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'why', 67: 'why'}, {65: \"'s\", 67: \"'s\"}], [{65: 'why', 67: 'why'}, {65: 's'}], [{65: 'why', 67: 'why'}, {65: \"'ll\", 67: 'will'}], [{65: 'why', 67: 'why'}, {65: 'll', 67: 'will'}], [{65: 'why', 67: 'why'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'why', 67: 'why'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'why', 67: 'why'}, {65: \"'re\", 67: 'are'}], [{65: 'why', 67: 'why'}, {65: 're', 67: 'are'}], [{65: 'why', 67: 'why'}, {65: \"'ve\"}], [{65: 'why'}, {65: 've', 67: 'have'}], [{65: 'why', 67: 'why'}, {65: \"'d\", 67: \"'d\"}], [{65: 'why', 67: 'why'}, {65: 'd', 67: \"'d\"}], [{65: 'why', 67: 'why'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'why', 67: 'why'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Why', 67: 'why'}, {65: 's'}], [{65: 'Why', 67: 'why'}, {65: \"'ll\", 67: 'will'}], [{65: 'Why', 67: 'why'}, {65: 'll', 67: 'will'}], [{65: 'Why', 67: 'why'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: \"'re\", 67: 'are'}], [{65: 'Why', 67: 'why'}, {65: 're', 67: 'are'}], [{65: 'Why', 67: 'why'}, {65: \"'ve\"}], [{65: 'Why'}, {65: 've', 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Why', 67: 'why'}, {65: 'd', 67: \"'d\"}], [{65: 'Why', 67: 'why'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'how', 67: 'how'}, {65: \"'s\", 67: \"'s\"}], [{65: 'how', 67: 'how'}, {65: 's'}], [{65: 'how', 67: 'how'}, {65: \"'ll\", 67: 'will'}], [{65: 'how', 67: 'how'}, {65: 'll', 67: 'will'}], [{65: 'how', 67: 'how'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'how', 67: 'how'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'how', 67: 'how'}, {65: \"'re\", 67: 'are'}], [{65: 'how', 67: 'how'}, {65: 're', 67: 'are'}], [{65: 'how', 67: 'how'}, {65: \"'ve\"}], [{65: 'how'}, {65: 've', 67: 'have'}], [{65: 'how', 67: 'how'}, {65: \"'d\", 67: \"'d\"}], [{65: 'how', 67: 'how'}, {65: 'd', 67: \"'d\"}], [{65: 'how', 67: 'how'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'how', 67: 'how'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'How', 67: 'how'}, {65: \"'s\", 67: \"'s\"}], [{65: 'How', 67: 'how'}, {65: 's'}], [{65: 'How', 67: 'how'}, {65: \"'ll\", 67: 'will'}], [{65: 'How', 67: 'how'}, {65: 'll', 67: 'will'}], [{65: 'How', 67: 'how'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'How', 67: 'how'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'How', 67: 'how'}, {65: \"'re\", 67: 'are'}], [{65: 'How', 67: 'how'}, {65: 're', 67: 'are'}], [{65: 'How', 67: 'how'}, {65: \"'ve\"}], [{65: 'How'}, {65: 've', 67: 'have'}], [{65: 'How', 67: 'how'}, {65: \"'d\", 67: \"'d\"}], [{65: 'How', 67: 'how'}, {65: 'd', 67: \"'d\"}], [{65: 'How', 67: 'how'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'How', 67: 'how'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'there', 67: 'there'}, {65: \"'s\", 67: \"'s\"}], [{65: 'there', 67: 'there'}, {65: 's'}], [{65: 'there', 67: 'there'}, {65: \"'ll\", 67: 'will'}], [{65: 'there', 67: 'there'}, {65: 'll', 67: 'will'}], [{65: 'there', 67: 'there'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'there', 67: 'there'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'there', 67: 'there'}, {65: \"'re\", 67: 'are'}], [{65: 'there', 67: 'there'}, {65: 're', 67: 'are'}], [{65: 'there', 67: 'there'}, {65: \"'ve\"}], [{65: 'there'}, {65: 've', 67: 'have'}], [{65: 'there', 67: 'there'}, {65: \"'d\", 67: \"'d\"}], [{65: 'there', 67: 'there'}, {65: 'd', 67: \"'d\"}], [{65: 'there', 67: 'there'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'there', 67: 'there'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'There', 67: 'there'}, {65: \"'s\", 67: \"'s\"}], [{65: 'There', 67: 'there'}, {65: 's'}], [{65: 'There', 67: 'there'}, {65: \"'ll\", 67: 'will'}], [{65: 'There', 67: 'there'}, {65: 'll', 67: 'will'}], [{65: 'There', 67: 'there'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'There', 67: 'there'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'There', 67: 'there'}, {65: \"'re\", 67: 'are'}], [{65: 'There', 67: 'there'}, {65: 're', 67: 'are'}], [{65: 'There', 67: 'there'}, {65: \"'ve\"}], [{65: 'There'}, {65: 've', 67: 'have'}], [{65: 'There', 67: 'there'}, {65: \"'d\", 67: \"'d\"}], [{65: 'There', 67: 'there'}, {65: 'd', 67: \"'d\"}], [{65: 'There', 67: 'there'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'There', 67: 'there'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'that', 67: 'that'}, {65: \"'s\", 67: \"'s\"}], [{65: 'that', 67: 'that'}, {65: 's'}], [{65: 'that', 67: 'that'}, {65: \"'ll\", 67: 'will'}], [{65: 'that', 67: 'that'}, {65: 'll', 67: 'will'}], [{65: 'that', 67: 'that'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'that', 67: 'that'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'that', 67: 'that'}, {65: \"'re\", 67: 'are'}], [{65: 'that', 67: 'that'}, {65: 're', 67: 'are'}], [{65: 'that', 67: 'that'}, {65: \"'ve\"}], [{65: 'that'}, {65: 've', 67: 'have'}], [{65: 'that', 67: 'that'}, {65: \"'d\", 67: \"'d\"}], [{65: 'that', 67: 'that'}, {65: 'd', 67: \"'d\"}], [{65: 'that', 67: 'that'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'that', 67: 'that'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'That', 67: 'that'}, {65: \"'s\", 67: \"'s\"}], [{65: 'That', 67: 'that'}, {65: 's'}], [{65: 'That', 67: 'that'}, {65: \"'ll\", 67: 'will'}], [{65: 'That', 67: 'that'}, {65: 'll', 67: 'will'}], [{65: 'That', 67: 'that'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'That', 67: 'that'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'That', 67: 'that'}, {65: \"'re\", 67: 'are'}], [{65: 'That', 67: 'that'}, {65: 're', 67: 'are'}], [{65: 'That', 67: 'that'}, {65: \"'ve\"}], [{65: 'That'}, {65: 've', 67: 'have'}], [{65: 'That', 67: 'that'}, {65: \"'d\", 67: \"'d\"}], [{65: 'That', 67: 'that'}, {65: 'd', 67: \"'d\"}], [{65: 'That', 67: 'that'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'That', 67: 'that'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'this', 67: 'this'}, {65: \"'s\", 67: \"'s\"}], [{65: 'this', 67: 'this'}, {65: 's'}], [{65: 'this', 67: 'this'}, {65: \"'ll\", 67: 'will'}], [{65: 'this', 67: 'this'}, {65: 'll', 67: 'will'}], [{65: 'this', 67: 'this'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'this', 67: 'this'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'this', 67: 'this'}, {65: \"'re\", 67: 'are'}], [{65: 'this', 67: 'this'}, {65: 're', 67: 'are'}], [{65: 'this', 67: 'this'}, {65: \"'ve\"}], [{65: 'this'}, {65: 've', 67: 'have'}], [{65: 'this', 67: 'this'}, {65: \"'d\", 67: \"'d\"}], [{65: 'this', 67: 'this'}, {65: 'd', 67: \"'d\"}], [{65: 'this', 67: 'this'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'this', 67: 'this'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'This', 67: 'this'}, {65: \"'s\", 67: \"'s\"}], [{65: 'This', 67: 'this'}, {65: 's'}], [{65: 'This', 67: 'this'}, {65: \"'ll\", 67: 'will'}], [{65: 'This', 67: 'this'}, {65: 'll', 67: 'will'}], [{65: 'This', 67: 'this'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'This', 67: 'this'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'This', 67: 'this'}, {65: \"'re\", 67: 'are'}], [{65: 'This', 67: 'this'}, {65: 're', 67: 'are'}], [{65: 'This', 67: 'this'}, {65: \"'ve\"}], [{65: 'This'}, {65: 've', 67: 'have'}], [{65: 'This', 67: 'this'}, {65: \"'d\", 67: \"'d\"}], [{65: 'This', 67: 'this'}, {65: 'd', 67: \"'d\"}], [{65: 'This', 67: 'this'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'This', 67: 'this'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'these', 67: 'these'}, {65: \"'s\", 67: \"'s\"}], [{65: 'these', 67: 'these'}, {65: 's'}], [{65: 'these', 67: 'these'}, {65: \"'ll\", 67: 'will'}], [{65: 'these', 67: 'these'}, {65: 'll', 67: 'will'}], [{65: 'these', 67: 'these'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'these', 67: 'these'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'these', 67: 'these'}, {65: \"'re\", 67: 'are'}], [{65: 'these', 67: 'these'}, {65: 're', 67: 'are'}], [{65: 'these', 67: 'these'}, {65: \"'ve\"}], [{65: 'these'}, {65: 've', 67: 'have'}], [{65: 'these', 67: 'these'}, {65: \"'d\", 67: \"'d\"}], [{65: 'these', 67: 'these'}, {65: 'd', 67: \"'d\"}], [{65: 'these', 67: 'these'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'these', 67: 'these'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'These', 67: 'these'}, {65: \"'s\", 67: \"'s\"}], [{65: 'These', 67: 'these'}, {65: 's'}], [{65: 'These', 67: 'these'}, {65: \"'ll\", 67: 'will'}], [{65: 'These', 67: 'these'}, {65: 'll', 67: 'will'}], [{65: 'These', 67: 'these'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'These', 67: 'these'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'These', 67: 'these'}, {65: \"'re\", 67: 'are'}], [{65: 'These', 67: 'these'}, {65: 're', 67: 'are'}], [{65: 'These', 67: 'these'}, {65: \"'ve\"}], [{65: 'These'}, {65: 've', 67: 'have'}], [{65: 'These', 67: 'these'}, {65: \"'d\", 67: \"'d\"}], [{65: 'These', 67: 'these'}, {65: 'd', 67: \"'d\"}], [{65: 'These', 67: 'these'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'These', 67: 'these'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'those', 67: 'those'}, {65: \"'s\", 67: \"'s\"}], [{65: 'those', 67: 'those'}, {65: 's'}], [{65: 'those', 67: 'those'}, {65: \"'ll\", 67: 'will'}], [{65: 'those', 67: 'those'}, {65: 'll', 67: 'will'}], [{65: 'those', 67: 'those'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'those', 67: 'those'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'those', 67: 'those'}, {65: \"'re\", 67: 'are'}], [{65: 'those', 67: 'those'}, {65: 're', 67: 'are'}], [{65: 'those', 67: 'those'}, {65: \"'ve\"}], [{65: 'those'}, {65: 've', 67: 'have'}], [{65: 'those', 67: 'those'}, {65: \"'d\", 67: \"'d\"}], [{65: 'those', 67: 'those'}, {65: 'd', 67: \"'d\"}], [{65: 'those', 67: 'those'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'those', 67: 'those'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: \"'s\", 67: \"'s\"}], [{65: 'Those', 67: 'those'}, {65: 's'}], [{65: 'Those', 67: 'those'}, {65: \"'ll\", 67: 'will'}], [{65: 'Those', 67: 'those'}, {65: 'll', 67: 'will'}], [{65: 'Those', 67: 'those'}, {65: \"'ll\", 67: 'will'}, {65: \"'ve\", 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: 'll', 67: 'will'}, {65: 've', 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: \"'re\", 67: 'are'}], [{65: 'Those', 67: 'those'}, {65: 're', 67: 'are'}], [{65: 'Those', 67: 'those'}, {65: \"'ve\"}], [{65: 'Those'}, {65: 've', 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: \"'d\", 67: \"'d\"}], [{65: 'Those', 67: 'those'}, {65: 'd', 67: \"'d\"}], [{65: 'Those', 67: 'those'}, {65: \"'d\", 67: 'would'}, {65: \"'ve\", 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: 'd', 67: 'would'}, {65: 've', 67: 'have'}], [{65: 'ca', 67: 'can'}, {65: \"n't\", 67: 'not'}], [{65: 'ca', 67: 'can'}, {65: 'nt', 67: 'not'}], [{65: 'ca', 67: 'can'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'ca', 67: 'can'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Ca', 67: 'can'}, {65: \"n't\", 67: 'not'}], [{65: 'Ca', 67: 'can'}, {65: 'nt', 67: 'not'}], [{65: 'Ca', 67: 'can'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Ca', 67: 'can'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'could', 67: 'could'}, {65: \"n't\", 67: 'not'}], [{65: 'could', 67: 'could'}, {65: 'nt', 67: 'not'}], [{65: 'could', 67: 'could'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'could', 67: 'could'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Could', 67: 'could'}, {65: \"n't\", 67: 'not'}], [{65: 'Could', 67: 'could'}, {65: 'nt', 67: 'not'}], [{65: 'Could', 67: 'could'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Could', 67: 'could'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'do', 67: 'do'}, {65: \"n't\", 67: 'not'}], [{65: 'do', 67: 'do'}, {65: 'nt', 67: 'not'}], [{65: 'do', 67: 'do'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'do', 67: 'do'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Do', 67: 'do'}, {65: \"n't\", 67: 'not'}], [{65: 'Do', 67: 'do'}, {65: 'nt', 67: 'not'}], [{65: 'Do', 67: 'do'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Do', 67: 'do'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'does', 67: 'does'}, {65: \"n't\", 67: 'not'}], [{65: 'does', 67: 'does'}, {65: 'nt', 67: 'not'}], [{65: 'does', 67: 'does'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'does', 67: 'does'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Does', 67: 'does'}, {65: \"n't\", 67: 'not'}], [{65: 'Does', 67: 'does'}, {65: 'nt', 67: 'not'}], [{65: 'Does', 67: 'does'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Does', 67: 'does'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'did', 67: 'do'}, {65: \"n't\", 67: 'not'}], [{65: 'did', 67: 'do'}, {65: 'nt', 67: 'not'}], [{65: 'did', 67: 'do'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'did', 67: 'do'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Did', 67: 'do'}, {65: \"n't\", 67: 'not'}], [{65: 'Did', 67: 'do'}, {65: 'nt', 67: 'not'}], [{65: 'Did', 67: 'do'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Did', 67: 'do'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'had', 67: 'have'}, {65: \"n't\", 67: 'not'}], [{65: 'had', 67: 'have'}, {65: 'nt', 67: 'not'}], [{65: 'had', 67: 'have'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'had', 67: 'have'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Had', 67: 'have'}, {65: \"n't\", 67: 'not'}], [{65: 'Had', 67: 'have'}, {65: 'nt', 67: 'not'}], [{65: 'Had', 67: 'have'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Had', 67: 'have'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'may', 67: 'may'}, {65: \"n't\", 67: 'not'}], [{65: 'may', 67: 'may'}, {65: 'nt', 67: 'not'}], [{65: 'may', 67: 'may'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'may', 67: 'may'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'May', 67: 'may'}, {65: \"n't\", 67: 'not'}], [{65: 'May', 67: 'may'}, {65: 'nt', 67: 'not'}], [{65: 'May', 67: 'may'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'May', 67: 'may'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'might', 67: 'might'}, {65: \"n't\", 67: 'not'}], [{65: 'might', 67: 'might'}, {65: 'nt', 67: 'not'}], [{65: 'might', 67: 'might'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'might', 67: 'might'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Might', 67: 'might'}, {65: \"n't\", 67: 'not'}], [{65: 'Might', 67: 'might'}, {65: 'nt', 67: 'not'}], [{65: 'Might', 67: 'might'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Might', 67: 'might'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'must', 67: 'must'}, {65: \"n't\", 67: 'not'}], [{65: 'must', 67: 'must'}, {65: 'nt', 67: 'not'}], [{65: 'must', 67: 'must'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'must', 67: 'must'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Must', 67: 'must'}, {65: \"n't\", 67: 'not'}], [{65: 'Must', 67: 'must'}, {65: 'nt', 67: 'not'}], [{65: 'Must', 67: 'must'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Must', 67: 'must'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'need', 67: 'need'}, {65: \"n't\", 67: 'not'}], [{65: 'need', 67: 'need'}, {65: 'nt', 67: 'not'}], [{65: 'need', 67: 'need'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'need', 67: 'need'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Need', 67: 'need'}, {65: \"n't\", 67: 'not'}], [{65: 'Need', 67: 'need'}, {65: 'nt', 67: 'not'}], [{65: 'Need', 67: 'need'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Need', 67: 'need'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'ought', 67: 'ought'}, {65: \"n't\", 67: 'not'}], [{65: 'ought', 67: 'ought'}, {65: 'nt', 67: 'not'}], [{65: 'ought', 67: 'ought'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'ought', 67: 'ought'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Ought', 67: 'ought'}, {65: \"n't\", 67: 'not'}], [{65: 'Ought', 67: 'ought'}, {65: 'nt', 67: 'not'}], [{65: 'Ought', 67: 'ought'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Ought', 67: 'ought'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'sha', 67: 'shall'}, {65: \"n't\", 67: 'not'}], [{65: 'sha', 67: 'shall'}, {65: 'nt', 67: 'not'}], [{65: 'sha', 67: 'shall'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'sha', 67: 'shall'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Sha', 67: 'shall'}, {65: \"n't\", 67: 'not'}], [{65: 'Sha', 67: 'shall'}, {65: 'nt', 67: 'not'}], [{65: 'Sha', 67: 'shall'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Sha', 67: 'shall'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'should', 67: 'should'}, {65: \"n't\", 67: 'not'}], [{65: 'should', 67: 'should'}, {65: 'nt', 67: 'not'}], [{65: 'should', 67: 'should'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'should', 67: 'should'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Should', 67: 'should'}, {65: \"n't\", 67: 'not'}], [{65: 'Should', 67: 'should'}, {65: 'nt', 67: 'not'}], [{65: 'Should', 67: 'should'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Should', 67: 'should'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'wo', 67: 'will'}, {65: \"n't\", 67: 'not'}], [{65: 'wo', 67: 'will'}, {65: 'nt', 67: 'not'}], [{65: 'wo', 67: 'will'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'wo', 67: 'will'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Wo', 67: 'will'}, {65: \"n't\", 67: 'not'}], [{65: 'Wo', 67: 'will'}, {65: 'nt', 67: 'not'}], [{65: 'Wo', 67: 'will'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Wo', 67: 'will'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'would', 67: 'would'}, {65: \"n't\", 67: 'not'}], [{65: 'would', 67: 'would'}, {65: 'nt', 67: 'not'}], [{65: 'would', 67: 'would'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'would', 67: 'would'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'Would', 67: 'would'}, {65: \"n't\", 67: 'not'}], [{65: 'Would', 67: 'would'}, {65: 'nt', 67: 'not'}], [{65: 'Would', 67: 'would'}, {65: \"n't\", 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Would', 67: 'would'}, {65: 'nt', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'could', 67: 'could'}, {65: \"'ve\"}], [{65: 'could', 67: 'could'}, {65: 've'}], [{65: 'Could', 67: 'could'}, {65: \"'ve\"}], [{65: 'Could', 67: 'could'}, {65: 've'}], [{65: 'might', 67: 'might'}, {65: \"'ve\"}], [{65: 'might', 67: 'might'}, {65: 've'}], [{65: 'Might', 67: 'might'}, {65: \"'ve\"}], [{65: 'Might', 67: 'might'}, {65: 've'}], [{65: 'must', 67: 'must'}, {65: \"'ve\"}], [{65: 'must', 67: 'must'}, {65: 've'}], [{65: 'Must', 67: 'must'}, {65: \"'ve\"}], [{65: 'Must', 67: 'must'}, {65: 've'}], [{65: 'should', 67: 'should'}, {65: \"'ve\"}], [{65: 'should', 67: 'should'}, {65: 've'}], [{65: 'Should', 67: 'should'}, {65: \"'ve\"}], [{65: 'Should', 67: 'should'}, {65: 've'}], [{65: 'would', 67: 'would'}, {65: \"'ve\"}], [{65: 'would', 67: 'would'}, {65: 've'}], [{65: 'Would', 67: 'would'}, {65: \"'ve\"}], [{65: 'Would', 67: 'would'}, {65: 've'}], [{65: 'ai'}, {65: \"n't\", 67: 'not'}], [{65: 'ai'}, {65: 'nt', 67: 'not'}], [{65: 'Ai'}, {65: \"n't\", 67: 'not'}], [{65: 'Ai'}, {65: 'nt', 67: 'not'}], [{65: 'are', 67: 'are'}, {65: \"n't\", 67: 'not'}], [{65: 'are', 67: 'are'}, {65: 'nt', 67: 'not'}], [{65: 'Are', 67: 'are'}, {65: \"n't\", 67: 'not'}], [{65: 'Are', 67: 'are'}, {65: 'nt', 67: 'not'}], [{65: 'is', 67: 'is'}, {65: \"n't\", 67: 'not'}], [{65: 'is', 67: 'is'}, {65: 'nt', 67: 'not'}], [{65: 'Is', 67: 'is'}, {65: \"n't\", 67: 'not'}], [{65: 'Is', 67: 'is'}, {65: 'nt', 67: 'not'}], [{65: 'was', 67: 'was'}, {65: \"n't\", 67: 'not'}], [{65: 'was', 67: 'was'}, {65: 'nt', 67: 'not'}], [{65: 'Was', 67: 'was'}, {65: \"n't\", 67: 'not'}], [{65: 'Was', 67: 'was'}, {65: 'nt', 67: 'not'}], [{65: 'were', 67: 'were'}, {65: \"n't\", 67: 'not'}], [{65: 'were', 67: 'were'}, {65: 'nt', 67: 'not'}], [{65: 'Were', 67: 'were'}, {65: \"n't\", 67: 'not'}], [{65: 'Were', 67: 'were'}, {65: 'nt', 67: 'not'}], [{65: 'have', 67: 'have'}, {65: \"n't\", 67: 'not'}], [{65: 'have', 67: 'have'}, {65: 'nt', 67: 'not'}], [{65: 'Have', 67: 'have'}, {65: \"n't\", 67: 'not'}], [{65: 'Have', 67: 'have'}, {65: 'nt', 67: 'not'}], [{65: 'has', 67: 'has'}, {65: \"n't\", 67: 'not'}], [{65: 'has', 67: 'has'}, {65: 'nt', 67: 'not'}], [{65: 'Has', 67: 'has'}, {65: \"n't\", 67: 'not'}], [{65: 'Has', 67: 'has'}, {65: 'nt', 67: 'not'}], [{65: 'dare', 67: 'dare'}, {65: \"n't\", 67: 'not'}], [{65: 'dare', 67: 'dare'}, {65: 'nt', 67: 'not'}], [{65: 'Dare', 67: 'dare'}, {65: \"n't\", 67: 'not'}], [{65: 'Dare', 67: 'dare'}, {65: 'nt', 67: 'not'}], [{65: 'doin', 67: 'doing'}], [{65: \"doin'\", 67: 'doing'}], [{65: 'Doin', 67: 'doing'}], [{65: \"Doin'\", 67: 'doing'}], [{65: 'goin', 67: 'going'}], [{65: \"goin'\", 67: 'going'}], [{65: 'Goin', 67: 'going'}], [{65: \"Goin'\", 67: 'going'}], [{65: 'nothin', 67: 'nothing'}], [{65: \"nothin'\", 67: 'nothing'}], [{65: 'Nothin', 67: 'nothing'}], [{65: \"Nothin'\", 67: 'nothing'}], [{65: 'nuthin', 67: 'nothing'}], [{65: \"nuthin'\", 67: 'nothing'}], [{65: 'Nuthin', 67: 'nothing'}], [{65: \"Nuthin'\", 67: 'nothing'}], [{65: 'ol', 67: 'old'}], [{65: \"ol'\", 67: 'old'}], [{65: 'Ol', 67: 'old'}], [{65: \"Ol'\", 67: 'old'}], [{65: 'somethin', 67: 'something'}], [{65: \"somethin'\", 67: 'something'}], [{65: 'Somethin', 67: 'something'}], [{65: \"Somethin'\", 67: 'something'}], [{65: 'em', 67: 'them'}], [{65: \"'em\", 67: 'them'}], [{65: 'll', 67: 'will'}], [{65: \"'ll\", 67: 'will'}], [{65: 'nuff', 67: 'enough'}], [{65: \"'nuff\", 67: 'enough'}], [{65: '1'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '1'}, {65: 'am', 67: 'a.m.'}], [{65: '1'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '1'}, {65: 'pm', 67: 'p.m.'}], [{65: '2'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '2'}, {65: 'am', 67: 'a.m.'}], [{65: '2'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '2'}, {65: 'pm', 67: 'p.m.'}], [{65: '3'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '3'}, {65: 'am', 67: 'a.m.'}], [{65: '3'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '3'}, {65: 'pm', 67: 'p.m.'}], [{65: '4'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '4'}, {65: 'am', 67: 'a.m.'}], [{65: '4'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '4'}, {65: 'pm', 67: 'p.m.'}], [{65: '5'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '5'}, {65: 'am', 67: 'a.m.'}], [{65: '5'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '5'}, {65: 'pm', 67: 'p.m.'}], [{65: '6'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '6'}, {65: 'am', 67: 'a.m.'}], [{65: '6'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '6'}, {65: 'pm', 67: 'p.m.'}], [{65: '7'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '7'}, {65: 'am', 67: 'a.m.'}], [{65: '7'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '7'}, {65: 'pm', 67: 'p.m.'}], [{65: '8'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '8'}, {65: 'am', 67: 'a.m.'}], [{65: '8'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '8'}, {65: 'pm', 67: 'p.m.'}], [{65: '9'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '9'}, {65: 'am', 67: 'a.m.'}], [{65: '9'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '9'}, {65: 'pm', 67: 'p.m.'}], [{65: '10'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '10'}, {65: 'am', 67: 'a.m.'}], [{65: '10'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '10'}, {65: 'pm', 67: 'p.m.'}], [{65: '11'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '11'}, {65: 'am', 67: 'a.m.'}], [{65: '11'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '11'}, {65: 'pm', 67: 'p.m.'}], [{65: '12'}, {65: 'a.m.', 67: 'a.m.'}], [{65: '12'}, {65: 'am', 67: 'a.m.'}], [{65: '12'}, {65: 'p.m.', 67: 'p.m.'}], [{65: '12'}, {65: 'pm', 67: 'p.m.'}], [{65: \"y'\", 67: 'you'}, {65: 'all'}], [{65: 'y', 67: 'you'}, {65: 'all'}], [{65: 'how'}, {65: \"'d\"}, {65: \"'y\", 67: 'you'}], [{65: 'How', 67: 'how'}, {65: \"'d\"}, {65: \"'y\", 67: 'you'}], [{65: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'not'}, {65: 've', 67: 'have'}], [{65: 'Not', 67: 'not'}, {65: \"'ve\", 67: 'have'}], [{65: 'Not', 67: 'not'}, {65: 've', 67: 'have'}], [{65: 'can'}, {65: 'not'}], [{65: 'Can', 67: 'can'}, {65: 'not'}], [{65: 'gon', 67: 'going'}, {65: 'na', 67: 'to'}], [{65: 'Gon', 67: 'going'}, {65: 'na', 67: 'to'}], [{65: 'got'}, {65: 'ta', 67: 'to'}], [{65: 'Got', 67: 'got'}, {65: 'ta', 67: 'to'}], [{65: 'let'}, {65: \"'s\", 67: 'us'}], [{65: 'Let', 67: 'let'}, {65: \"'s\", 67: 'us'}], [{65: \"c'm\", 67: 'come'}, {65: 'on'}], [{65: \"C'm\", 67: 'come'}, {65: 'on'}], [{65: \"'S\", 67: \"'s\"}], [{65: \"'s\", 67: \"'s\"}], [{65: '‘S', 67: \"'s\"}], [{65: '‘s', 67: \"'s\"}], [{65: 'and/or', 67: 'and/or'}], [{65: 'w/o', 67: 'without'}], [{65: \"'re\", 67: 'are'}], [{65: \"'Cause\", 67: 'because'}], [{65: \"'cause\", 67: 'because'}], [{65: \"'cos\", 67: 'because'}], [{65: \"'Cos\", 67: 'because'}], [{65: \"'coz\", 67: 'because'}], [{65: \"'Coz\", 67: 'because'}], [{65: \"'cuz\", 67: 'because'}], [{65: \"'Cuz\", 67: 'because'}], [{65: \"'bout\", 67: 'about'}], [{65: \"ma'am\", 67: 'madam'}], [{65: \"Ma'am\", 67: 'madam'}], [{65: \"o'clock\", 67: \"o'clock\"}], [{65: \"O'clock\", 67: \"o'clock\"}], [{65: \"lovin'\", 67: 'loving'}], [{65: \"Lovin'\", 67: 'loving'}], [{65: 'lovin', 67: 'loving'}], [{65: 'Lovin', 67: 'loving'}], [{65: \"havin'\", 67: 'having'}], [{65: \"Havin'\", 67: 'having'}], [{65: 'havin', 67: 'having'}], [{65: 'Havin', 67: 'having'}], [{65: 'Mt.', 67: 'Mount'}], [{65: 'Ak.', 67: 'Alaska'}], [{65: 'Ala.', 67: 'Alabama'}], [{65: 'Apr.', 67: 'April'}], [{65: 'Ariz.', 67: 'Arizona'}], [{65: 'Ark.', 67: 'Arkansas'}], [{65: 'Aug.', 67: 'August'}], [{65: 'Calif.', 67: 'California'}], [{65: 'Colo.', 67: 'Colorado'}], [{65: 'Conn.', 67: 'Connecticut'}], [{65: 'Dec.', 67: 'December'}], [{65: 'Del.', 67: 'Delaware'}], [{65: 'Feb.', 67: 'February'}], [{65: 'Fla.', 67: 'Florida'}], [{65: 'Ga.', 67: 'Georgia'}], [{65: 'Ia.', 67: 'Iowa'}], [{65: 'Id.', 67: 'Idaho'}], [{65: 'Ill.', 67: 'Illinois'}], [{65: 'Ind.', 67: 'Indiana'}], [{65: 'Jan.', 67: 'January'}], [{65: 'Jul.', 67: 'July'}], [{65: 'Jun.', 67: 'June'}], [{65: 'Kan.', 67: 'Kansas'}], [{65: 'Kans.', 67: 'Kansas'}], [{65: 'Ky.', 67: 'Kentucky'}], [{65: 'La.', 67: 'Louisiana'}], [{65: 'Mar.', 67: 'March'}], [{65: 'Mass.', 67: 'Massachusetts'}], [{65: 'May.', 67: 'May'}], [{65: 'Mich.', 67: 'Michigan'}], [{65: 'Minn.', 67: 'Minnesota'}], [{65: 'Miss.', 67: 'Mississippi'}], [{65: 'N.C.', 67: 'North Carolina'}], [{65: 'N.D.', 67: 'North Dakota'}], [{65: 'N.H.', 67: 'New Hampshire'}], [{65: 'N.J.', 67: 'New Jersey'}], [{65: 'N.M.', 67: 'New Mexico'}], [{65: 'N.Y.', 67: 'New York'}], [{65: 'Neb.', 67: 'Nebraska'}], [{65: 'Nebr.', 67: 'Nebraska'}], [{65: 'Nev.', 67: 'Nevada'}], [{65: 'Nov.', 67: 'November'}], [{65: 'Oct.', 67: 'October'}], [{65: 'Okla.', 67: 'Oklahoma'}], [{65: 'Ore.', 67: 'Oregon'}], [{65: 'Pa.', 67: 'Pennsylvania'}], [{65: 'S.C.', 67: 'South Carolina'}], [{65: 'Sep.', 67: 'September'}], [{65: 'Sept.', 67: 'September'}], [{65: 'Tenn.', 67: 'Tennessee'}], [{65: 'Va.', 67: 'Virginia'}], [{65: 'Wash.', 67: 'Washington'}], [{65: 'Wis.', 67: 'Wisconsin'}], [{65: \"'d\"}], [{65: 'a.m.'}], [{65: 'Adm.'}], [{65: 'Bros.'}], [{65: 'co.'}], [{65: 'Co.'}], [{65: 'Corp.'}], [{65: 'D.C.'}], [{65: 'Dr.'}], [{65: 'e.g.'}], [{65: 'E.g.'}], [{65: 'E.G.'}], [{65: 'Gen.'}], [{65: 'Gov.'}], [{65: 'i.e.'}], [{65: 'I.e.'}], [{65: 'I.E.'}], [{65: 'Inc.'}], [{65: 'Jr.'}], [{65: 'Ltd.'}], [{65: 'Md.'}], [{65: 'Messrs.'}], [{65: 'Mo.'}], [{65: 'Mont.'}], [{65: 'Mr.'}], [{65: 'Mrs.'}], [{65: 'Ms.'}], [{65: 'p.m.'}], [{65: 'Ph.D.'}], [{65: 'Prof.'}], [{65: 'Rep.'}], [{65: 'Rev.'}], [{65: 'Sen.'}], [{65: 'St.'}], [{65: 'vs.'}], [{65: 'v.s.'}], [{65: '’'}], [{65: '’’'}], [{65: ':’)'}], [{65: ':’-)'}], [{65: ':’('}], [{65: ':’-('}], [{65: 'i', 67: 'i'}, {65: '’m', 67: 'am'}], [{65: 'i', 67: 'i'}, {65: '’m', 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'I', 67: 'i'}, {65: '’m', 67: 'am'}], [{65: 'I', 67: 'i'}, {65: '’m', 67: 'am'}, {65: 'a', 67: 'gonna'}], [{65: 'i', 67: 'i'}, {65: '’ll', 67: 'will'}], [{65: 'i', 67: 'i'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'i', 67: 'i'}, {65: '’d', 67: \"'d\"}], [{65: 'i', 67: 'i'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: '’ll', 67: 'will'}], [{65: 'I', 67: 'i'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: '’d', 67: \"'d\"}], [{65: 'I', 67: 'i'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: '’ll', 67: 'will'}], [{65: 'you', 67: 'you'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: '’d', 67: \"'d\"}], [{65: 'you', 67: 'you'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: '’ll', 67: 'will'}], [{65: 'You', 67: 'you'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: '’d', 67: \"'d\"}], [{65: 'You', 67: 'you'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'he', 67: 'he'}, {65: '’ll', 67: 'will'}], [{65: 'he', 67: 'he'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'he', 67: 'he'}, {65: '’d', 67: \"'d\"}], [{65: 'he', 67: 'he'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'He', 67: 'he'}, {65: '’ll', 67: 'will'}], [{65: 'He', 67: 'he'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'He', 67: 'he'}, {65: '’d', 67: \"'d\"}], [{65: 'He', 67: 'he'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'she', 67: 'she'}, {65: '’ll', 67: 'will'}], [{65: 'she', 67: 'she'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'she', 67: 'she'}, {65: '’d', 67: \"'d\"}], [{65: 'she', 67: 'she'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'She', 67: 'she'}, {65: '’ll', 67: 'will'}], [{65: 'She', 67: 'she'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'She', 67: 'she'}, {65: '’d', 67: \"'d\"}], [{65: 'She', 67: 'she'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'it', 67: 'it'}, {65: '’ll', 67: 'will'}], [{65: 'it', 67: 'it'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'it', 67: 'it'}, {65: '’d', 67: \"'d\"}], [{65: 'it', 67: 'it'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'It', 67: 'it'}, {65: '’ll', 67: 'will'}], [{65: 'It', 67: 'it'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'It', 67: 'it'}, {65: '’d', 67: \"'d\"}], [{65: 'It', 67: 'it'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: '’ll', 67: 'will'}], [{65: 'we', 67: 'we'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: '’d', 67: \"'d\"}], [{65: 'we', 67: 'we'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: '’ll', 67: 'will'}], [{65: 'We', 67: 'we'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: '’d', 67: \"'d\"}], [{65: 'We', 67: 'we'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: '’ll', 67: 'will'}], [{65: 'they', 67: 'they'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: '’d', 67: \"'d\"}], [{65: 'they', 67: 'they'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: '’ll', 67: 'will'}], [{65: 'They', 67: 'they'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: '’d', 67: \"'d\"}], [{65: 'They', 67: 'they'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'i', 67: 'i'}, {65: '’ve', 67: 'have'}], [{65: 'I', 67: 'i'}, {65: '’ve', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: '’ve', 67: 'have'}], [{65: 'You', 67: 'you'}, {65: '’ve', 67: 'have'}], [{65: 'we', 67: 'we'}, {65: '’ve', 67: 'have'}], [{65: 'We', 67: 'we'}, {65: '’ve', 67: 'have'}], [{65: 'they', 67: 'they'}, {65: '’ve', 67: 'have'}], [{65: 'They', 67: 'they'}, {65: '’ve', 67: 'have'}], [{65: 'you', 67: 'you'}, {65: '’re', 67: 'are'}], [{65: 'You', 67: 'you'}, {65: '’re', 67: 'are'}], [{65: 'we', 67: 'we'}, {65: '’re', 67: 'are'}], [{65: 'We', 67: 'we'}, {65: '’re', 67: 'are'}], [{65: 'they', 67: 'they'}, {65: '’re', 67: 'are'}], [{65: 'They', 67: 'they'}, {65: '’re', 67: 'are'}], [{65: 'he', 67: 'he'}, {65: '’s', 67: \"'s\"}], [{65: 'He', 67: 'he'}, {65: '’s', 67: \"'s\"}], [{65: 'she', 67: 'she'}, {65: '’s', 67: \"'s\"}], [{65: 'She', 67: 'she'}, {65: '’s', 67: \"'s\"}], [{65: 'it', 67: 'it'}, {65: '’s', 67: \"'s\"}], [{65: 'It', 67: 'it'}, {65: '’s', 67: \"'s\"}], [{65: 'who', 67: 'who'}, {65: '’s', 67: \"'s\"}], [{65: 'who', 67: 'who'}, {65: '’ll', 67: 'will'}], [{65: 'who', 67: 'who'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'who', 67: 'who'}, {65: '’re', 67: 'are'}], [{65: 'who', 67: 'who'}, {65: '’ve'}], [{65: 'who', 67: 'who'}, {65: '’d', 67: \"'d\"}], [{65: 'who', 67: 'who'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: '’s', 67: \"'s\"}], [{65: 'Who', 67: 'who'}, {65: '’ll', 67: 'will'}], [{65: 'Who', 67: 'who'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'Who', 67: 'who'}, {65: '’re', 67: 'are'}], [{65: 'Who', 67: 'who'}, {65: '’ve'}], [{65: 'Who', 67: 'who'}, {65: '’d', 67: \"'d\"}], [{65: 'Who', 67: 'who'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'what', 67: 'what'}, {65: '’s', 67: \"'s\"}], [{65: 'what', 67: 'what'}, {65: '’ll', 67: 'will'}], [{65: 'what', 67: 'what'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'what', 67: 'what'}, {65: '’re', 67: 'are'}], [{65: 'what', 67: 'what'}, {65: '’ve'}], [{65: 'what', 67: 'what'}, {65: '’d', 67: \"'d\"}], [{65: 'what', 67: 'what'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'What', 67: 'what'}, {65: '’s', 67: \"'s\"}], [{65: 'What', 67: 'what'}, {65: '’ll', 67: 'will'}], [{65: 'What', 67: 'what'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'What', 67: 'what'}, {65: '’re', 67: 'are'}], [{65: 'What', 67: 'what'}, {65: '’ve'}], [{65: 'What', 67: 'what'}, {65: '’d', 67: \"'d\"}], [{65: 'What', 67: 'what'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'when', 67: 'when'}, {65: '’s', 67: \"'s\"}], [{65: 'when', 67: 'when'}, {65: '’ll', 67: 'will'}], [{65: 'when', 67: 'when'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'when', 67: 'when'}, {65: '’re', 67: 'are'}], [{65: 'when', 67: 'when'}, {65: '’ve'}], [{65: 'when', 67: 'when'}, {65: '’d', 67: \"'d\"}], [{65: 'when', 67: 'when'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'When', 67: 'when'}, {65: '’s', 67: \"'s\"}], [{65: 'When', 67: 'when'}, {65: '’ll', 67: 'will'}], [{65: 'When', 67: 'when'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'When', 67: 'when'}, {65: '’re', 67: 'are'}], [{65: 'When', 67: 'when'}, {65: '’ve'}], [{65: 'When', 67: 'when'}, {65: '’d', 67: \"'d\"}], [{65: 'When', 67: 'when'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'where', 67: 'where'}, {65: '’s', 67: \"'s\"}], [{65: 'where', 67: 'where'}, {65: '’ll', 67: 'will'}], [{65: 'where', 67: 'where'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'where', 67: 'where'}, {65: '’re', 67: 'are'}], [{65: 'where', 67: 'where'}, {65: '’ve'}], [{65: 'where', 67: 'where'}, {65: '’d', 67: \"'d\"}], [{65: 'where', 67: 'where'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: '’s', 67: \"'s\"}], [{65: 'Where', 67: 'where'}, {65: '’ll', 67: 'will'}], [{65: 'Where', 67: 'where'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'Where', 67: 'where'}, {65: '’re', 67: 'are'}], [{65: 'Where', 67: 'where'}, {65: '’ve'}], [{65: 'Where', 67: 'where'}, {65: '’d', 67: \"'d\"}], [{65: 'Where', 67: 'where'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'why', 67: 'why'}, {65: '’s', 67: \"'s\"}], [{65: 'why', 67: 'why'}, {65: '’ll', 67: 'will'}], [{65: 'why', 67: 'why'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'why', 67: 'why'}, {65: '’re', 67: 'are'}], [{65: 'why', 67: 'why'}, {65: '’ve'}], [{65: 'why', 67: 'why'}, {65: '’d', 67: \"'d\"}], [{65: 'why', 67: 'why'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: '’s', 67: \"'s\"}], [{65: 'Why', 67: 'why'}, {65: '’ll', 67: 'will'}], [{65: 'Why', 67: 'why'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'Why', 67: 'why'}, {65: '’re', 67: 'are'}], [{65: 'Why', 67: 'why'}, {65: '’ve'}], [{65: 'Why', 67: 'why'}, {65: '’d', 67: \"'d\"}], [{65: 'Why', 67: 'why'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'how', 67: 'how'}, {65: '’s', 67: \"'s\"}], [{65: 'how', 67: 'how'}, {65: '’ll', 67: 'will'}], [{65: 'how', 67: 'how'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'how', 67: 'how'}, {65: '’re', 67: 'are'}], [{65: 'how', 67: 'how'}, {65: '’ve'}], [{65: 'how', 67: 'how'}, {65: '’d', 67: \"'d\"}], [{65: 'how', 67: 'how'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'How', 67: 'how'}, {65: '’s', 67: \"'s\"}], [{65: 'How', 67: 'how'}, {65: '’ll', 67: 'will'}], [{65: 'How', 67: 'how'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'How', 67: 'how'}, {65: '’re', 67: 'are'}], [{65: 'How', 67: 'how'}, {65: '’ve'}], [{65: 'How', 67: 'how'}, {65: '’d', 67: \"'d\"}], [{65: 'How', 67: 'how'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'there', 67: 'there'}, {65: '’s', 67: \"'s\"}], [{65: 'there', 67: 'there'}, {65: '’ll', 67: 'will'}], [{65: 'there', 67: 'there'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'there', 67: 'there'}, {65: '’re', 67: 'are'}], [{65: 'there', 67: 'there'}, {65: '’ve'}], [{65: 'there', 67: 'there'}, {65: '’d', 67: \"'d\"}], [{65: 'there', 67: 'there'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'There', 67: 'there'}, {65: '’s', 67: \"'s\"}], [{65: 'There', 67: 'there'}, {65: '’ll', 67: 'will'}], [{65: 'There', 67: 'there'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'There', 67: 'there'}, {65: '’re', 67: 'are'}], [{65: 'There', 67: 'there'}, {65: '’ve'}], [{65: 'There', 67: 'there'}, {65: '’d', 67: \"'d\"}], [{65: 'There', 67: 'there'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'that', 67: 'that'}, {65: '’s', 67: \"'s\"}], [{65: 'that', 67: 'that'}, {65: '’ll', 67: 'will'}], [{65: 'that', 67: 'that'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'that', 67: 'that'}, {65: '’re', 67: 'are'}], [{65: 'that', 67: 'that'}, {65: '’ve'}], [{65: 'that', 67: 'that'}, {65: '’d', 67: \"'d\"}], [{65: 'that', 67: 'that'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'That', 67: 'that'}, {65: '’s', 67: \"'s\"}], [{65: 'That', 67: 'that'}, {65: '’ll', 67: 'will'}], [{65: 'That', 67: 'that'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'That', 67: 'that'}, {65: '’re', 67: 'are'}], [{65: 'That', 67: 'that'}, {65: '’ve'}], [{65: 'That', 67: 'that'}, {65: '’d', 67: \"'d\"}], [{65: 'That', 67: 'that'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'this', 67: 'this'}, {65: '’s', 67: \"'s\"}], [{65: 'this', 67: 'this'}, {65: '’ll', 67: 'will'}], [{65: 'this', 67: 'this'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'this', 67: 'this'}, {65: '’re', 67: 'are'}], [{65: 'this', 67: 'this'}, {65: '’ve'}], [{65: 'this', 67: 'this'}, {65: '’d', 67: \"'d\"}], [{65: 'this', 67: 'this'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'This', 67: 'this'}, {65: '’s', 67: \"'s\"}], [{65: 'This', 67: 'this'}, {65: '’ll', 67: 'will'}], [{65: 'This', 67: 'this'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'This', 67: 'this'}, {65: '’re', 67: 'are'}], [{65: 'This', 67: 'this'}, {65: '’ve'}], [{65: 'This', 67: 'this'}, {65: '’d', 67: \"'d\"}], [{65: 'This', 67: 'this'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'these', 67: 'these'}, {65: '’s', 67: \"'s\"}], [{65: 'these', 67: 'these'}, {65: '’ll', 67: 'will'}], [{65: 'these', 67: 'these'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'these', 67: 'these'}, {65: '’re', 67: 'are'}], [{65: 'these', 67: 'these'}, {65: '’ve'}], [{65: 'these', 67: 'these'}, {65: '’d', 67: \"'d\"}], [{65: 'these', 67: 'these'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'These', 67: 'these'}, {65: '’s', 67: \"'s\"}], [{65: 'These', 67: 'these'}, {65: '’ll', 67: 'will'}], [{65: 'These', 67: 'these'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'These', 67: 'these'}, {65: '’re', 67: 'are'}], [{65: 'These', 67: 'these'}, {65: '’ve'}], [{65: 'These', 67: 'these'}, {65: '’d', 67: \"'d\"}], [{65: 'These', 67: 'these'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'those', 67: 'those'}, {65: '’s', 67: \"'s\"}], [{65: 'those', 67: 'those'}, {65: '’ll', 67: 'will'}], [{65: 'those', 67: 'those'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'those', 67: 'those'}, {65: '’re', 67: 'are'}], [{65: 'those', 67: 'those'}, {65: '’ve'}], [{65: 'those', 67: 'those'}, {65: '’d', 67: \"'d\"}], [{65: 'those', 67: 'those'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: '’s', 67: \"'s\"}], [{65: 'Those', 67: 'those'}, {65: '’ll', 67: 'will'}], [{65: 'Those', 67: 'those'}, {65: '’ll', 67: 'will'}, {65: '’ve', 67: 'have'}], [{65: 'Those', 67: 'those'}, {65: '’re', 67: 'are'}], [{65: 'Those', 67: 'those'}, {65: '’ve'}], [{65: 'Those', 67: 'those'}, {65: '’d', 67: \"'d\"}], [{65: 'Those', 67: 'those'}, {65: '’d', 67: 'would'}, {65: '’ve', 67: 'have'}], [{65: 'ca', 67: 'can'}, {65: 'n’t', 67: 'not'}], [{65: 'ca', 67: 'can'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Ca', 67: 'can'}, {65: 'n’t', 67: 'not'}], [{65: 'Ca', 67: 'can'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'could', 67: 'could'}, {65: 'n’t', 67: 'not'}], [{65: 'could', 67: 'could'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Could', 67: 'could'}, {65: 'n’t', 67: 'not'}], [{65: 'Could', 67: 'could'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'do', 67: 'do'}, {65: 'n’t', 67: 'not'}], [{65: 'do', 67: 'do'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Do', 67: 'do'}, {65: 'n’t', 67: 'not'}], [{65: 'Do', 67: 'do'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'does', 67: 'does'}, {65: 'n’t', 67: 'not'}], [{65: 'does', 67: 'does'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Does', 67: 'does'}, {65: 'n’t', 67: 'not'}], [{65: 'Does', 67: 'does'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'did', 67: 'do'}, {65: 'n’t', 67: 'not'}], [{65: 'did', 67: 'do'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Did', 67: 'do'}, {65: 'n’t', 67: 'not'}], [{65: 'Did', 67: 'do'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'had', 67: 'have'}, {65: 'n’t', 67: 'not'}], [{65: 'had', 67: 'have'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Had', 67: 'have'}, {65: 'n’t', 67: 'not'}], [{65: 'Had', 67: 'have'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'may', 67: 'may'}, {65: 'n’t', 67: 'not'}], [{65: 'may', 67: 'may'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'May', 67: 'may'}, {65: 'n’t', 67: 'not'}], [{65: 'May', 67: 'may'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'might', 67: 'might'}, {65: 'n’t', 67: 'not'}], [{65: 'might', 67: 'might'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Might', 67: 'might'}, {65: 'n’t', 67: 'not'}], [{65: 'Might', 67: 'might'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'must', 67: 'must'}, {65: 'n’t', 67: 'not'}], [{65: 'must', 67: 'must'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Must', 67: 'must'}, {65: 'n’t', 67: 'not'}], [{65: 'Must', 67: 'must'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'need', 67: 'need'}, {65: 'n’t', 67: 'not'}], [{65: 'need', 67: 'need'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Need', 67: 'need'}, {65: 'n’t', 67: 'not'}], [{65: 'Need', 67: 'need'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'ought', 67: 'ought'}, {65: 'n’t', 67: 'not'}], [{65: 'ought', 67: 'ought'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Ought', 67: 'ought'}, {65: 'n’t', 67: 'not'}], [{65: 'Ought', 67: 'ought'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'sha', 67: 'shall'}, {65: 'n’t', 67: 'not'}], [{65: 'sha', 67: 'shall'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Sha', 67: 'shall'}, {65: 'n’t', 67: 'not'}], [{65: 'Sha', 67: 'shall'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'should', 67: 'should'}, {65: 'n’t', 67: 'not'}], [{65: 'should', 67: 'should'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Should', 67: 'should'}, {65: 'n’t', 67: 'not'}], [{65: 'Should', 67: 'should'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'wo', 67: 'will'}, {65: 'n’t', 67: 'not'}], [{65: 'wo', 67: 'will'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Wo', 67: 'will'}, {65: 'n’t', 67: 'not'}], [{65: 'Wo', 67: 'will'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'would', 67: 'would'}, {65: 'n’t', 67: 'not'}], [{65: 'would', 67: 'would'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Would', 67: 'would'}, {65: 'n’t', 67: 'not'}], [{65: 'Would', 67: 'would'}, {65: 'n’t', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'could', 67: 'could'}, {65: '’ve'}], [{65: 'Could', 67: 'could'}, {65: '’ve'}], [{65: 'might', 67: 'might'}, {65: '’ve'}], [{65: 'Might', 67: 'might'}, {65: '’ve'}], [{65: 'must', 67: 'must'}, {65: '’ve'}], [{65: 'Must', 67: 'must'}, {65: '’ve'}], [{65: 'should', 67: 'should'}, {65: '’ve'}], [{65: 'Should', 67: 'should'}, {65: '’ve'}], [{65: 'would', 67: 'would'}, {65: '’ve'}], [{65: 'Would', 67: 'would'}, {65: '’ve'}], [{65: 'ai'}, {65: 'n’t', 67: 'not'}], [{65: 'Ai'}, {65: 'n’t', 67: 'not'}], [{65: 'are', 67: 'are'}, {65: 'n’t', 67: 'not'}], [{65: 'Are', 67: 'are'}, {65: 'n’t', 67: 'not'}], [{65: 'is', 67: 'is'}, {65: 'n’t', 67: 'not'}], [{65: 'Is', 67: 'is'}, {65: 'n’t', 67: 'not'}], [{65: 'was', 67: 'was'}, {65: 'n’t', 67: 'not'}], [{65: 'Was', 67: 'was'}, {65: 'n’t', 67: 'not'}], [{65: 'were', 67: 'were'}, {65: 'n’t', 67: 'not'}], [{65: 'Were', 67: 'were'}, {65: 'n’t', 67: 'not'}], [{65: 'have', 67: 'have'}, {65: 'n’t', 67: 'not'}], [{65: 'Have', 67: 'have'}, {65: 'n’t', 67: 'not'}], [{65: 'has', 67: 'has'}, {65: 'n’t', 67: 'not'}], [{65: 'Has', 67: 'has'}, {65: 'n’t', 67: 'not'}], [{65: 'dare', 67: 'dare'}, {65: 'n’t', 67: 'not'}], [{65: 'Dare', 67: 'dare'}, {65: 'n’t', 67: 'not'}], [{65: 'doin’', 67: 'doing'}], [{65: 'Doin’', 67: 'doing'}], [{65: 'goin’', 67: 'going'}], [{65: 'Goin’', 67: 'going'}], [{65: 'nothin’', 67: 'nothing'}], [{65: 'Nothin’', 67: 'nothing'}], [{65: 'nuthin’', 67: 'nothing'}], [{65: 'Nuthin’', 67: 'nothing'}], [{65: 'ol’', 67: 'old'}], [{65: 'Ol’', 67: 'old'}], [{65: 'somethin’', 67: 'something'}], [{65: 'Somethin’', 67: 'something'}], [{65: '’em', 67: 'them'}], [{65: '’ll', 67: 'will'}], [{65: '’nuff', 67: 'enough'}], [{65: 'y’', 67: 'you'}, {65: 'all'}], [{65: 'how'}, {65: '’d'}, {65: '’y', 67: 'you'}], [{65: 'How', 67: 'how'}, {65: '’d'}, {65: '’y', 67: 'you'}], [{65: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'Not', 67: 'not'}, {65: '’ve', 67: 'have'}], [{65: 'let'}, {65: '’s', 67: 'us'}], [{65: 'Let', 67: 'let'}, {65: '’s', 67: 'us'}], [{65: 'c’m', 67: 'come'}, {65: 'on'}], [{65: 'C’m', 67: 'come'}, {65: 'on'}], [{65: '’S', 67: \"'s\"}], [{65: '’s', 67: \"'s\"}], [{65: '’re', 67: 'are'}], [{65: '’Cause', 67: 'because'}], [{65: '’cause', 67: 'because'}], [{65: '’cos', 67: 'because'}], [{65: '’Cos', 67: 'because'}], [{65: '’coz', 67: 'because'}], [{65: '’Coz', 67: 'because'}], [{65: '’cuz', 67: 'because'}], [{65: '’Cuz', 67: 'because'}], [{65: '’bout', 67: 'about'}], [{65: 'ma’am', 67: 'madam'}], [{65: 'Ma’am', 67: 'madam'}], [{65: 'o’clock', 67: \"o'clock\"}], [{65: 'O’clock', 67: \"o'clock\"}], [{65: 'lovin’', 67: 'loving'}], [{65: 'Lovin’', 67: 'loving'}], [{65: 'havin’', 67: 'having'}], [{65: 'Havin’', 67: 'having'}], [{65: '’d'}]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.103258Z",
     "start_time": "2021-03-28T20:32:44.092331Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "tokens = tokenizer(\"This is a $STOCK.\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "$\n",
      "STOCK.\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.109723Z",
     "start_time": "2021-03-28T20:32:44.104174Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can add special prefixes in the form of regex by doing:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "custom_prefixes = nlp.Defaults.prefixes + [r\"\\$[a-zA-Z]+\"]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.115930Z",
     "start_time": "2021-03-28T20:32:44.111815Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "prefix_re = spacy.util.compile_prefix_regex(custom_prefixes)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.124436Z",
     "start_time": "2021-03-28T20:32:44.117070Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import re\n",
    "\n",
    "prefix_re = re.compile(r\"\\$[a-zA-Z]+\")\n",
    "tokenizer = Tokenizer(\n",
    "    nlp.vocab, prefix_search=prefix_re.search, suffix_search=suffix_re.search\n",
    ")\n",
    "\n",
    "tokens = tokenizer(\"This is a $STOCK.\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "$STOCK\n",
      ".\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.134261Z",
     "start_time": "2021-03-28T20:32:44.125454Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can add also special-case tokenization rules. This mechanism is also used to add custom tokenizer exceptions to the language data. See the usage guide on the [languages data](https://spacy.io/usage/linguistic-features#language-data) and [tokenizer special cases](https://spacy.io/usage/linguistic-features#special-cases) for more details and examples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from spacy.attrs import ORTH, NORM, LOWER\n",
    "\n",
    "dont_case = [{ORTH: \"do\"}, {ORTH: \"n't\", NORM: \"not\"}]\n",
    "gimme_case = [{ORTH: \"gi\", NORM:\"give\"}, {ORTH: \"me\", NORM: \"me\"}]\n",
    "tokenizer.add_special_case(\"don't\", dont_case)\n",
    "tokenizer.add_special_case(\"gimme\", gimme_case)\n",
    "tokens = tokenizer(\"Yo! gimme five!\")\n",
    "for token in tokens:\n",
    "    print(token.norm_)\n",
    "tokens = tokenizer(\"You don't do that\")\n",
    "for token in tokens:\n",
    "    print(token.norm_)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "[E997] Tokenizer special cases are not allowed to modify the text. This would map 'gimme' to 'gime' given token attributes '[{65: 'gi', 67: 'give'}, {65: 'me', 67: 'me'}]'.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b8c2c9d13842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgimme_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"gi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNORM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"give\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mORTH\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"me\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNORM\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"me\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"don't\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdont_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gimme\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgimme_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Yo! gimme five!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.9/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep_learning/lib/python3.9/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E997] Tokenizer special cases are not allowed to modify the text. This would map 'gimme' to 'gime' given token attributes '[{65: 'gi', 67: 'give'}, {65: 'me', 67: 'me'}]'."
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.146952Z",
     "start_time": "2021-03-28T20:32:44.135427Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you load a model with pretrained NER (Named Entity Recognition), like `en_core_web_sm`, it is possible to make the tokenizer to merge the token for the entities it finds. Let's check what is inside the pipeline performed by `nlp`:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "nlp.pipeline"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f8850258430>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f8860afcbe0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f8860afcca0>)]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 21;\n                var nbb_unformatted_code = \"nlp.pipeline\";\n                var nbb_formatted_code = \"nlp.pipeline\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.151274Z",
     "start_time": "2021-03-28T20:32:44.148001Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There's a tagger, a dependency parser and the entity recognizer. Let's check the entities of the following sentence:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "doc = nlp(\"Apple is a $1000b company.\")\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 22;\n                var nbb_unformatted_code = \"doc = nlp(\\\"Apple is a $1000b company.\\\")\";\n                var nbb_formatted_code = \"doc = nlp(\\\"Apple is a $1000b company.\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.161196Z",
     "start_time": "2021-03-28T20:32:44.152194Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple\n",
      "is\n",
      "a\n",
      "$\n",
      "1000b\n",
      "company\n",
      ".\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 23;\n                var nbb_unformatted_code = \"for token in doc:\\n    print(token)\";\n                var nbb_formatted_code = \"for token in doc:\\n    print(token)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.166456Z",
     "start_time": "2021-03-28T20:32:44.162183Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apple ORG\n",
      "1000b MONEY\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 24;\n                var nbb_unformatted_code = \"for ent in doc.ents:\\n    print(ent, ent.label_)\";\n                var nbb_formatted_code = \"for ent in doc.ents:\\n    print(ent, ent.label_)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.172302Z",
     "start_time": "2021-03-28T20:32:44.167426Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "doc = nlp(\n",
    "    \"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\"\n",
    ")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This\n",
      "is\n",
      "Strive\n",
      "School\n",
      ".\n",
      "It\n",
      "'s\n",
      "worthy\n",
      "to\n",
      "merge\n",
      "'\n",
      "Strive\n",
      "School\n",
      "'\n",
      "as\n",
      "a\n",
      "single\n",
      "token\n",
      "instead\n",
      "of\n",
      "two\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 25;\n                var nbb_unformatted_code = \"doc = nlp(\\n    \\\"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\\\"\\n)\\n\\nfor token in doc:\\n    print(token)\";\n                var nbb_formatted_code = \"doc = nlp(\\n    \\\"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\\\"\\n)\\n\\nfor token in doc:\\n    print(token)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.200653Z",
     "start_time": "2021-03-28T20:32:44.178705Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent, ent.label_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Strive School ORG\n",
      "Strive School' ORG\n",
      "two CARDINAL\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 26;\n                var nbb_unformatted_code = \"for ent in doc.ents:\\n    print(ent, ent.label_)\";\n                var nbb_formatted_code = \"for ent in doc.ents:\\n    print(ent, ent.label_)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.207796Z",
     "start_time": "2021-03-28T20:32:44.203763Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's add \"merge_entities\" to the pipeline (you can do it only if there is the entity recognizer):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 27;\n                var nbb_unformatted_code = \"nlp.add_pipe(nlp.create_pipe(\\\"merge_entities\\\"))\";\n                var nbb_formatted_code = \"nlp.add_pipe(nlp.create_pipe(\\\"merge_entities\\\"))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.212102Z",
     "start_time": "2021-03-28T20:32:44.208805Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "nlp.pipeline"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f8850258430>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f8860afcbe0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f8860afcca0>),\n",
       " ('merge_entities', <function spacy.pipeline.functions.merge_entities(doc)>)]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 28;\n                var nbb_unformatted_code = \"nlp.pipeline\";\n                var nbb_formatted_code = \"nlp.pipeline\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.216239Z",
     "start_time": "2021-03-28T20:32:44.213031Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "doc = nlp(\n",
    "    \"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\"\n",
    ")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This\n",
      "is\n",
      "Strive School\n",
      ".\n",
      "It\n",
      "'s\n",
      "worthy\n",
      "to\n",
      "merge\n",
      "'\n",
      "Strive School'\n",
      "as\n",
      "a\n",
      "single\n",
      "token\n",
      "instead\n",
      "of\n",
      "two\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 29;\n                var nbb_unformatted_code = \"doc = nlp(\\n    \\\"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\\\"\\n)\\n\\nfor token in doc:\\n    print(token)\";\n                var nbb_formatted_code = \"doc = nlp(\\n    \\\"This is Strive School. It's worthy to merge 'Strive School' as a single token instead of two\\\"\\n)\\n\\nfor token in doc:\\n    print(token)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.233121Z",
     "start_time": "2021-03-28T20:32:44.217083Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "TEXTS = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 30;\n                var nbb_unformatted_code = \"TEXTS = [\\n    \\\"Net income was $9.4 million compared to the prior year of $2.7 million.\\\",\\n    \\\"Revenue exceeded twelve billion dollars, with a loss of $1b.\\\",\\n]\";\n                var nbb_formatted_code = \"TEXTS = [\\n    \\\"Net income was $9.4 million compared to the prior year of $2.7 million.\\\",\\n    \\\"Revenue exceeded twelve billion dollars, with a loss of $1b.\\\",\\n]\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.237660Z",
     "start_time": "2021-03-28T20:32:44.234118Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "for sentence in nlp.pipe(TEXTS):\n",
    "    for token in sentence:\n",
    "        print(token)\n",
    "    print(\"------------------\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net\n",
      "income\n",
      "was\n",
      "$9.4 million\n",
      "compared\n",
      "to\n",
      "the prior year\n",
      "of\n",
      "$2.7 million\n",
      ".\n",
      "------------------\n",
      "Revenue\n",
      "exceeded\n",
      "twelve billion dollars\n",
      ",\n",
      "with\n",
      "a\n",
      "loss\n",
      "of\n",
      "$\n",
      "1b\n",
      ".\n",
      "------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 31;\n                var nbb_unformatted_code = \"for sentence in nlp.pipe(TEXTS):\\n    for token in sentence:\\n        print(token)\\n    print(\\\"------------------\\\")\";\n                var nbb_formatted_code = \"for sentence in nlp.pipe(TEXTS):\\n    for token in sentence:\\n        print(token)\\n    print(\\\"------------------\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.257141Z",
     "start_time": "2021-03-28T20:32:44.238468Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's also possible to merge the noun chunks into one:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 32;\n                var nbb_unformatted_code = \"nlp.add_pipe(nlp.create_pipe(\\\"merge_noun_chunks\\\"))\";\n                var nbb_formatted_code = \"nlp.add_pipe(nlp.create_pipe(\\\"merge_noun_chunks\\\"))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.263274Z",
     "start_time": "2021-03-28T20:32:44.258469Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "nlp.pipeline"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f8850258430>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f8860afcbe0>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f8860afcca0>),\n",
       " ('merge_entities', <function spacy.pipeline.functions.merge_entities(doc)>),\n",
       " ('merge_noun_chunks',\n",
       "  <function spacy.pipeline.functions.merge_noun_chunks(doc)>)]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 33;\n                var nbb_unformatted_code = \"nlp.pipeline\";\n                var nbb_formatted_code = \"nlp.pipeline\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.268793Z",
     "start_time": "2021-03-28T20:32:44.264555Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "doc = nlp(\"Hello, I'm Antonio Marsella, nice to meet you.\")\n",
    "for token in doc:\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello\n",
      ",\n",
      "I\n",
      "'m\n",
      "Antonio Marsella\n",
      ",\n",
      "nice\n",
      "to\n",
      "meet\n",
      "you\n",
      ".\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 34;\n                var nbb_unformatted_code = \"doc = nlp(\\\"Hello, I'm Antonio Marsella, nice to meet you.\\\")\\nfor token in doc:\\n    print(token)\";\n                var nbb_formatted_code = \"doc = nlp(\\\"Hello, I'm Antonio Marsella, nice to meet you.\\\")\\nfor token in doc:\\n    print(token)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.282564Z",
     "start_time": "2021-03-28T20:32:44.269711Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing stop words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general, it's convenient to remove all the stop words, *i.e. very common words in a language*, because they don't help most of NLP problem such as semantic analysis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(\"Number of stop words: %d\" % len(spacy_stopwords))\n",
    "print(\"First ten stop words: %s\" % list(spacy_stopwords)[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['front', 'within', 'down', 'besides', 'each', 'until', 'more', 'take', 'have', 'various']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 35;\n                var nbb_unformatted_code = \"spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\\nprint(\\\"Number of stop words: %d\\\" % len(spacy_stopwords))\\nprint(\\\"First ten stop words: %s\\\" % list(spacy_stopwords)[:10])\";\n                var nbb_formatted_code = \"spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\\nprint(\\\"Number of stop words: %d\\\" % len(spacy_stopwords))\\nprint(\\\"First ten stop words: %s\\\" % list(spacy_stopwords)[:10])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.290058Z",
     "start_time": "2021-03-28T20:32:44.283484Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To remove them:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc if not token.is_stop]\n",
    "for token in tokens:\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "determined\n",
      "drop\n",
      "his litigation\n",
      "the monastry\n",
      ",\n",
      "relinguish\n",
      "his claims\n",
      "wood\n",
      "-\n",
      "cuting\n",
      "\n",
      "\n",
      "fishery rihgts\n",
      ".\n",
      "ready\n",
      "this becuase\n",
      "the rights\n",
      "valuable\n",
      ",\n",
      "\n",
      "\n",
      "indeed the vaguest idea\n",
      "the wood\n",
      "river\n",
      "question\n",
      ".\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 36;\n                var nbb_unformatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\ndoc = nlp(text)\\n\\ntokens = [token.text for token in doc if not token.is_stop]\\nfor token in tokens:\\n    print(token)\";\n                var nbb_formatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\ndoc = nlp(text)\\n\\ntokens = [token.text for token in doc if not token.is_stop]\\nfor token in tokens:\\n    print(token)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:32:44.317300Z",
     "start_time": "2021-03-28T20:32:44.291085Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For adding customized stop words:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "customize_stop_words = [\"computing\", \"filtered\"]\n",
    "for w in customize_stop_words:\n",
    "    nlp.vocab[w].is_stop = True"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 39;\n                var nbb_unformatted_code = \"customize_stop_words = [\\\"computing\\\", \\\"filtered\\\"]\\nfor w in customize_stop_words:\\n    nlp.vocab[w].is_stop = True\";\n                var nbb_formatted_code = \"customize_stop_words = [\\\"computing\\\", \\\"filtered\\\"]\\nfor w in customize_stop_words:\\n    nlp.vocab[w].is_stop = True\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:30.546200Z",
     "start_time": "2021-03-28T20:33:30.528780Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "In most natural languages, a root word can have many variants. For example, the word ‘play’ can be used as ‘playing’, ‘played’, ‘plays’, etc. You can think of similar examples (and there are plenty).\n",
    "\n",
    "**Stemming**\n",
    "\n",
    "Let’s first understand stemming:\n",
    "\n",
    "Stemming is a text normalization technique that cuts off the end or beginning of a word by taking into account a list of common prefixes or suffixes that could be found in that word\n",
    "It is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word\n",
    " \n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "Lemmatization, on the other hand, is an organized & step-by-step procedure of obtaining the root form of the word. It makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n",
    "\n",
    "Stemming algorithm works by cutting the suffix or prefix from the word. Lemmatization is a more powerful operation as it takes into consideration the morphological analysis of the word.\n",
    "\n",
    "Lemmatization returns the lemma, which is the root word of all its inflection forms.\n",
    "\n",
    "We can say that stemming is a quick and dirty method of chopping off words to its root form while on the other hand, lemmatization is an intelligent operation that uses dictionaries which are created by in-depth linguistic knowledge. Hence, Lemmatization helps in forming better features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))\n",
    "# not using merge_chunk_nouns\n",
    "doc = nlp(\n",
    "    u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    ")\n",
    "\n",
    "lemma_word1 = []\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "    lemma_word1.append(token.lemma_)\n",
    "lemma_word1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['determine',\n",
       " 'drop',\n",
       " 'litigation',\n",
       " 'monastry',\n",
       " ',',\n",
       " 'relinguish',\n",
       " 'claim',\n",
       " 'wood',\n",
       " '-',\n",
       " 'cut',\n",
       " '\\n',\n",
       " 'fishery',\n",
       " 'rihgts',\n",
       " '.',\n",
       " 'ready',\n",
       " 'becuase',\n",
       " 'right',\n",
       " 'valuable',\n",
       " ',',\n",
       " '\\n',\n",
       " 'vague',\n",
       " 'idea',\n",
       " 'wood',\n",
       " 'river',\n",
       " 'question',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 40
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 40;\n                var nbb_unformatted_code = \"nlp = spacy.load(\\\"en_core_web_sm\\\")\\nnlp.add_pipe(nlp.create_pipe(\\\"merge_entities\\\"))\\n# not using merge_chunk_nouns\\ndoc = nlp(\\n    u\\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n)\\n\\nlemma_word1 = []\\nfor token in doc:\\n    if token.is_stop:\\n        continue\\n    lemma_word1.append(token.lemma_)\\nlemma_word1\";\n                var nbb_formatted_code = \"nlp = spacy.load(\\\"en_core_web_sm\\\")\\nnlp.add_pipe(nlp.create_pipe(\\\"merge_entities\\\"))\\n# not using merge_chunk_nouns\\ndoc = nlp(\\n    u\\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n)\\n\\nlemma_word1 = []\\nfor token in doc:\\n    if token.is_stop:\\n        continue\\n    lemma_word1.append(token.lemma_)\\nlemma_word1\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:36.947990Z",
     "start_time": "2021-03-28T20:33:36.465998Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing the punctuation\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "\n",
    "import string\n",
    "\n",
    "text_no_punct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "text_no_punct"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'He determined to drop his litigation with the monastry and relinguish his claims to the woodcuting and \\nfishery rihgts at once He was the more ready to do this becuase the rights had become much less valuable and he had \\nindeed the vaguest idea where the wood and river in question were'"
      ]
     },
     "metadata": {},
     "execution_count": 41
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 41;\n                var nbb_unformatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\n\\nimport string\\n\\ntext_no_punct = \\\"\\\".join([char for char in text if char not in string.punctuation])\\n\\ntext_no_punct\";\n                var nbb_formatted_code = \"text = \\\"\\\"\\\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \\nfishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \\nindeed the vaguest idea where the wood and river in question were.\\\"\\\"\\\"\\n\\n\\nimport string\\n\\ntext_no_punct = \\\"\\\".join([char for char in text if char not in string.punctuation])\\n\\ntext_no_punct\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.236544Z",
     "start_time": "2021-03-28T20:33:38.220156Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "doc = nlp(text_no_punct)\n",
    "for token in doc:\n",
    "    print(token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "He\n",
      "determined\n",
      "to\n",
      "drop\n",
      "his\n",
      "litigation\n",
      "with\n",
      "the\n",
      "monastry\n",
      "and\n",
      "relinguish\n",
      "his\n",
      "claims\n",
      "to\n",
      "the\n",
      "woodcuting\n",
      "and\n",
      "\n",
      "\n",
      "fishery\n",
      "rihgts\n",
      "at\n",
      "once\n",
      "He\n",
      "was\n",
      "the\n",
      "more\n",
      "ready\n",
      "to\n",
      "do\n",
      "this\n",
      "becuase\n",
      "the\n",
      "rights\n",
      "had\n",
      "become\n",
      "much\n",
      "less\n",
      "valuable\n",
      "and\n",
      "he\n",
      "had\n",
      "\n",
      "\n",
      "indeed\n",
      "the\n",
      "vaguest\n",
      "idea\n",
      "where\n",
      "the\n",
      "wood\n",
      "and\n",
      "river\n",
      "in\n",
      "question\n",
      "were\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 42;\n                var nbb_unformatted_code = \"doc = nlp(text_no_punct)\\nfor token in doc:\\n    print(token)\";\n                var nbb_formatted_code = \"doc = nlp(text_no_punct)\\nfor token in doc:\\n    print(token)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:33:38.840465Z",
     "start_time": "2021-03-28T20:33:38.808474Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For text extracted from dialogues or chats, it is convenient to preprocess the text so that multiple occurrences of the same characters get condensed into one or two, and then use a spell checker to find the correct form of the word.\n",
    "\n",
    "A way to do that is to replace all the occurrences of repeated characters with a single one and then use a spell checker: \"hhheeelllllooo hoooowww areee youuu?\" becomes \"helo how are you?\" and then the spell checker would make it \"hello how are you?\"\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "st = \"hhheeeLLLLooo hoooowww areee youuu?????\"\n",
    "text = re.sub(r\"(.)\\1+\", r\"\\1\", st)\n",
    "text"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'heLo how are you?'"
      ]
     },
     "metadata": {},
     "execution_count": 99
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 99;\n                var nbb_unformatted_code = \"st = \\\"hhheeeLLLLooo hoooowww areee youuu?????\\\"\\ntext = re.sub(r\\\"(.)\\\\1+\\\", r\\\"\\\\1\\\", st)\\ntext\";\n                var nbb_formatted_code = \"st = \\\"hhheeeLLLLooo hoooowww areee youuu?????\\\"\\ntext = re.sub(r\\\"(.)\\\\1+\\\", r\\\"\\\\1\\\", st)\\ntext\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:46.506377Z",
     "start_time": "2021-03-28T20:44:46.492492Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "text = nlp(text)\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown([token.text for token in text])\n",
    "\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 100;\n                var nbb_unformatted_code = \"from spellchecker import SpellChecker\\n\\ntext = nlp(text)\\nspell = SpellChecker()\\n\\n# find those words that may be misspelled\\nmisspelled = spell.unknown([token.text for token in text])\\n\\nfor word in misspelled:\\n    # Get the one `most likely` answer\\n    print(spell.correction(word))\\n\\n    # Get a list of `likely` options\\n    print(spell.candidates(word))\";\n                var nbb_formatted_code = \"from spellchecker import SpellChecker\\n\\ntext = nlp(text)\\nspell = SpellChecker()\\n\\n# find those words that may be misspelled\\nmisspelled = spell.unknown([token.text for token in text])\\n\\nfor word in misspelled:\\n    # Get the one `most likely` answer\\n    print(spell.correction(word))\\n\\n    # Get a list of `likely` options\\n    print(spell.candidates(word))\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:44:48.622525Z",
     "start_time": "2021-03-28T20:44:48.518501Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It didn't find any mispelled (even if there was \"helo\"). Try another spell checker:\n",
    "\n",
    "https://github.com/fsondej/autocorrect"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller()\n",
    "\n",
    "spell(text.text)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'hero how are you?'"
      ]
     },
     "metadata": {},
     "execution_count": 95
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 95;\n                var nbb_unformatted_code = \"from autocorrect import Speller\\n\\nspell = Speller()\\n\\nspell(text.text)\";\n                var nbb_formatted_code = \"from autocorrect import Speller\\n\\nspell = Speller()\\n\\nspell(text.text)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-28T20:43:37.953727Z",
     "start_time": "2021-03-28T20:43:37.889573Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, it's not always working properly! However, overall it should improve your text.\n",
    "\n",
    "If you want to create a separate lemmatizer instead of having it in the pipeline:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**For spacy before v3**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n",
    "\n",
    "lemmatizer = nlp.vocab.morphology.lemmatizer\n",
    "print(lemmatizer(\"studying\", VERB))\n",
    "print(lemmatizer(\"studying\", NOUN))\n",
    "print(lemmatizer(\"studying\", ADJ))\n",
    "\n",
    "# or as alternative\n",
    "\n",
    "print(lemmatizer.verb(\"studying\"))\n",
    "print(lemmatizer.noun(\"studying\"))\n",
    "print(lemmatizer.adj(\"studying\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['study']\n",
      "['studying']\n",
      "['studying']\n",
      "['study']\n",
      "['studying']\n",
      "['studying']\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T19:49:41.875502Z",
     "start_time": "2021-03-30T19:49:41.866457Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "spaCy has no built-in stemming! However, Lemmatization is enough for most of the tasks. As alternative, you can use [NLTK library](https://www.nltk.org)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part of Speech (POS) Tagging\n",
    "\n",
    "Parts of speech tagging simply refers to assigning parts of speech to individual words in a sentence, which means that, unlike phrase matching, which is performed at the sentence or multi-word level, parts of speech tagging is performed at the token level."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\n",
    "\n",
    "for token in sentence:\n",
    "    print(token.pos_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PROPN\n",
      "AUX\n",
      "VERB\n",
      "PROPN\n",
      "ADP\n",
      "PROPN\n",
      "PROPN\n",
      "PUNCT\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:36:29.855279Z",
     "start_time": "2021-03-31T04:36:29.828402Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `.pos_` attribute gives the *coarse-grained* POS tag. To inspect the *fine-grained* POS tags we could use the `.tag_`attribute:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "sentence = nlp(\"Antonio is learning Python in Strive School.\")\n",
    "\n",
    "for token in sentence:\n",
    "    print(token.tag_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NNP\n",
      "VBZ\n",
      "VBG\n",
      "NNP\n",
      "IN\n",
      "NNP\n",
      "NNP\n",
      ".\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:38:16.398804Z",
     "start_time": "2021-03-31T04:38:16.359022Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While the output of the `.pos_` attribute is easy to decrypt (`PROPN`: proper noun,\n",
    "`AUX`: Auxiliary verb,\n",
    "`VERB`: verb,\n",
    "`ADP`: Adposition,\n",
    "`PUNCT`: Punctuation), the `.tag_`'s output is more cryptic. For this, you can use the `spacy.explain()` function to get the intuition behind that:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "for token in sentence:\n",
    "    print(spacy.explain(token.tag_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "noun, proper singular\n",
      "verb, 3rd person singular present\n",
      "verb, gerund or present participle\n",
      "noun, proper singular\n",
      "conjunction, subordinating or preposition\n",
      "noun, proper singular\n",
      "noun, proper singular\n",
      "punctuation mark, sentence closer\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:42:21.613532Z",
     "start_time": "2021-03-31T04:42:21.602635Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Go and dig up your primary school grammar book!\n",
    "\n",
    "Let's put everything together:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for token in sentence:\n",
    "    print(f'{token.text:{12}} {token.pos_:{10}} {token.tag_:{8}} {spacy.explain(token.tag_)}')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Antonio      PROPN      NNP      noun, proper singular\n",
      "is           AUX        VBZ      verb, 3rd person singular present\n",
      "learning     VERB       VBG      verb, gerund or present participle\n",
      "Python       PROPN      NNP      noun, proper singular\n",
      "in           ADP        IN       conjunction, subordinating or preposition\n",
      "Strive       PROPN      NNP      noun, proper singular\n",
      "School       PROPN      NNP      noun, proper singular\n",
      ".            PUNCT      .        punctuation mark, sentence closer\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T04:45:40.533471Z",
     "start_time": "2021-03-31T04:45:40.515639Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(the numbers between curly brackets define spaces for a better formatting)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can count the number of occurrences of each POS tag by calling the `count_by` method. \n",
    "\n",
    "The syntax is as follows (you need to pass `spacy.attrs.POS` as argument of the method):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "sentence = nlp(\"Antonio is learning Python Programming Language\")\n",
    "\n",
    "num_pos = sentence.count_by(spacy.attrs.POS)\n",
    "num_pos"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{96: 4, 87: 1, 100: 1}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:01:14.881989Z",
     "start_time": "2021-03-31T05:01:14.735095Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The keys of the vocabulary are the ID of the POS tags, the values are their frequencies of occurrence. To retrieve the POS tags given the ID, you can do as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "sentence.vocab[96].text"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:55.476564Z",
     "start_time": "2021-03-31T05:05:55.466354Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "where 96 is the ID of the tag. Printing all together:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "for ID, frequency in num_pos.items():\n",
    "    print(f\"{ID} stands for {sentence.vocab[ID].text:{8}}: {frequency}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "96 stands for PROPN   : 4\n",
      "87 stands for AUX     : 1\n",
      "100 stands for VERB    : 1\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:05:04.754457Z",
     "start_time": "2021-03-31T05:05:04.749748Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\n",
    "\n",
    "Named entities are available as the ents property of a Doc.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "doc = nlp(\"Antonio works at Strive School.\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 129;\n                var nbb_unformatted_code = \"doc = nlp(\\\"Antonio works at Strive School.\\\")\";\n                var nbb_formatted_code = \"doc = nlp(\\\"Antonio works at Strive School.\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:34.612659Z",
     "start_time": "2021-03-29T06:04:34.586397Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Antonio works at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Strive School\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 131;\n                var nbb_unformatted_code = \"from spacy import displacy\\n\\ndisplacy.render(doc, style=\\\"ent\\\")\";\n                var nbb_formatted_code = \"from spacy import displacy\\n\\ndisplacy.render(doc, style=\\\"ent\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:04:49.092105Z",
     "start_time": "2021-03-29T06:04:49.075027Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "doc = nlp(\"Rome is a big city.\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 136;\n                var nbb_unformatted_code = \"doc = nlp(\\\"Rome is a big city.\\\")\";\n                var nbb_formatted_code = \"doc = nlp(\\\"Rome is a big city.\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:02.488436Z",
     "start_time": "2021-03-29T06:06:02.459044Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rome\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is a big city.</div></span>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 137;\n                var nbb_unformatted_code = \"displacy.render(doc, style=\\\"ent\\\")\";\n                var nbb_formatted_code = \"displacy.render(doc, style=\\\"ent\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            "
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T06:06:05.912053Z",
     "start_time": "2021-03-29T06:06:05.901532Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ORG stands for organization, GPE stands for Geopolitical Entity. Some other tags are:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In spaCy you can list the entities by doing:\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "doc = nlp('Manchester United is looking to sign Harry Kane for $90 million')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.282842Z",
     "start_time": "2021-03-31T05:54:59.233747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "doc.ents"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Manchester United, Harry Kane, $90 million)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:54:59.773187Z",
     "start_time": "2021-03-31T05:54:59.766740Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can access the entities text, label by doing:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Manchester United ORG\n",
      "Harry Kane PERSON\n",
      "$90 million MONEY\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:56:13.150302Z",
     "start_time": "2021-03-31T05:56:13.141509Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even if the entities are self-explanatory for this example, you can use `spacy.explain()` for a detailed description."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, spacy.explain(ent.label_))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Manchester United ORG Companies, agencies, institutions, etc.\n",
      "Harry Kane PERSON People, including fictional\n",
      "$90 million MONEY Monetary values, including unit\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T05:57:32.686265Z",
     "start_time": "2021-03-31T05:57:32.678082Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding new entities\n",
    "\n",
    "If in a text an entity has not being identified correctly, you can manually add it as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "sentence = nlp(u'Strive is setting up a new course.')\n",
    "for entity in sentence.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:08:13.466172Z",
     "start_time": "2021-03-31T06:08:13.421585Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "from spacy.tokens import Span\n",
    "\n",
    "ORG = sentence.vocab.strings[u'ORG']\n",
    "\n",
    "new_entity = Span(sentence, 0, 1, label=ORG)\n",
    "sentence.ents = list(sentence.ents) + [new_entity]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:08:15.759623Z",
     "start_time": "2021-03-31T06:08:15.749143Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "sentence.ents"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Strive,)"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:08:15.930620Z",
     "start_time": "2021-03-31T06:08:15.922601Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we need to import the Span class from the `spacy.tokens` module. Next, we need to get the hash value of the ORG entity type from our document. After that, we need to assign the hash value of ORG to the span. Since \"Strive\" is the first word in the document, the span is 0-1. Finally, we need to add the new entity span to the list of entities. Now if you execute the following script, you will see \"Strive\" in the list of entities."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "for entity in sentence.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Strive - ORG - Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:10:27.513455Z",
     "start_time": "2021-03-31T06:10:27.498248Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(sentence, style=\"ent\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Strive\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is setting up a new course.</div></span>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:11:28.227061Z",
     "start_time": "2021-03-31T06:11:28.210463Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also filter which entity to display:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "sentence = nlp(u'Manchester United is looking to sign Harry Kane for $90 million. David demand 100 Million Dollars')\n",
    "displacy.render(sentence, style='ent', jupyter=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester United\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking to sign \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Harry Kane\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $90 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    David\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " demand \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100 Million Dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:13.012412Z",
     "start_time": "2021-03-31T06:12:12.922949Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "filter = {'ents': ['ORG']}\n",
    "displacy.render(sentence, style='ent', jupyter=True, options=filter)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester United\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking to sign Harry Kane for $90 million. David demand 100 Million Dollars</div></span>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:12:25.772261Z",
     "start_time": "2021-03-31T06:12:25.766031Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Matcher"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Matcher lets you find words and phrases using rules describing their token attributes. Rules can refer to token annotations (like the text or part-of-speech tags), as well as lexical attributes like Token.is_punct. Applying the matcher to a Doc gives you access to the matched tokens in context. For in-depth examples and workflows for combining rules and statistical models, see the usage guide on rule-based matching.\n",
    "\n",
    "https://spacy.io/api/matcher"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test the explorer: https://explosion.ai/demos/matcher"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from spacy.matcher import Matcher"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:48.315906Z",
     "start_time": "2021-03-31T09:15:48.312447Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Let’s say we want to enable spaCy to find a combination of three tokens:\n",
    "\n",
    "- A token whose lowercase form matches “hello”, e.g. “Hello” or “HELLO”.\n",
    "- A token whose is_punct flag is set to True, i.e. any punctuation.\n",
    "- A token whose lowercase form matches “world”, e.g. “World” or “WORLD”.\n",
    "\n",
    "The pattern would be:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:28:52.532218Z",
     "start_time": "2021-03-31T06:28:52.518505Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can define a custom matcher that satisfy that pattern by doing:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"HelloWorld\", [pattern])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:29:52.821170Z",
     "start_time": "2021-03-31T06:29:52.795421Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HelloWorld 0 3 Hello, world\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:30:16.443906Z",
     "start_time": "2021-03-31T06:30:16.415047Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "golang_pattern = [{\"LOWER\":{\n",
    "    \"IN\":[\"go\", \"golang\"]}, \n",
    "                   \"POS\":{\"NOT_IN\":[\"VERB\"]}}]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:33:07.773269Z",
     "start_time": "2021-03-31T06:33:07.763583Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "doc = nlp(\"I go the learn the Go programming language\")\n",
    "matcher.add(\"GOLANG\", [golang_pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GOLANG 5 6 Go\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:36:07.310668Z",
     "start_time": "2021-03-31T06:36:07.279388Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "for token in doc:\n",
    "    print(token.pos_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PRON\n",
      "VERB\n",
      "DET\n",
      "VERB\n",
      "DET\n",
      "PROPN\n",
      "NOUN\n",
      "NOUN\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:36:12.070611Z",
     "start_time": "2021-03-31T06:36:12.066899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "text = nlp(\"This is a tweet about Dogecoin $DOGE #DOGE\")\n",
    "\n",
    "for i, token in enumerate(text):\n",
    "    print(i, token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 This\n",
      "1 is\n",
      "2 a\n",
      "3 tweet\n",
      "4 about\n",
      "5 Dogecoin\n",
      "6 $\n",
      "7 DOGE\n",
      "8 #\n",
      "9 DOGE\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:35.197670Z",
     "start_time": "2021-03-31T09:15:35.171259Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "pattern = [ {\"ORTH\":\"$\", \"OP\":\"?\"},\n",
    "           {\"ORTH\":\"#\", \"OP\":\"?\"},\n",
    "        {\"LOWER\": {\"IN\":[\"doge\", \"dogecoin\"]}}\n",
    "]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:39.837461Z",
     "start_time": "2021-03-31T09:15:39.833204Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"DOGE\", [pattern])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:51.783195Z",
     "start_time": "2021-03-31T09:15:51.779647Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "matches = matcher(text)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = text[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DOGE 5 6 Dogecoin\n",
      "DOGE 6 8 $DOGE\n",
      "DOGE 7 8 DOGE\n",
      "DOGE 8 10 #DOGE\n",
      "DOGE 9 10 DOGE\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:15:52.142087Z",
     "start_time": "2021-03-31T09:15:52.131733Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also match \"whatever token\" by adding an empty \\{\\} in the pattern:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "text = nlp(\"I want to match whatever follows here\")\n",
    "pattern = [ {\"LOWER\":\"follows\"},\n",
    "             {}\n",
    "]"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:18:49.324240Z",
     "start_time": "2021-03-31T09:18:49.304322Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"WHATEVER\", [pattern])"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:18:49.728059Z",
     "start_time": "2021-03-31T09:18:49.725041Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "matches = matcher(text)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = text[start:end]  # The matched span\n",
    "    print(string_id, start, end, span.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WHATEVER 5 7 follows here\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T09:18:49.890304Z",
     "start_time": "2021-03-31T09:18:49.882152Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}